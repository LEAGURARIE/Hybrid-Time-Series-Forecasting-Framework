# üìà Google Stock Returns Prediction Framework

**A Multi-Layered Research and Forecasting System for GOOGL Stock**

End-to-end ML pipeline for predicting daily stock log returns, combining tree-based models (XGBoost, LightGBM) with Deep Learning (LSTM, GRU, Hybrid architectures), advanced ensemble methods, and statistical validation via Bootstrap confidence intervals.

---

## üåü Key Features

### Feature Engineering
- **Macro indicators**: CPI, Federal Funds Rate, inflation acceleration
- **Market sentiment**: VIX levels, VIX term structure, regime detection
- **Fundamentals**: EPS surprise from earnings reports
- **Cross-asset signals**: Rolling beta and correlation vs SPY, QQQ, NASDAQ
- **EU market gaps**: Pre-market signals from DAX overnight moves (using ^GDAXI Close only)
- **Technical features**: Rolling statistics, volume patterns, momentum

### Feature Selection (Multi-Stage)
1. **Spearman filter**: Remove highly correlated features (œÅ > 0.9)
2. **XGBoost importance**: GAIN-based ranking with permutation validation
3. **Mutual Information**: For neural network feature sets

### Models
| Type | Models | Notes |
|------|--------|-------|
| Tree-based | XGBoost, LightGBM | 3-stage HPO (280 trials) |
| Deep Learning | LSTM, GRU | Sequence-based with early stopping |
| Hybrid | Sequential (LSTM‚ÜíGRU), Parallel (LSTM‚à•GRU) | Combined architectures |
| Ensemble | Weighted Average, Stacking | Inverse-wRMSE weighting |

### Validation & Explainability
- **Temporal split**: Train ‚Üí Valid (ES + Score) ‚Üí Test
- **Bootstrap CI**: 95% confidence intervals (n=1000)
- **SHAP analysis**: Global importance + Beeswarm plots
- **Baselines**: ZERO (predict 0) and NAIVE (last value) benchmarks

---

## üèóÔ∏è Project Structure

The project follows a **modular architecture** where each component (data loading, feature engineering, models, etc.) is a separate module that can be used independently or as part of the full pipeline.

```
GoogleStockProject/
‚îú‚îÄ‚îÄ src/                          # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # CLI configuration (CONFIG dict + setup_cli_paths)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                  # I/O, metrics, statistics
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders.py            # Data fetching (yfinance, FRED, etc.)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ split.py              # Train/Valid/Test splitting
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engineering.py        # Feature construction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selection.py          # Feature selection pipeline
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xgboost_model.py      # XGBoost training + SHAP
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lightgbm_model.py     # LightGBM training + SHAP
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm_gru.py           # LSTM/GRU wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural_utils.py       # NN training utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid.py             # Hybrid architectures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ensemble.py           # Ensemble methods
‚îÇ   ‚îî‚îÄ‚îÄ tuning/
‚îÇ       ‚îî‚îÄ‚îÄ hpo.py                # Hyperparameter optimization
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py           # CLI entry point (orchestrates modules)
‚îÇ
‚îú‚îÄ‚îÄ output/                       # All outputs (created at runtime)
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Downloaded price data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interim/              # Feature-engineered data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processed/            # Train/Valid/Test splits
‚îÇ   ‚îú‚îÄ‚îÄ runs/                     # Experiment snapshots
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {RUN_ID}/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config/           # Run configuration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ feature_selection/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ models/           # Trained models + metrics
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ predictions/      # Model predictions
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ outputs/          # Ensemble results
‚îÇ   ‚îî‚îÄ‚îÄ results_summary/          # Leaderboards, reports
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ google_stock_ml_unified.ipynb  # Interactive Colab version (self-contained)
‚îÇ
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üöÄ Quick Start

### Installation
```bash
pip install -r requirements.txt
```

### CLI Usage

**Full pipeline:**
```bash
python scripts/run_pipeline.py
```

**Specific steps only:**
```bash
python scripts/run_pipeline.py --steps models,ensemble,summary
```

**Selected models:**
```bash
python scripts/run_pipeline.py --models xgb,lgb,lstm
```

### Google Colab
Upload `notebooks/google_stock_ml_unified.ipynb` to Colab and run all cells.

---

## ‚öôÔ∏è Configuration

### üìç Where to Configure

| Environment | Configuration Location | Notes |
|-------------|----------------------|-------|
| **CLI** | `src/config.py` ‚Üí `CONFIG` dict | Used by `run_pipeline.py` |
| **Notebook** | Cell 3 ‚Üí `RUN_PARAMS` dict | Self-contained, independent from CLI |

> **Note**: CLI and Notebook configurations are completely separate. Changes in one do not affect the other.

---

### üìÖ Date Configuration (Data Split)

The data split is controlled by the following parameters:

```python
"data": {
    "start_date": "2004-09-01",      # First date to download
    "end_date": "2026-01-15",        # Last date (None = today)
    "limit_start_date": "2005-12-31", # First date for modeling (after warmup)
    "train_end": "2020-12-31",       # Last date for training
    "valid_start": "2021-01-01",     # First date for validation
    "valid_end": "2023-12-31",       # Last date for validation
    "test_start": "2023-01-01",      # First date for test
    "test_end": None,                # Last date for test (None = end_date)
}
```

**HPO validation dates** (for hyperparameter tuning):

```python
"hpo": {
    "valid_es_start": "2021-01-01",    # Early stopping validation start
    "valid_es_end": "2021-12-31",      # Early stopping validation end
    "valid_score_start": "2022-01-01", # Scoring validation start
    "valid_score_end": "2023-12-31",   # Scoring validation end
}
```

#### Timeline Visualization

```
|-------- TRAIN --------|--- VALID_ES ---|--- VALID_SCORE ---|---- TEST ----|
2005                   2020            2021               2022            2023‚Üí
     limit_start        train_end    valid_es_end    valid_score_end    end_date
```

#### ‚ö†Ô∏è Important Notes

1. **Reproducibility**: Set `end_date` to a fixed date for consistent results across runs
2. **Warmup period**: `limit_start_date` should be after `start_date` to allow rolling feature calculation
3. **No overlap**: Ensure `valid_end` ‚â• `valid_start` and `test_start` > `train_end`
4. **HPO validation**: `valid_score_end` is used for model selection during hyperparameter optimization

---

### üîß Other Key Settings

#### Feature Exclusions

Some tickers lack reliable Volume data and should be excluded from volume-based features:

```python
"features": {
    "exclude_raw_ohlc": ["^VIX", "^TNX", "^GDAXI"],  # No volume features
}
```

> **Note**: ^GDAXI (DAX index) is still used for EU break close flags (using Close price only), but excluded from volume feature engineering.

#### EU Break Close Configuration

```python
"eu_break_close": {
    "enabled": True,
    "eu_ticker": "^GDAXI",
    "apply_to": "next_us_trading_day",
}
```

---

## üìä Example Output

```
============================================================
 RESULTS SUMMARY
============================================================
 rank  model                      test_wrmse  test_diracc
    1  Ensemble-weighted_average    0.020029     0.4693
    2  XGBoost                      0.022925     0.4244
    3  GRU                          0.023500     0.5102
    4  Hybrid-Seq                   0.023107     0.4407
    5  LightGBM                     0.023852     0.4431
    6  LSTM                         0.035235     0.5539

Bootstrap 95% CI (n=1000):
  Ensemble: 0.020029 [0.018237, 0.022062]
  XGBoost:  0.022925 [0.020658, 0.025923]
```

---

## üîç SHAP Explainability

The framework includes SHAP analysis for model interpretation:

- **Global Importance**: Ranking of most influential features
- **Beeswarm Plots**: Feature value impact on predictions
- **Per-prediction breakdown**: Understanding individual forecasts

Output files:
- `shap_feature_importance.csv`
- `shap_summary_bar.png`
- `shap_summary_beeswarm.png`

---

## üìà Metrics

| Metric | Description |
|--------|-------------|
| **wRMSE** | Weighted Root Mean Squared Error (primary metric) |
| **wMAE** | Weighted Mean Absolute Error |
| **DirAcc** | Directional Accuracy (% correct sign predictions) |

Sample weights use exponential decay to emphasize recent observations.

---

## üõ†Ô∏è Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| Missing Volume data for ^GDAXI | Already excluded via `exclude_raw_ohlc` |
| `xgb_selected` features not found | Ensure XGB feature selection ran before neural models |
| Date alignment errors | Check that all date parameters are consistent |

---

## ü§ù Contributing

Contributions welcome:

1. **Alternative data**: Sentiment analysis, news signals
2. **New architectures**: Transformers, Temporal Fusion Transformer
3. **Custom loss functions**: Asymmetric loss for risk management
4. **Backtesting framework**: Strategy simulation

---

## ‚ö†Ô∏è Disclaimer

This project is for **research and educational purposes only**. It does not constitute financial advice or a recommendation to trade securities. Past performance does not guarantee future results.

---

## üìÑ License

MIT License

---

## üë©‚Äçüíª Author

Created by **Lea G.**
