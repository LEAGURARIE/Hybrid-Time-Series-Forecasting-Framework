{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vZRr01hvSAW5",
      "metadata": {
        "id": "vZRr01hvSAW5"
      },
      "source": [
        "# ðŸ“ˆ Google Stock ML Prediction Project\n",
        "\n",
        "End-to-end machine learning pipeline for predicting Google (GOOGL) stock next-day returns.\n",
        "\n",
        "## Models\n",
        "- **XGBoost** â€” Gradient boosting with 3-stage HPO\n",
        "- **LSTM & GRU** â€” Recurrent neural networks\n",
        "- **Hybrid** â€” Sequential & Parallel architectures\n",
        "\n",
        "## Sections\n",
        "1. Configuration & Helpers\n",
        "2. Data Ingestion & Preprocessing\n",
        "3. Exploratory Data Analysis\n",
        "4. Train/Valid/Test Split & NN Features\n",
        "5. Feature Selection\n",
        "6. XGBoost HPO\n",
        "7. XGBoost Final Model\n",
        "8. LSTM & GRU\n",
        "9. Hybrid Neural Networks\n",
        "10. Final Summary\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wb7SetOlSAW7",
      "metadata": {
        "id": "wb7SetOlSAW7"
      },
      "source": [
        "---\n",
        "# SECTION 1: Configuration & Helpers\n",
        "\n",
        "**Setup, imports, paths, and helper functions**\n",
        "\n",
        "**Blocks:** 0-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50747bf",
      "metadata": {
        "id": "c50747bf"
      },
      "source": [
        "## BOOT + BLOCK 0 â€” CONFIG + HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "a51108ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51108ec",
        "outputId": "f92dd64b-9624-407f-8649-6b38e5630c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BOOT] DRIVE_PROJECT_ROOT: /content/drive/MyDrive/my_project\n",
            "[BOOT] LOCAL_PROJECT_ROOT: /content/my_project\n",
            "[BOOT] PROJECT_ROOT (active): /content/my_project\n",
            "[CONFIG] RUN_ID: 20260118_092035\n",
            "[CONFIG] LOCAL_RUN_DIR: /content/my_project/runs/20260118_092035\n",
            "[CONFIG] DRIVE_RUN_DIR: /content/drive/MyDrive/my_project/runs/20260118_092035\n",
            "[SNAPSHOT] Notebook saved via Colab API\n",
            "[SNAPSHOT] LOCAL: Notebook not found\n",
            "[SNAPSHOT] DRIVE: Notebook not found\n",
            "[OK] BOOT + BLOCK 0 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "import shutil\n",
        "import subprocess\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "from glob import glob\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Colab Detection & Drive Mount (once) ---\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    if not Path(\"/content/drive/MyDrive\").exists():\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# --- Project Paths ---\n",
        "DRIVE_PROJECT_ROOT = Path(os.environ.get(\n",
        "    \"DRIVE_PROJECT_ROOT\", \"/content/drive/MyDrive/my_project\"\n",
        ")).expanduser()\n",
        "LOCAL_PROJECT_ROOT = Path(os.environ.get(\n",
        "    \"LOCAL_PROJECT_ROOT\", \"/content/my_project\"\n",
        ")).expanduser()\n",
        "\n",
        "DRIVE_PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "LOCAL_PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Sync Drive -> Local if local is empty but Drive has content\n",
        "local_has_files = any(LOCAL_PROJECT_ROOT.rglob(\"*\"))\n",
        "drive_has_files = any(DRIVE_PROJECT_ROOT.rglob(\"*\"))\n",
        "\n",
        "if (not local_has_files) and drive_has_files:\n",
        "    for item in LOCAL_PROJECT_ROOT.iterdir():\n",
        "        if item.is_dir():\n",
        "            shutil.rmtree(item)\n",
        "        else:\n",
        "            item.unlink()\n",
        "    shutil.copytree(DRIVE_PROJECT_ROOT, LOCAL_PROJECT_ROOT, dirs_exist_ok=True)\n",
        "\n",
        "# Active project root (local for fast I/O)\n",
        "PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
        "\n",
        "print(\"[BOOT] DRIVE_PROJECT_ROOT:\", DRIVE_PROJECT_ROOT)\n",
        "print(\"[BOOT] LOCAL_PROJECT_ROOT:\", LOCAL_PROJECT_ROOT)\n",
        "print(\"[BOOT] PROJECT_ROOT (active):\", PROJECT_ROOT)\n",
        "\n",
        "# --- Run ID & Directories ---\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "LOCAL_RUNS_ROOT = Path(os.environ.get(\n",
        "    \"LOCAL_RUNS_ROOT\", str(PROJECT_ROOT / \"runs\")\n",
        ")).expanduser()\n",
        "DRIVE_RUNS_ROOT = Path(os.environ.get(\n",
        "    \"DRIVE_RUNS_ROOT\", str(DRIVE_PROJECT_ROOT / \"runs\")\n",
        ")).expanduser()\n",
        "\n",
        "LOCAL_RUN_DIR = LOCAL_RUNS_ROOT / RUN_ID\n",
        "DRIVE_RUN_DIR = DRIVE_RUNS_ROOT / RUN_ID\n",
        "\n",
        "\n",
        "def _mk_run_dirs(run_dir: Path) -> Dict[str, Path]:\n",
        "    \"\"\"Create standard run directory structure.\"\"\"\n",
        "    paths = {\n",
        "        \"run_dir\": run_dir,\n",
        "        \"outputs_dir\": run_dir / \"outputs\",\n",
        "        \"models_dir\": run_dir / \"models\",\n",
        "        \"reports_dir\": run_dir / \"reports\",\n",
        "        \"plots_dir\": run_dir / \"plots\",\n",
        "        \"config_dir\": run_dir / \"config\",\n",
        "        \"logs_dir\": run_dir / \"logs\",\n",
        "        \"proc_dir\": run_dir / \"processed\",\n",
        "        \"fs_dir\": run_dir / \"feature_selection\",\n",
        "        \"ms_dir\": run_dir / \"model_selection\",\n",
        "    }\n",
        "    for p in paths.values():\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "    return paths\n",
        "\n",
        "\n",
        "LOCAL_PATHS = _mk_run_dirs(LOCAL_RUN_DIR)\n",
        "DRIVE_PATHS = _mk_run_dirs(DRIVE_RUN_DIR)\n",
        "\n",
        "# Active runtime uses LOCAL paths (fast I/O)\n",
        "RUN_DIR = LOCAL_PATHS[\"run_dir\"]\n",
        "OUTPUTS_DIR = LOCAL_PATHS[\"outputs_dir\"]\n",
        "MODELS_DIR = LOCAL_PATHS[\"models_dir\"]\n",
        "REPORTS_DIR = LOCAL_PATHS[\"reports_dir\"]\n",
        "PLOTS_DIR = LOCAL_PATHS[\"plots_dir\"]\n",
        "CONFIG_DIR = LOCAL_PATHS[\"config_dir\"]\n",
        "LOGS_DIR = LOCAL_PATHS[\"logs_dir\"]\n",
        "PROC_DIR = LOCAL_PATHS[\"proc_dir\"]\n",
        "FS_DIR = LOCAL_PATHS[\"fs_dir\"]\n",
        "MS_DIR = LOCAL_PATHS[\"ms_dir\"]\n",
        "\n",
        "# Project-level data directories (not run-specific)\n",
        "DATA_DIRS_LOCAL = {\n",
        "    \"raw\": PROJECT_ROOT / \"data\" / \"raw\",\n",
        "    \"interim\": PROJECT_ROOT / \"data\" / \"interim\",\n",
        "    \"processed\": PROJECT_ROOT / \"data\" / \"processed\",\n",
        "}\n",
        "DATA_DIRS_DRIVE = {\n",
        "    \"raw\": DRIVE_PROJECT_ROOT / \"data\" / \"raw\",\n",
        "    \"interim\": DRIVE_PROJECT_ROOT / \"data\" / \"interim\",\n",
        "    \"processed\": DRIVE_PROJECT_ROOT / \"data\" / \"processed\",\n",
        "}\n",
        "for _d in list(DATA_DIRS_LOCAL.values()) + list(DATA_DIRS_DRIVE.values()):\n",
        "    _d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"[CONFIG] RUN_ID:\", RUN_ID)\n",
        "print(\"[CONFIG] LOCAL_RUN_DIR:\", LOCAL_RUN_DIR)\n",
        "print(\"[CONFIG] DRIVE_RUN_DIR:\", DRIVE_RUN_DIR)\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def ensure_dir(p: Path) -> Path:\n",
        "    \"\"\"Create directory if not exists, return path.\"\"\"\n",
        "    p = Path(p)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "def save_text(text: str, path: Path) -> Path:\n",
        "    \"\"\"Save text to file.\"\"\"\n",
        "    path = Path(path)\n",
        "    ensure_dir(path.parent)\n",
        "    path.write_text(text, encoding=\"utf-8\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def save_json(obj: Any, path: Path, indent: int = 2) -> Path:\n",
        "    \"\"\"Save object as JSON.\"\"\"\n",
        "    path = Path(path)\n",
        "    ensure_dir(path.parent)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
        "    return path\n",
        "\n",
        "\n",
        "def save_pickle(obj: Any, path: Path) -> Path:\n",
        "    \"\"\"Save object as pickle.\"\"\"\n",
        "    path = Path(path)\n",
        "    ensure_dir(path.parent)\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    return path\n",
        "\n",
        "\n",
        "def load_pickle(path: Path) -> Any:\n",
        "    \"\"\"Load object from pickle.\"\"\"\n",
        "    path = Path(path)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_with_fallback(filename: str, run_dir_local: Path, fallback_dir_local: Path,\n",
        "                        run_dir_drive: Path = None, fallback_dir_drive: Path = None,\n",
        "                        use_pandas: bool = False) -> Any:\n",
        "    \"\"\"Load file with 4-level fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE.\n",
        "\n",
        "    Args:\n",
        "        filename: Name of the file to load\n",
        "        run_dir_local: Local run directory (runs/RUN_ID/...)\n",
        "        fallback_dir_local: Local persistent directory (data/processed/)\n",
        "        run_dir_drive: Drive run directory (optional)\n",
        "        fallback_dir_drive: Drive persistent directory (optional)\n",
        "        use_pandas: If True, use pd.read_pickle; otherwise use load_pickle\n",
        "\n",
        "    Returns:\n",
        "        Loaded object\n",
        "    \"\"\"\n",
        "    # Build list of paths to try\n",
        "    paths_to_try = [\n",
        "        (Path(run_dir_local) / filename, \"RUN_ID (LOCAL)\"),\n",
        "        (Path(fallback_dir_local) / filename, \"data/processed (LOCAL)\"),\n",
        "    ]\n",
        "\n",
        "    if run_dir_drive:\n",
        "        paths_to_try.append((Path(run_dir_drive) / filename, \"RUN_ID (DRIVE)\"))\n",
        "    if fallback_dir_drive:\n",
        "        paths_to_try.append((Path(fallback_dir_drive) / filename, \"data/processed (DRIVE)\"))\n",
        "\n",
        "    # Try each path\n",
        "    for path, source_name in paths_to_try:\n",
        "        if path.exists():\n",
        "            print(f\"  [LOAD] {filename} <- {source_name}\")\n",
        "            if use_pandas:\n",
        "                return pd.read_pickle(path)\n",
        "            return load_pickle(path)\n",
        "\n",
        "    # None found\n",
        "    tried = \"\\n  \".join([f\"{src}: {p}\" for p, src in paths_to_try])\n",
        "    raise FileNotFoundError(f\"File not found: {filename}\\nTried:\\n  {tried}\")\n",
        "\n",
        "def copy_file(src: Path, dst: Path) -> Path:\n",
        "    \"\"\"Copy file with metadata.\"\"\"\n",
        "    src, dst = Path(src), Path(dst)\n",
        "    ensure_dir(dst.parent)\n",
        "    shutil.copy2(src, dst)\n",
        "    return dst\n",
        "\n",
        "\n",
        "def copy_tree(src_dir: Path, dst_dir: Path, ignore: Optional[Any] = None) -> Path:\n",
        "    \"\"\"Copy directory tree.\"\"\"\n",
        "    src_dir, dst_dir = Path(src_dir), Path(dst_dir)\n",
        "    ensure_dir(dst_dir.parent)\n",
        "    if dst_dir.exists():\n",
        "        shutil.rmtree(dst_dir)\n",
        "    shutil.copytree(src_dir, dst_dir, ignore=ignore)\n",
        "    return dst_dir\n",
        "\n",
        "\n",
        "def run_dirs() -> Tuple[Path, Path]:\n",
        "    \"\"\"Return (plots_dir, reports_dir) for current run.\"\"\"\n",
        "    return Path(PLOTS_DIR), Path(REPORTS_DIR)\n",
        "\n",
        "\n",
        "# --- RUN PARAMETERS ---\n",
        "RUN_PARAMS: Dict[str, Any] = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"random_state\": 42,  # Global random state for reproducibility\n",
        "    # Used by: Cell 5 (paths setup)\n",
        "    \"paths\": {\n",
        "        \"project_root_local\": str(PROJECT_ROOT),\n",
        "        \"project_root_drive\": str(DRIVE_PROJECT_ROOT),\n",
        "        \"run_dir_local\": str(LOCAL_RUN_DIR),\n",
        "        \"outputs_dir_local\": str(LOCAL_PATHS[\"outputs_dir\"]),\n",
        "        \"models_dir_local\": str(LOCAL_PATHS[\"models_dir\"]),\n",
        "        \"reports_dir_local\": str(LOCAL_PATHS[\"reports_dir\"]),\n",
        "        \"plots_dir_local\": str(LOCAL_PATHS[\"plots_dir\"]),\n",
        "        \"config_dir_local\": str(LOCAL_PATHS[\"config_dir\"]),\n",
        "        \"logs_dir_local\": str(LOCAL_PATHS[\"logs_dir\"]),\n",
        "        \"proc_dir_local\": str(LOCAL_PATHS[\"proc_dir\"]),\n",
        "        \"fs_dir_local\": str(LOCAL_PATHS[\"fs_dir\"]),\n",
        "        \"ms_dir_local\": str(LOCAL_PATHS[\"ms_dir\"]),\n",
        "        \"run_dir_drive\": str(DRIVE_RUN_DIR),\n",
        "        \"outputs_dir_drive\": str(DRIVE_PATHS[\"outputs_dir\"]),\n",
        "        \"models_dir_drive\": str(DRIVE_PATHS[\"models_dir\"]),\n",
        "        \"reports_dir_drive\": str(DRIVE_PATHS[\"reports_dir\"]),\n",
        "        \"plots_dir_drive\": str(DRIVE_PATHS[\"plots_dir\"]),\n",
        "        \"config_dir_drive\": str(DRIVE_PATHS[\"config_dir\"]),\n",
        "        \"logs_dir_drive\": str(DRIVE_PATHS[\"logs_dir\"]),\n",
        "        \"proc_dir_drive\": str(DRIVE_PATHS[\"proc_dir\"]),\n",
        "        \"fs_dir_drive\": str(DRIVE_PATHS[\"fs_dir\"]),\n",
        "        \"ms_dir_drive\": str(DRIVE_PATHS[\"ms_dir\"]),\n",
        "        # Project-level data directories (not run-specific)\n",
        "        \"data_raw_local\": str(DATA_DIRS_LOCAL[\"raw\"]),\n",
        "        \"data_interim_local\": str(DATA_DIRS_LOCAL[\"interim\"]),\n",
        "        \"data_processed_local\": str(DATA_DIRS_LOCAL[\"processed\"]),\n",
        "        \"data_raw_drive\": str(DATA_DIRS_DRIVE[\"raw\"]),\n",
        "        \"data_interim_drive\": str(DATA_DIRS_DRIVE[\"interim\"]),\n",
        "        \"data_processed_drive\": str(DATA_DIRS_DRIVE[\"processed\"]),\n",
        "    },\n",
        "    # Used by: Blocks 3,4,14,15,19,20 (data loading & split)\n",
        "    \"data\": {\n",
        "        \"target\": \"GOOGL_logret_t1\",\n",
        "        \"target_src_col\": \"GOOGL_logret_cc\",\n",
        "        \"target_col\": \"GOOGL_logret_t1\",\n",
        "        \"start_date\": \"2023-11-20\",  # ~21 trading days before train start\n",
        "        \"end_date\": \"2026-01-15\",\n",
        "        \"limit_start_date\": \"2023-12-31\",  # Train starts exactly at 01/01/2024\n",
        "        # Short-range split (1 year train, 6 months valid, 6 months test)\n",
        "        \"train_end\": \"2025-05-20\",      # Training: full year 2024\n",
        "        \"valid_start\": \"2025-05-21\",    # Validation: first half of 2025\n",
        "        \"valid_end\": \"2025-09-10\",\n",
        "        \"test_start\": \"2025-09-11\",     # Test: July 2025 to today\n",
        "        \"test_end\": None,\n",
        "    },\n",
        "    # Used by: Blocks 7-13,18,19,20 (feature engineering)\n",
        "    \"features\": {\n",
        "        \"feature_set_name\": \"XGB-30\",\n",
        "        \"feature_selection_artifact\": \"selected_features_xgb.pkl\",\n",
        "        # Rolling windows\n",
        "        \"rolling_w_short\": 5,\n",
        "        \"rolling_w_long\": 21,\n",
        "        \"do_volume_rolling\": True,\n",
        "        # Cross-asset\n",
        "        \"cross_asset_base\": \"GOOGL\",\n",
        "        \"cross_asset_peers\": [\"SPY\", \"QQQ\", \"^IXIC\", \"XLK\"],\n",
        "        \"cross_asset_windows\": [5, 21],\n",
        "        # Regime\n",
        "        \"regime_base\": \"GOOGL\",\n",
        "        \"market_vol_ticker\": \"SPY\",\n",
        "        # Exclusions\n",
        "        \"exclude_raw_ohlc\": [\"^VIX\", \"^TNX\", \"^GDAXI\"],\n",
        "        \"drop_volume_tickers\": [\"^VIX\", \"^TNX\", \"^GDAXI\"],  # Drop raw Volume columns\n",
        "        # Crisis periods\n",
        "        \"covid_start\": \"2020-02-01\",\n",
        "        \"covid_end\": \"2023-05-05\",\n",
        "        \"crisis_2008_start\": \"2007-07-01\",\n",
        "        \"crisis_2008_end\": \"2009-09-01\",\n",
        "        # Numeric safety\n",
        "        \"eps\": 1e-12,\n",
        "    },\n",
        "    # XGBoost\n",
        "\n",
        "\n",
        "    # EU Break Close Flags\n",
        "    # Used by: Block 2B (EU break close flags)\n",
        "    \"eu_break_close\": {\n",
        "        \"enabled\": True,\n",
        "        \"eu_ticker\": \"^GDAXI\",\n",
        "        \"gap_days_threshold\": 2,\n",
        "        \"apply_to\": \"next_us_trading_day\",  # \"same_calendar_date\" or \"next_us_trading_day\"\n",
        "    },\n",
        "    # EDA\n",
        "    # Used by: Blocks 17,18,19 (EDA)\n",
        "    \"eda\": {\n",
        "        \"enabled\": True,\n",
        "        \"returns_bins\": 50,\n",
        "    },\n",
        "    # Sample weights\n",
        "    # Used by: Block 20 (sample weights)\n",
        "    \"weights\": {\n",
        "        \"c\": 1.0,\n",
        "        \"max_w\": 4.0,\n",
        "    },\n",
        "    # NN Feature Selection\n",
        "    # Used by: Block 21 (NN feature groups)\n",
        "    \"nn_feature_select\": {\n",
        "        \"n40\": 40,\n",
        "        \"n80\": 80,\n",
        "        \"per_group_40\": 4,\n",
        "        \"per_group_80\": 8,\n",
        "        \"corr_thr\": 0.95,\n",
        "        \"mi_n_neighbors\": 5,\n",
        "        \"mi_random_state\": 42,\n",
        "    },\n",
        "    # Used by: Block 23 (XGB feature selection)\n",
        "    \"xgb_fs\": {\n",
        "        \"spearman_thresh\": 0.85,\n",
        "        \"gain_cum_thresh\": 0.85,\n",
        "        \"min_features\": 10,\n",
        "        \"neg_sigma\": 1.2,\n",
        "        \"pos_sigma\": 0.7,\n",
        "        \"min_gain\": 0.001,\n",
        "        \"perm_repeats\": 15,\n",
        "        \"n_estimators\": 1000,\n",
        "        \"learning_rate\": 0.02,\n",
        "        \"max_depth\": 3,\n",
        "        \"min_child_weight\": 15,\n",
        "        \"gamma\": 1,\n",
        "        \"subsample\": 0.60,\n",
        "        \"colsample_bytree\": 0.60,\n",
        "        \"reg_alpha\": 0.1,\n",
        "        \"reg_lambda\": 10.0,\n",
        "        \"max_delta_step\": 1,\n",
        "        \"early_stopping_rounds\": 40,\n",
        "        \"random_state\": 42,\n",
        "    },\n",
        "    # Used by: Blocks 24,25,26,28 (HPO & model training)\n",
        "    \"hpo\": {\n",
        "        \"n_estimators\": 1500,\n",
        "        \"early_stopping_rounds\": 40,\n",
        "        \"n_trials_stage1\": 120,\n",
        "        \"n_trials_stage2\": 60,\n",
        "        \"n_trials_stage2_lowlr\": 30,\n",
        "        \"print_every_stage1\": 10,\n",
        "        \"print_every_stage2\": 20,\n",
        "        \"tie_tol\": 1e-5,\n",
        "        \"random_state\": 42,\n",
        "        # Shared lookback for alignment with Neural Networks\n",
        "        \"lookback\": 7,\n",
        "        # XGBoost model settings\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"eval_metric\": \"rmse\",\n",
        "        \"tree_method\": \"hist\",\n",
        "        # Date-based\n",
        "        \"valid_es_start\": \"2025-05-21\",   # Q1 for early stopping\n",
        "        \"valid_es_end\": \"2025-07-15\",\n",
        "        \"valid_score_start\": \"2025-07-16\",  # Q2 for model scoring\n",
        "        \"valid_score_end\": \"2025-09-10\",\n",
        "        # Sampling parameters for HPO\n",
        "        # Used by: Block 24 (HPO sampling)\n",
        "    \"sampling\": {\n",
        "            # Used by: Block 24 (HPO Stage 1)\n",
        "    \"broad\": {\n",
        "                \"max_depth\": [2, 7],\n",
        "                \"lr_low\": [0.003, 0.06],\n",
        "                \"lr_high\": [0.06, 0.12],\n",
        "                \"lr_high_prob\": 0.10,\n",
        "                \"min_child_weight_log\": [0.5, 20.0],\n",
        "                \"subsample\": [0.6, 1.0],\n",
        "                \"colsample_bytree\": [0.55, 1.0],\n",
        "                \"gamma\": [0.5, 4.0],\n",
        "                \"reg_alpha_exp\": [-9, -2],\n",
        "                \"reg_lambda_exp\": [-2, 1.3],\n",
        "                \"max_delta_step\": [0.0, 1.0],\n",
        "            },\n",
        "            # Used by: Block 24 (HPO Stage 2)\n",
        "    \"refine\": {\n",
        "                \"max_depth_delta\": [-1, 1],\n",
        "                \"max_depth_clip\": [2, 8],\n",
        "                \"lr_sigma\": 0.20,\n",
        "                \"lr_clip\": [0.002, 0.15],\n",
        "                \"min_child_weight_sigma\": 0.40,\n",
        "                \"min_child_weight_clip\": [0.3, 30.0],\n",
        "                \"subsample_sigma\": 0.06,\n",
        "                \"subsample_clip\": [0.5, 1.0],\n",
        "                \"colsample_sigma\": 0.06,\n",
        "                \"colsample_clip\": [0.5, 1.0],\n",
        "                \"gamma_sigma\": 0.25,\n",
        "                \"gamma_clip\": [0.0, 5.0],\n",
        "                \"reg_alpha_sigma\": 0.7,\n",
        "                \"reg_alpha_exp_clip\": [-10, 0],\n",
        "                \"reg_lambda_sigma\": 0.4,\n",
        "                \"reg_lambda_exp_clip\": [-3, 2],\n",
        "                \"max_delta_step_sigma\": 0.15,\n",
        "                \"max_delta_step_clip\": [0.0, 4.0],\n",
        "            },\n",
        "            # Used by: Block 24 (HPO Stage 2 Low-LR)\n",
        "    \"refine_low_lr\": {\n",
        "                \"lr_shift\": -0.8,\n",
        "                \"lr_clip\": [0.0015, 0.06],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    # Used by: Blocks 25,27,29 (plotting)\n",
        "    \"plot\": {\n",
        "        \"n_plot\": 200,\n",
        "        \"figsize\": [13, 5],\n",
        "        \"dpi\": 150,\n",
        "    },\n",
        "    # Used by: Block 25 (SHAP analysis)\n",
        "    \"shap\": {\n",
        "        \"enabled\": True,\n",
        "        \"top_n_features\": None,  # None = use xgb_selected, or int (e.g., 10) for top N SHAP features\n",
        "        \"max_display\": 20,\n",
        "        \"figsize\": [10, 8],\n",
        "        \"plot_type_bar\": True,\n",
        "        \"plot_type_beeswarm\": True,\n",
        "        \"save_values\": True,\n",
        "    },\n",
        "    # Used by: Blocks 26,27 (LSTM model)\n",
        "        \"lstm\": {\n",
        "        \"lookback\": 7,\n",
        "        \"stride\": 1,\n",
        "        \"units_1\": 8,\n",
        "        \"units_2\": 4,\n",
        "        \"dense_units\": 1,\n",
        "        \"dropout\": 0.20,\n",
        "        \"learning_rate\": 4e-4,\n",
        "        \"clipnorm\": 1.0,\n",
        "        \"loss\": \"mse\",\n",
        "        \"dense_activation\": \"relu\",\n",
        "        \"output_activation\": \"linear\",\n",
        "        \"epochs\": 50,\n",
        "        \"batch_size\": 4,\n",
        "        \"patience\": 5,\n",
        "        \"random_state\": 42,\n",
        "        \"feature_sets\": [\"neural_40\", \"neural_80\"],\n",
        "    },\n",
        "    # Used by: Blocks 26,27 (GRU model)\n",
        "    \"gru\": {\n",
        "        \"lookback\": 7,\n",
        "        \"stride\": 1,\n",
        "        \"units_1\": 8,\n",
        "        \"units_2\": 4,\n",
        "        \"dense_units\": 1,\n",
        "        \"dropout\": 0.20,\n",
        "        \"learning_rate\": 4e-4,\n",
        "        \"clipnorm\": 1.0,\n",
        "        \"loss\": \"mse\",\n",
        "        \"dense_activation\": \"relu\",\n",
        "        \"output_activation\": \"linear\",\n",
        "        \"epochs\": 50,\n",
        "        \"batch_size\": 4,\n",
        "        \"patience\": 5,\n",
        "        \"random_state\": 42,\n",
        "        \"feature_sets\": [\"neural_40\", \"neural_80\"],\n",
        "    },\n",
        "    # Used by: Blocks 28,29 (Hybrid Sequential)\n",
        "    \"hybrid_seq\": {\n",
        "        \"lookback\": 7,\n",
        "        \"stride\": 1,\n",
        "        \"lstm_units\": 24,\n",
        "        \"gru_units\": 12,\n",
        "        \"dense_units\": 1,\n",
        "        \"dropout\": 0.20,\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"clipnorm\": 1.0,\n",
        "        \"loss\": \"mse\",\n",
        "        \"dense_activation\": \"relu\",\n",
        "        \"output_activation\": \"linear\",\n",
        "        \"epochs\": 50,\n",
        "        \"batch_size\": 4,\n",
        "        \"patience\": 6,\n",
        "        \"random_state\": 42,\n",
        "        \"feature_sets\": [\"neural_40\", \"neural_80\"],\n",
        "    },\n",
        "    # Used by: Blocks 28,29 (Hybrid Parallel)\n",
        "    \"hybrid_par\": {\n",
        "        \"lookback\": 7,\n",
        "        \"stride\": 1,\n",
        "        \"lstm_units\": 16,\n",
        "        \"gru_units\": 16,\n",
        "        \"dense_units\": 1,\n",
        "        \"dropout\": 0.20,\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"clipnorm\": 1.0,\n",
        "        \"loss\": \"mse\",\n",
        "        \"dense_activation\": \"relu\",\n",
        "        \"output_activation\": \"linear\",\n",
        "        \"epochs\": 50,\n",
        "        \"batch_size\": 4,\n",
        "        \"patience\": 6,\n",
        "        \"random_state\": 42,\n",
        "        \"feature_sets\": [\"neural_40\", \"neural_80\"],\n",
        "    },\n",
        "    # --- Ensemble ---\n",
        "    \"ensemble\": {\n",
        "        \"method\": \"weighted_average\",  # simple_average, weighted_average, stacking, rank_average\n",
        "        \"models\": [\"xgb\", \"lgb\", \"lstm\", \"gru\", \"hybrid_seq\", \"hybrid_par\"],\n",
        "        \"filter\": {\n",
        "            \"min_diracc\": 0.55,  # e.g., 0.52 - remove models with DirAcc < threshold\n",
        "            \"max_wrmse\": 0.020,   # e.g., 0.025 - remove models with wRMSE > threshold\n",
        "            \"top_n\": 4,       # e.g., 4 - keep only top N models by wRMSE\n",
        "        },\n",
        "        \"weights\": \"auto\",  # \"auto\" for inverse_wrmse, or dict\n",
        "        \"weight_method\": \"inverse_wrmse_squared\",  # inverse_wrmse_squared gives advantage to stable models\n",
        "        \"meta_model\": \"ridge\",\n",
        "        \"meta_params\": {\n",
        "            \"alpha\": 5.0,  # higher regularization for small data\n",
        "            \"random_state\": 42,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save run params to BOTH local and drive\n",
        "save_json(RUN_PARAMS, LOCAL_PATHS[\"config_dir\"] / \"run_params.json\")\n",
        "save_text(\n",
        "    json.dumps(RUN_PARAMS, indent=2, ensure_ascii=False),\n",
        "    LOCAL_PATHS[\"config_dir\"] / \"run_params.txt\",\n",
        ")\n",
        "save_json(RUN_PARAMS, DRIVE_PATHS[\"config_dir\"] / \"run_params.json\")\n",
        "save_text(\n",
        "    json.dumps(RUN_PARAMS, indent=2, ensure_ascii=False),\n",
        "    DRIVE_PATHS[\"config_dir\"] / \"run_params.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "def save_run_outputs(\n",
        "    metrics: Dict[str, Any],\n",
        "    predictions_valid: Optional[\"pd.DataFrame\"] = None,\n",
        "    predictions_test: Optional[\"pd.DataFrame\"] = None,\n",
        "    extra_artifacts: Optional[Dict[str, Any]] = None,\n",
        "    model: Optional[Any] = None,\n",
        "    model_filename: str = \"model.json\",\n",
        ") -> None:\n",
        "    \"\"\"Save standard run outputs (metrics, predictions, model).\"\"\"\n",
        "    save_json(metrics, OUTPUTS_DIR / \"metrics.json\")\n",
        "    save_text(\n",
        "        \"\\n\".join([f\"{k}: {v}\" for k, v in metrics.items()]),\n",
        "        OUTPUTS_DIR / \"metrics.txt\",\n",
        "    )\n",
        "\n",
        "    if predictions_valid is not None:\n",
        "        predictions_valid.to_csv(OUTPUTS_DIR / \"predictions_valid.csv\", index=True)\n",
        "    if predictions_test is not None:\n",
        "        predictions_test.to_csv(OUTPUTS_DIR / \"predictions_test.csv\", index=True)\n",
        "\n",
        "    if extra_artifacts:\n",
        "        for name, obj in extra_artifacts.items():\n",
        "            save_pickle(obj, OUTPUTS_DIR / f\"{name}.pkl\")\n",
        "\n",
        "    if model is not None:\n",
        "        path = MODELS_DIR / model_filename\n",
        "        if hasattr(model, \"save_model\"):\n",
        "            model.save_model(str(path))\n",
        "        else:\n",
        "            save_pickle(model, MODELS_DIR / (Path(model_filename).stem + \".pkl\"))\n",
        "\n",
        "\n",
        "# --- Code Snapshot ---\n",
        "EXPORT_CODE = True\n",
        "\n",
        "\n",
        "def snapshot_code(project_root: Path, local_run_dir: Path, drive_run_dir: Path) -> None:\n",
        "    \"\"\"Save current notebook to run directories for reproducibility.\"\"\"\n",
        "\n",
        "    # Step 1: Save the notebook first (Colab-specific)\n",
        "    try:\n",
        "        from google.colab import _message\n",
        "        _message.blocking_request('save_notebook', {'save': True})\n",
        "        print(\"[SNAPSHOT] Notebook saved via Colab API\")\n",
        "    except Exception:\n",
        "        print(\"[SNAPSHOT] Could not auto-save notebook (not in Colab or already saved)\")\n",
        "\n",
        "    # Step 2: Find the notebook file\n",
        "    notebook_path = None\n",
        "    search_paths = [\n",
        "        project_root / \"google_stock_ml_unified.ipynb\",\n",
        "        Path(\"/content/google_stock_ml_unified.ipynb\"),\n",
        "        Path(\"/content/my_project/google_stock_ml_unified.ipynb\"),\n",
        "    ]\n",
        "\n",
        "    # Also search for any .ipynb files\n",
        "    for pattern in [\"/content/*.ipynb\", \"/content/my_project/*.ipynb\"]:\n",
        "        for p in glob(pattern):\n",
        "            if \"checkpoint\" not in p.lower():\n",
        "                search_paths.append(Path(p))\n",
        "\n",
        "    for p in search_paths:\n",
        "        if p.exists():\n",
        "            notebook_path = p\n",
        "            break\n",
        "\n",
        "    # Step 3: Copy to both locations\n",
        "    def _save_snapshot(dst_run_dir: Path, location: str):\n",
        "        export_dir = dst_run_dir / \"code_snapshot\"\n",
        "        export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if notebook_path and notebook_path.exists():\n",
        "            dst = export_dir / notebook_path.name\n",
        "            copy_file(notebook_path, dst)\n",
        "\n",
        "            # Save metadata\n",
        "            meta = {\n",
        "                \"run_id\": RUN_ID,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"source_path\": str(notebook_path),\n",
        "            }\n",
        "            save_json(meta, export_dir / \"snapshot_meta.json\")\n",
        "            print(f\"[SNAPSHOT] {location}: Saved {notebook_path.name}\")\n",
        "        else:\n",
        "            save_text(f\"Notebook not found. Searched: {[str(p) for p in search_paths]}\",\n",
        "                     export_dir / \"NO_SNAPSHOT.txt\")\n",
        "            print(f\"[SNAPSHOT] {location}: Notebook not found\")\n",
        "\n",
        "    _save_snapshot(local_run_dir, \"LOCAL\")\n",
        "    _save_snapshot(drive_run_dir, \"DRIVE\")\n",
        "\n",
        "if EXPORT_CODE:\n",
        "    snapshot_code(PROJECT_ROOT, LOCAL_RUN_DIR, DRIVE_RUN_DIR)\n",
        "    save_text(\"Snapshot completed\", LOCAL_PATHS[\"logs_dir\"] / \"export_log.txt\")\n",
        "    save_text(\"Snapshot completed\", DRIVE_PATHS[\"logs_dir\"] / \"export_log.txt\")\n",
        "\n",
        "print(\"[OK] BOOT + BLOCK 0 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d545dfea",
      "metadata": {
        "id": "d545dfea"
      },
      "source": [
        "## BLOCK 1 â€” ENV + IMPORTS (XGB + LSTM/GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "916aaef8",
      "metadata": {
        "id": "916aaef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a80438-1f77-45c9-d639-7e55ff904c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ENV] python: 3.12.12\n",
            "[ENV] numpy: 2.0.2\n",
            "[ENV] pandas: 2.2.2\n",
            "[ENV] yfinance: 0.2.66\n",
            "[ENV] pandas_datareader: unknown\n",
            "[ENV] pandas_market_calendars: 5.2.4\n",
            "[ENV] scipy: 1.16.3\n",
            "[ENV] xgboost: 3.1.2\n",
            "[ENV] tensorflow: OK\n",
            "[ENV] RANDOM_STATE: 42\n",
            "[ENV] TARGET_T1: GOOGL_logret_t1\n",
            "[ENV] EPS: 1e-12\n",
            "[OK] BLOCK 1 complete.\n",
            "[ENV] Metric functions loaded: w_rmse, w_mae, dir_acc\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Colab installs (only if missing)\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        import yfinance  # noqa: F401\n",
        "        import pandas_datareader  # noqa: F401\n",
        "        import pandas_market_calendars  # noqa: F401\n",
        "        import scipy  # noqa: F401\n",
        "    except ImportError:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "             \"yfinance\", \"pandas_datareader\", \"pandas-market-calendars\", \"scipy\"],\n",
        "            check=True\n",
        "        )\n",
        "    try:\n",
        "        import lightgbm  # noqa: F401\n",
        "    except ImportError:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lightgbm\"],\n",
        "            check=True\n",
        "        )\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch  # Required for EDA volatility plots\n",
        "\n",
        "import re\n",
        "import csv\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "import pandas_market_calendars as mcal\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    display = print\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Random state from RUN_PARAMS\n",
        "RANDOM_STATE = int(RUN_PARAMS.get(\"random_state\", 42))\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# TensorFlow (optional)\n",
        "TF_AVAILABLE = False\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    tf.random.set_seed(RANDOM_STATE)\n",
        "    TF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TF_AVAILABLE = False\n",
        "\n",
        "# NN config is defined per-model in RUN_PARAMS[\"lstm\"], [\"gru\"], [\"hybrid_seq\"], [\"hybrid_par\"]\n",
        "\n",
        "print(\"[ENV] python:\", sys.version.split()[0])\n",
        "print(\"[ENV] numpy:\", np.__version__)\n",
        "print(\"[ENV] pandas:\", pd.__version__)\n",
        "print(\"[ENV] yfinance:\", getattr(yf, \"__version__\", \"unknown\"))\n",
        "print(\"[ENV] pandas_datareader:\", getattr(pdr, \"__version__\", \"unknown\"))\n",
        "print(\"[ENV] pandas_market_calendars:\", getattr(mcal, \"__version__\", \"unknown\"))\n",
        "print(\"[ENV] scipy:\", getattr(sys.modules.get(\"scipy\"), \"__version__\", \"unknown\"))\n",
        "print(\"[ENV] xgboost:\", getattr(xgb, \"__version__\", \"unknown\"))\n",
        "print(\"[ENV] tensorflow:\", \"OK\" if TF_AVAILABLE else \"NOT AVAILABLE\")\n",
        "\n",
        "print(\"[ENV] RANDOM_STATE:\", RANDOM_STATE)\n",
        "\n",
        "# NN config is per-model (see RUN_PARAMS[\"lstm\"], [\"gru\"], etc.)\n",
        "\n",
        "\n",
        "# Global constants from config\n",
        "TARGET_T1 = str(RUN_PARAMS[\"data\"][\"target_col\"])\n",
        "EPS = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "print(\"[ENV] TARGET_T1:\", TARGET_T1)\n",
        "print(\"[ENV] EPS:\", EPS)\n",
        "\n",
        "print(\"[OK] BLOCK 1 complete.\")\n",
        "\n",
        "# -------------------------\n",
        "# Shared Metric Functions (used across all model blocks)\n",
        "# -------------------------\n",
        "def _to_np(x):\n",
        "    \"\"\"Convert to numpy array.\"\"\"\n",
        "    return np.asarray(x, dtype=float)\n",
        "\n",
        "\n",
        "def w_rmse(y_true, y_pred, w):\n",
        "    \"\"\"Weighted Root Mean Squared Error (Corrected).\"\"\"\n",
        "    y_true = _to_np(y_true)\n",
        "    y_pred = _to_np(y_pred)\n",
        "    w = _to_np(w)\n",
        "    mse_w = np.sum(w * (y_true - y_pred) ** 2) / (np.sum(w) + EPS)\n",
        "    return float(np.sqrt(mse_w))\n",
        "\n",
        "\n",
        "def w_mae(y_true, y_pred, w):\n",
        "    \"\"\"Weighted Mean Absolute Error (Corrected).\"\"\"\n",
        "    y_true = _to_np(y_true)\n",
        "    y_pred = _to_np(y_pred)\n",
        "    w = _to_np(w)\n",
        "    mae_w = np.sum(w * np.abs(y_true - y_pred)) / (np.sum(w) + EPS)\n",
        "    return float(mae_w)\n",
        "\n",
        "\n",
        "def dir_acc(y_true, y_pred):\n",
        "    \"\"\"Directional Accuracy.\"\"\"\n",
        "    y_true = _to_np(y_true)\n",
        "    y_pred = _to_np(y_pred)\n",
        "    return float(np.mean((y_true > 0) == (y_pred > 0)))\n",
        "\n",
        "\n",
        "print(\"[ENV] Metric functions loaded: w_rmse, w_mae, dir_acc\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wdf1G2W7SAW-",
      "metadata": {
        "id": "Wdf1G2W7SAW-"
      },
      "source": [
        "---\n",
        "# SECTION 2: Data Ingestion & Preprocessing\n",
        "\n",
        "**Download data, feature engineering, interim processing**\n",
        "\n",
        "**Blocks:** 2-16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0df8e9",
      "metadata": {
        "id": "2b0df8e9"
      },
      "source": [
        "## BLOCK 2 â€” LOAD PRICES + EARNINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "aeb33505",
      "metadata": {
        "id": "aeb33505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c814a72-e353-4ad1-88f5-45c53200ef6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FEATURE INFO SUMMARY ===\n",
            "               feature   dtype  non_null  null  null_pct  unique\n",
            "           GOOGL_Close float64       540     0  0.000000     537\n",
            "            GOOGL_High float64       540     0  0.000000     539\n",
            "             GOOGL_Low float64       540     0  0.000000     539\n",
            "            GOOGL_Open float64       540     0  0.000000     539\n",
            "            MSFT_Close float64       540     0  0.000000     537\n",
            "             MSFT_High float64       540     0  0.000000     539\n",
            "              MSFT_Low float64       540     0  0.000000     538\n",
            "             MSFT_Open float64       540     0  0.000000     537\n",
            "            NVDA_Close float64       540     0  0.000000     533\n",
            "             NVDA_High float64       540     0  0.000000     539\n",
            "              NVDA_Low float64       540     0  0.000000     539\n",
            "             NVDA_Open float64       540     0  0.000000     539\n",
            "             QQQ_Close float64       540     0  0.000000     534\n",
            "              QQQ_High float64       540     0  0.000000     539\n",
            "               QQQ_Low float64       540     0  0.000000     539\n",
            "              QQQ_Open float64       540     0  0.000000     539\n",
            "             SPY_Close float64       540     0  0.000000     539\n",
            "              SPY_High float64       540     0  0.000000     538\n",
            "               SPY_Low float64       540     0  0.000000     539\n",
            "              SPY_Open float64       540     0  0.000000     539\n",
            "             XLK_Close float64       540     0  0.000000     534\n",
            "              XLK_High float64       540     0  0.000000     538\n",
            "               XLK_Low float64       540     0  0.000000     539\n",
            "              XLK_Open float64       540     0  0.000000     538\n",
            "          ^GDAXI_Close float64       540     0  0.000000     528\n",
            "           ^GDAXI_High float64       540     0  0.000000     528\n",
            "            ^GDAXI_Low float64       540     0  0.000000     528\n",
            "           ^GDAXI_Open float64       540     0  0.000000     528\n",
            "           ^IXIC_Close float64       540     0  0.000000     539\n",
            "            ^IXIC_High float64       540     0  0.000000     539\n",
            "             ^IXIC_Low float64       540     0  0.000000     539\n",
            "            ^IXIC_Open float64       540     0  0.000000     539\n",
            "            ^TNX_Close float64       540     0  0.000000     398\n",
            "             ^TNX_High float64       540     0  0.000000     391\n",
            "              ^TNX_Low float64       540     0  0.000000     365\n",
            "             ^TNX_Open float64       540     0  0.000000     384\n",
            "            ^VIX_Close float64       540     0  0.000000     426\n",
            "             ^VIX_High float64       540     0  0.000000     426\n",
            "              ^VIX_Low float64       540     0  0.000000     415\n",
            "             ^VIX_Open float64       540     0  0.000000     407\n",
            " has_eps_surprise_calc    int8       540     0  0.000000       2\n",
            "has_eps_surprise_yahoo    int8       540     0  0.000000       2\n",
            "       is_earnings_day    int8       540     0  0.000000       2\n",
            "          GOOGL_Volume float64       539     1  0.185185     539\n",
            "           MSFT_Volume float64       539     1  0.185185     539\n",
            "           NVDA_Volume float64       539     1  0.185185     539\n",
            "            QQQ_Volume float64       539     1  0.185185     539\n",
            "            SPY_Volume float64       539     1  0.185185     539\n",
            "            XLK_Volume float64       539     1  0.185185     538\n",
            "          ^IXIC_Volume float64       539     1  0.185185     539\n",
            " eps_surprise_pct_calc float64         8   532 98.518519       8\n",
            "eps_surprise_pct_yahoo float64         8   532 98.518519       8\n",
            "\n",
            "=== FEATURE FREQUENCY SUMMARY ===\n",
            "               feature   dtype  n_total  n_nan  n_nonnull  n_zero  n_nonzero   pct_nan  pct_nonzero  pct_nonzero_of_nonnull\n",
            "           GOOGL_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            GOOGL_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             GOOGL_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            GOOGL_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            MSFT_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             MSFT_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              MSFT_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             MSFT_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            NVDA_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             NVDA_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              NVDA_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             NVDA_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             QQQ_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              QQQ_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "               QQQ_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              QQQ_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             SPY_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              SPY_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "               SPY_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              SPY_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             XLK_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              XLK_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "               XLK_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              XLK_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "          ^GDAXI_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "           ^GDAXI_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            ^GDAXI_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "           ^GDAXI_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "           ^IXIC_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            ^IXIC_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             ^IXIC_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            ^IXIC_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            ^TNX_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             ^TNX_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              ^TNX_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             ^TNX_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "            ^VIX_Close float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             ^VIX_High float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "              ^VIX_Low float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            "             ^VIX_Open float64      540      0        540       0        540  0.000000   100.000000              100.000000\n",
            " has_eps_surprise_calc    int8      540      0        540     532          8  0.000000     1.481481                1.481481\n",
            "has_eps_surprise_yahoo    int8      540      0        540     532          8  0.000000     1.481481                1.481481\n",
            "       is_earnings_day    int8      540      0        540     532          8  0.000000     1.481481                1.481481\n",
            "          GOOGL_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            "           MSFT_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            "           NVDA_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            "            QQQ_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            "            SPY_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            "            XLK_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            "          ^IXIC_Volume float64      540      1        539       0        539  0.185185    99.814815              100.000000\n",
            " eps_surprise_pct_calc float64      540    532          8       0          8 98.518519     1.481481              100.000000\n",
            "eps_surprise_pct_yahoo float64      540    532          8       0          8 98.518519     1.481481              100.000000\n",
            "[OK] BLOCK 2 complete. full_df shape: (540, 52)\n"
          ]
        }
      ],
      "source": [
        "# Date range\n",
        "start = RUN_PARAMS[\"data\"][\"start_date\"]\n",
        "end = RUN_PARAMS[\"data\"].get(\"end_date\") or datetime.now().strftime(\"%Y-%m-%d\")\n",
        "RUN_PARAMS[\"data\"][\"end_date\"] = end\n",
        "\n",
        "# --- 1. Define official calendar (NASDAQ) to avoid relying on Yahoo alone ---\n",
        "nyse = mcal.get_calendar('NASDAQ')\n",
        "valid_days = nyse.valid_days(start_date=start, end_date=end)\n",
        "master_index = pd.Index(valid_days.tz_localize(None).normalize(), name=\"Date\")\n",
        "\n",
        "# Price tickers (includes GDAXI in main list)\n",
        "price_tickers = [\n",
        "    \"GOOGL\", \"MSFT\", \"NVDA\",\n",
        "    \"^IXIC\", \"SPY\", \"QQQ\",\n",
        "    \"^VIX\", \"^TNX\",\n",
        "    \"XLK\", \"^GDAXI\"\n",
        "]\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "# Download price data\n",
        "for t in price_tickers:\n",
        "    df = yf.download(t, start=start, end=end, auto_adjust=True, progress=False)\n",
        "    if df is None or df.empty:\n",
        "        continue\n",
        "\n",
        "    df.columns = [f\"{t}_{c[0] if isinstance(c, tuple) else c}\" for c in df.columns]\n",
        "\n",
        "    if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    data_dict[t] = df\n",
        "\n",
        "# --- 2. Build Master Table aligned to official calendar ---\n",
        "if \"GOOGL\" not in data_dict:\n",
        "    raise ValueError(\"GOOGL data is missing. Cannot build master timeline.\")\n",
        "\n",
        "# Use master_index as base for all data\n",
        "prices_all = pd.DataFrame(index=master_index)\n",
        "\n",
        "for t, df in data_dict.items():\n",
        "    # Left join to calendar ensures we don't miss official trading days\n",
        "    prices_all = prices_all.join(df, how=\"left\")\n",
        "\n",
        "# Forward fill prices only\n",
        "price_cols = [c for c in prices_all.columns if any(s in c for s in ['_Open', '_High', '_Low', '_Close'])]\n",
        "prices_all[price_cols] = prices_all.sort_index()[price_cols].ffill()\n",
        "\n",
        "# Earnings data (GOOGL)\n",
        "tkr = yf.Ticker(\"GOOGL\")\n",
        "edf = tkr.get_earnings_dates(limit=100)\n",
        "\n",
        "earnings = pd.DataFrame(index=prices_all.index)\n",
        "earnings[\"is_earnings_day\"] = 0\n",
        "\n",
        "# Pre-create columns\n",
        "earnings[\"eps_surprise_pct_yahoo\"] = np.nan\n",
        "earnings[\"has_eps_surprise_yahoo\"] = 0\n",
        "earnings[\"eps_surprise_pct_calc\"] = np.nan\n",
        "earnings[\"has_eps_surprise_calc\"] = 0\n",
        "\n",
        "if edf is not None and len(edf) > 0:\n",
        "    edf = edf.copy()\n",
        "    idx = pd.to_datetime(edf.index)\n",
        "    if getattr(idx, \"tz\", None) is not None:\n",
        "        idx = idx.tz_convert(None)\n",
        "    idx = idx.normalize()\n",
        "\n",
        "    edf.index = idx\n",
        "    edf = edf[~edf.index.duplicated(keep=\"last\")].sort_index()\n",
        "\n",
        "    cols_lower = {c.lower(): c for c in edf.columns}\n",
        "\n",
        "    def pick_col(possible_names):\n",
        "        for name in possible_names:\n",
        "            key = name.lower()\n",
        "            if key in cols_lower:\n",
        "                return cols_lower[key]\n",
        "        return None\n",
        "\n",
        "    col_exp = pick_col([\"EPS Estimate\", \"eps estimate\", \"Eps Estimate\"])\n",
        "    col_act = pick_col([\"Reported EPS\", \"reported eps\", \"EPS Actual\", \"eps actual\"])\n",
        "    col_pct = pick_col([\"Surprise(%)\", \"surprise(%)\", \"Surprise (%)\", \"surprise (%)\"])\n",
        "\n",
        "    eps_daily = pd.DataFrame(index=edf.index)\n",
        "    eps_daily[\"eps_expected\"] = edf[col_exp] if col_exp else np.nan\n",
        "    eps_daily[\"eps_actual\"] = edf[col_act] if col_act else np.nan\n",
        "\n",
        "    if col_exp and col_act:\n",
        "        eps_surprise = eps_daily[\"eps_actual\"] - eps_daily[\"eps_expected\"]\n",
        "        denom = eps_daily[\"eps_expected\"].abs()\n",
        "        eps_daily[\"eps_surprise_pct_calc\"] = np.where(denom > 0, 100.0 * (eps_surprise / denom), np.nan)\n",
        "    else:\n",
        "        eps_daily[\"eps_surprise_pct_calc\"] = np.nan\n",
        "\n",
        "    eps_daily[\"eps_surprise_pct_yahoo\"] = edf[col_pct] if col_pct else np.nan\n",
        "    data_dict[\"EARNINGS_EPS_DEBUG\"] = eps_daily.copy()\n",
        "\n",
        "    # Align to official trading days in prices_all\n",
        "    eps_on_trading_days = eps_daily.reindex(prices_all.index)\n",
        "\n",
        "    earnings[\"is_earnings_day\"] = prices_all.index.isin(eps_daily.index).astype(\"int8\")\n",
        "    earnings[\"eps_surprise_pct_yahoo\"] = eps_on_trading_days[\"eps_surprise_pct_yahoo\"].values\n",
        "    earnings[\"eps_surprise_pct_calc\"] = eps_on_trading_days[\"eps_surprise_pct_calc\"].values\n",
        "    earnings[\"has_eps_surprise_yahoo\"] = earnings[\"eps_surprise_pct_yahoo\"].notna().astype(\"int8\")\n",
        "    earnings[\"has_eps_surprise_calc\"] = earnings[\"eps_surprise_pct_calc\"].notna().astype(\"int8\")\n",
        "\n",
        "# Merge prices + earnings\n",
        "full_df = prices_all.join(earnings, how=\"left\")\n",
        "\n",
        "# Drop low-information volume columns\n",
        "DROP_VOLUME_TICKERS = RUN_PARAMS[\"features\"].get(\"drop_volume_tickers\", [])\n",
        "DROP_VOLUME_COLS = [f\"{t}_Volume\" for t in DROP_VOLUME_TICKERS]\n",
        "full_df = full_df.drop(columns=[c for c in DROP_VOLUME_COLS if c in full_df.columns])\n",
        "\n",
        "# --- Summary functions (unchanged) ---\n",
        "def feature_info(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return (\n",
        "        pd.DataFrame({\n",
        "            \"feature\": df.columns,\n",
        "            \"dtype\": [df[c].dtype for c in df.columns],\n",
        "            \"non_null\": [int(df[c].notna().sum()) for c in df.columns],\n",
        "            \"null\": [int(df[c].isna().sum()) for c in df.columns],\n",
        "            \"null_pct\": [float(df[c].isna().mean() * 100.0) for c in df.columns],\n",
        "            \"unique\": [int(df[c].nunique(dropna=True)) for c in df.columns],\n",
        "        })\n",
        "        .sort_values([\"null_pct\", \"feature\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "def frequency_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "    for c in df.columns:\n",
        "        s = df[c]\n",
        "        n_nan = int(s.isna().sum())\n",
        "        n_nonnull = int(s.notna().sum())\n",
        "        if pd.api.types.is_numeric_dtype(s):\n",
        "            s_nn = s.dropna()\n",
        "            n_zero = int((s_nn == 0).sum())\n",
        "            n_nonzero = int((s_nn != 0).sum())\n",
        "            pct_nonzero_of_nonnull = (100.0 * n_nonzero / n_nonnull) if n_nonnull else np.nan\n",
        "        else:\n",
        "            n_zero = 0\n",
        "            n_nonzero = n_nonnull\n",
        "            pct_nonzero_of_nonnull = (100.0 * n_nonzero / n_nonnull) if n_nonnull else np.nan\n",
        "        rows.append({\n",
        "            \"feature\": c, \"dtype\": str(s.dtype), \"n_total\": n, \"n_nan\": n_nan,\n",
        "            \"n_nonnull\": n_nonnull, \"n_zero\": n_zero, \"n_nonzero\": n_nonzero,\n",
        "            \"pct_nan\": (100.0 * n_nan / n) if n else np.nan,\n",
        "            \"pct_nonzero\": (100.0 * n_nonzero / n) if n else np.nan,\n",
        "            \"pct_nonzero_of_nonnull\": pct_nonzero_of_nonnull,\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values([\"pct_nan\", \"feature\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== FEATURE INFO SUMMARY ===\")\n",
        "print(feature_info(full_df).to_string(index=False))\n",
        "\n",
        "print(\"\\n=== FEATURE FREQUENCY SUMMARY ===\")\n",
        "print(frequency_summary(full_df).to_string(index=False))\n",
        "\n",
        "print(\"[OK] BLOCK 2 complete. full_df shape:\", full_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8SuYKygjSAW_",
      "metadata": {
        "id": "8SuYKygjSAW_"
      },
      "source": [
        "## BLOCK 2B â€” EU BREAK CLOSE FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "5a0XuG0FSAW_",
      "metadata": {
        "id": "5a0XuG0FSAW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5430fdce-923a-4078-d034-fcfd607531e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Building EU break close flags...\n",
            "[INFO] EU break close events: 16\n",
            "[INFO] EU break close up: 10\n",
            "[INFO] EU break close down: 6\n",
            "[OK] BLOCK 2B complete. full_df shape: (540, 55)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# EU \"GAP\" FLAGS (US Closed, EU Open)\n",
        "# =========================\n",
        "def build_eu_info_gap_flags(\n",
        "    full_df: pd.DataFrame,\n",
        "    eu_ticker: str = \"^GDAXI\",\n",
        "    apply_to: str = \"next_us_trading_day\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        - EU_break_close_flag (int8): 1 if US was closed previously while EU was open.\n",
        "        - EU_break_close_up   (int8): 1 if EU cumulative return during US holiday was positive.\n",
        "        - EU_break_close_down (int8): 1 if EU cumulative return during US holiday was negative.\n",
        "    \"\"\"\n",
        "    # 1. Get EU data from data_dict downloaded in block 2\n",
        "    if eu_ticker not in data_dict:\n",
        "        raise ValueError(f\"{eu_ticker} missing from data_dict. Ensure Block 2 ran correctly.\")\n",
        "\n",
        "    eu_data = data_dict[eu_ticker].copy()\n",
        "    eu_days = eu_data.index.normalize()\n",
        "    us_days = full_df.index.normalize()\n",
        "\n",
        "    # 2. Identify gap days: Europe open, US closed\n",
        "    gap_days = eu_days.difference(us_days)\n",
        "\n",
        "    # 3. Compute log returns (allows summing over consecutive holiday days)\n",
        "    close_col = f\"{eu_ticker}_Close\"\n",
        "    eu_log_ret = np.log(eu_data[close_col] / eu_data[close_col].shift(1))\n",
        "\n",
        "    # 4. Create events table for gap days\n",
        "    eu_gap_events = pd.DataFrame(index=gap_days)\n",
        "    eu_gap_events[\"gap_return\"] = eu_log_ret.reindex(gap_days)\n",
        "\n",
        "    # 5. Map to next US trading day\n",
        "    # searchsorted(side='left') finds first US index >= holiday day\n",
        "    pos = np.searchsorted(us_days, eu_gap_events.index, side=\"left\")\n",
        "    valid_mask = pos < len(us_days)\n",
        "\n",
        "    eu_gap_events['target_us_date'] = pd.NaT  # NaT for datetime instead of np.nan\n",
        "    eu_gap_events.loc[valid_mask, 'target_us_date'] = us_days[pos[valid_mask]]\n",
        "\n",
        "    # 6. Aggregate (for long holidays, sum all EU returns to US opening day)\n",
        "    agg_gap = eu_gap_events.dropna(subset=['target_us_date']).groupby('target_us_date')[\"gap_return\"].sum()\n",
        "\n",
        "    # 7. Create output table in original format\n",
        "    out = pd.DataFrame(index=us_days)\n",
        "    out[\"EU_break_close_flag\"] = np.int8(0)\n",
        "    out[\"EU_break_close_up\"] = np.int8(0)\n",
        "    out[\"EU_break_close_down\"] = np.int8(0)\n",
        "\n",
        "    out.loc[agg_gap.index, \"EU_break_close_flag\"] = 1\n",
        "    out.loc[agg_gap.index, \"EU_break_close_up\"] = (agg_gap > 0).astype(\"int8\")\n",
        "    out.loc[agg_gap.index, \"EU_break_close_down\"] = (agg_gap < 0).astype(\"int8\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# =========================\n",
        "# USAGE\n",
        "# =========================\n",
        "EU_CFG = RUN_PARAMS.get(\"eu_break_close\", {})\n",
        "EU_ENABLED = bool(EU_CFG.get(\"enabled\", True))\n",
        "\n",
        "if EU_ENABLED:\n",
        "    print(\"[INFO] Building EU break close flags...\")\n",
        "\n",
        "    # Function now receives updated full_df\n",
        "    eu_flags = build_eu_info_gap_flags(\n",
        "        full_df=full_df,\n",
        "        eu_ticker=EU_CFG.get(\"eu_ticker\", \"^GDAXI\")\n",
        "    )\n",
        "\n",
        "    # Join - align to original full_df index\n",
        "    eu_flags.index = full_df.index\n",
        "\n",
        "    # Remove existing columns if block runs again\n",
        "    cols_to_drop = [c for c in eu_flags.columns if c in full_df.columns]\n",
        "    if cols_to_drop:\n",
        "        full_df = full_df.drop(columns=cols_to_drop)\n",
        "\n",
        "    full_df = full_df.join(eu_flags, how=\"left\")\n",
        "\n",
        "    # Fill NaN with 0 for flag columns\n",
        "    for col in [\"EU_break_close_flag\", \"EU_break_close_up\", \"EU_break_close_down\"]:\n",
        "        if col in full_df.columns:\n",
        "            full_df[col] = full_df[col].fillna(0).astype(\"int8\")\n",
        "\n",
        "    n_events = full_df[\"EU_break_close_flag\"].sum()\n",
        "    print(f\"[INFO] EU break close events: {n_events}\")\n",
        "    print(f\"[INFO] EU break close up: {full_df['EU_break_close_up'].sum()}\")\n",
        "    print(f\"[INFO] EU break close down: {full_df['EU_break_close_down'].sum()}\")\n",
        "    print(f\"[OK] BLOCK 2B complete. full_df shape: {full_df.shape}\")\n",
        "else:\n",
        "    print(\"[SKIP] BLOCK 2B â€” EU break close disabled in config.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b615080",
      "metadata": {
        "id": "9b615080"
      },
      "source": [
        "## BLOCK 3 â€” MACRO FEATURES (FRED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "f2809eaa",
      "metadata": {
        "id": "f2809eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcef9324-0750-4996-c37a-3bddb8fdc4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] full_df range: 2023-11-20 00:00:00 -> 2026-01-15 00:00:00 | rows: 540\n",
            "[INFO] FRED pull range: 2023-11-20 00:00:00 -> 2026-01-15 00:00:00\n",
            "[OK] EPS surprise handled: eps_surprise_pct_* filled with 0 where has_* == 0 (flags preserved).\n",
            "\n",
            "[INFO] Monthly CPI feats head:\n",
            "             CPI_pct_mom  CPI_accel_pct_mom\n",
            "DATE                                      \n",
            "2023-12-01          NaN                NaN\n",
            "2024-01-01     0.003430                NaN\n",
            "2024-02-01     0.003964           0.000534\n",
            "2024-03-01     0.003488          -0.000475\n",
            "2024-04-01     0.002912          -0.000576\n",
            "2024-05-01     0.000396          -0.002516\n",
            "\n",
            "[INFO] Monthly FEDFUNDS feats head:\n",
            "             FEDFUNDS_delta_mom  FEDFUNDS_changed  FEDFUNDS_level\n",
            "DATE                                                            \n",
            "2023-12-01                 NaN                 0            5.33\n",
            "2024-01-01                 0.0                 0            5.33\n",
            "2024-02-01                 0.0                 0            5.33\n",
            "2024-03-01                 0.0                 0            5.33\n",
            "2024-04-01                 0.0                 0            5.33\n",
            "2024-05-01                 0.0                 0            5.33\n",
            "\n",
            "[CHECK] Macro columns (including flags + release): ['CPI_pct_mom', 'CPI_accel_pct_mom', 'FEDFUNDS_delta_mom', 'FEDFUNDS_changed', 'FEDFUNDS_level', 'CPI_pct_mom_is_missing', 'CPI_accel_pct_mom_is_missing', 'FEDFUNDS_delta_mom_is_missing', 'CPI_release_day', 'FEDFUNDS_release_day']\n",
            "\n",
            "[CHECK] Macro head (aligned to trading dates):\n",
            "             CPI_pct_mom  CPI_accel_pct_mom  FEDFUNDS_delta_mom  \\\n",
            "Date                                                             \n",
            "2023-11-20          0.0                0.0                 0.0   \n",
            "2023-11-21          0.0                0.0                 0.0   \n",
            "2023-11-22          0.0                0.0                 0.0   \n",
            "2023-11-24          0.0                0.0                 0.0   \n",
            "2023-11-27          0.0                0.0                 0.0   \n",
            "2023-11-28          0.0                0.0                 0.0   \n",
            "2023-11-29          0.0                0.0                 0.0   \n",
            "2023-11-30          0.0                0.0                 0.0   \n",
            "2023-12-01          0.0                0.0                 0.0   \n",
            "2023-12-04          0.0                0.0                 0.0   \n",
            "2023-12-05          0.0                0.0                 0.0   \n",
            "2023-12-06          0.0                0.0                 0.0   \n",
            "2023-12-07          0.0                0.0                 0.0   \n",
            "2023-12-08          0.0                0.0                 0.0   \n",
            "2023-12-11          0.0                0.0                 0.0   \n",
            "\n",
            "            FEDFUNDS_changed  FEDFUNDS_level  CPI_pct_mom_is_missing  \\\n",
            "Date                                                                   \n",
            "2023-11-20                 0             NaN                       1   \n",
            "2023-11-21                 0             NaN                       1   \n",
            "2023-11-22                 0             NaN                       1   \n",
            "2023-11-24                 0             NaN                       1   \n",
            "2023-11-27                 0             NaN                       1   \n",
            "2023-11-28                 0             NaN                       1   \n",
            "2023-11-29                 0             NaN                       1   \n",
            "2023-11-30                 0             NaN                       1   \n",
            "2023-12-01                 0            5.33                       1   \n",
            "2023-12-04                 0            5.33                       1   \n",
            "2023-12-05                 0            5.33                       1   \n",
            "2023-12-06                 0            5.33                       1   \n",
            "2023-12-07                 0            5.33                       1   \n",
            "2023-12-08                 0            5.33                       1   \n",
            "2023-12-11                 0            5.33                       1   \n",
            "\n",
            "            CPI_accel_pct_mom_is_missing  FEDFUNDS_delta_mom_is_missing  \\\n",
            "Date                                                                      \n",
            "2023-11-20                             1                              1   \n",
            "2023-11-21                             1                              1   \n",
            "2023-11-22                             1                              1   \n",
            "2023-11-24                             1                              1   \n",
            "2023-11-27                             1                              1   \n",
            "2023-11-28                             1                              1   \n",
            "2023-11-29                             1                              1   \n",
            "2023-11-30                             1                              1   \n",
            "2023-12-01                             1                              1   \n",
            "2023-12-04                             1                              1   \n",
            "2023-12-05                             1                              1   \n",
            "2023-12-06                             1                              1   \n",
            "2023-12-07                             1                              1   \n",
            "2023-12-08                             1                              1   \n",
            "2023-12-11                             1                              1   \n",
            "\n",
            "            CPI_release_day  FEDFUNDS_release_day  \n",
            "Date                                               \n",
            "2023-11-20                1                     0  \n",
            "2023-11-21                0                     0  \n",
            "2023-11-22                0                     0  \n",
            "2023-11-24                0                     0  \n",
            "2023-11-27                0                     0  \n",
            "2023-11-28                0                     0  \n",
            "2023-11-29                0                     0  \n",
            "2023-11-30                0                     0  \n",
            "2023-12-01                0                     1  \n",
            "2023-12-04                0                     0  \n",
            "2023-12-05                0                     0  \n",
            "2023-12-06                0                     0  \n",
            "2023-12-07                0                     0  \n",
            "2023-12-08                0                     0  \n",
            "2023-12-11                0                     0  \n",
            "\n",
            "[CHECK] Macro NaNs count:\n",
            " CPI_pct_mom                      0\n",
            "CPI_accel_pct_mom                0\n",
            "FEDFUNDS_delta_mom               0\n",
            "FEDFUNDS_changed                 0\n",
            "FEDFUNDS_level                   8\n",
            "CPI_pct_mom_is_missing           0\n",
            "CPI_accel_pct_mom_is_missing     0\n",
            "FEDFUNDS_delta_mom_is_missing    0\n",
            "CPI_release_day                  0\n",
            "FEDFUNDS_release_day             0\n",
            "dtype: int64\n",
            "\n",
            "[CHECK] FEDFUNDS_changed dtype: int8\n",
            "\n",
            "[CHECK] FEDFUNDS_changed value counts:\n",
            " FEDFUNDS_changed\n",
            "0    341\n",
            "1    199\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[CHECK] CPI_release_day value counts:\n",
            " CPI_release_day\n",
            "0    517\n",
            "1     23\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[CHECK] FEDFUNDS_release_day value counts:\n",
            " FEDFUNDS_release_day\n",
            "0    530\n",
            "1     10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[CHECK] EPS eps_surprise_pct_yahoo: na_total=0 | na_when_has_eps_surprise_yahoo==0 => 0\n",
            "\n",
            "[CHECK] EPS eps_surprise_pct_calc: na_total=0 | na_when_has_eps_surprise_calc==0 => 0\n",
            "\n",
            "[CHECK] full_df_merged info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 540 entries, 2023-11-20 to 2026-01-15\n",
            "Data columns (total 65 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   GOOGL_Close                    540 non-null    float64\n",
            " 1   GOOGL_High                     540 non-null    float64\n",
            " 2   GOOGL_Low                      540 non-null    float64\n",
            " 3   GOOGL_Open                     540 non-null    float64\n",
            " 4   GOOGL_Volume                   539 non-null    float64\n",
            " 5   MSFT_Close                     540 non-null    float64\n",
            " 6   MSFT_High                      540 non-null    float64\n",
            " 7   MSFT_Low                       540 non-null    float64\n",
            " 8   MSFT_Open                      540 non-null    float64\n",
            " 9   MSFT_Volume                    539 non-null    float64\n",
            " 10  NVDA_Close                     540 non-null    float64\n",
            " 11  NVDA_High                      540 non-null    float64\n",
            " 12  NVDA_Low                       540 non-null    float64\n",
            " 13  NVDA_Open                      540 non-null    float64\n",
            " 14  NVDA_Volume                    539 non-null    float64\n",
            " 15  ^IXIC_Close                    540 non-null    float64\n",
            " 16  ^IXIC_High                     540 non-null    float64\n",
            " 17  ^IXIC_Low                      540 non-null    float64\n",
            " 18  ^IXIC_Open                     540 non-null    float64\n",
            " 19  ^IXIC_Volume                   539 non-null    float64\n",
            " 20  SPY_Close                      540 non-null    float64\n",
            " 21  SPY_High                       540 non-null    float64\n",
            " 22  SPY_Low                        540 non-null    float64\n",
            " 23  SPY_Open                       540 non-null    float64\n",
            " 24  SPY_Volume                     539 non-null    float64\n",
            " 25  QQQ_Close                      540 non-null    float64\n",
            " 26  QQQ_High                       540 non-null    float64\n",
            " 27  QQQ_Low                        540 non-null    float64\n",
            " 28  QQQ_Open                       540 non-null    float64\n",
            " 29  QQQ_Volume                     539 non-null    float64\n",
            " 30  ^VIX_Close                     540 non-null    float64\n",
            " 31  ^VIX_High                      540 non-null    float64\n",
            " 32  ^VIX_Low                       540 non-null    float64\n",
            " 33  ^VIX_Open                      540 non-null    float64\n",
            " 34  ^TNX_Close                     540 non-null    float64\n",
            " 35  ^TNX_High                      540 non-null    float64\n",
            " 36  ^TNX_Low                       540 non-null    float64\n",
            " 37  ^TNX_Open                      540 non-null    float64\n",
            " 38  XLK_Close                      540 non-null    float64\n",
            " 39  XLK_High                       540 non-null    float64\n",
            " 40  XLK_Low                        540 non-null    float64\n",
            " 41  XLK_Open                       540 non-null    float64\n",
            " 42  XLK_Volume                     539 non-null    float64\n",
            " 43  ^GDAXI_Close                   540 non-null    float64\n",
            " 44  ^GDAXI_High                    540 non-null    float64\n",
            " 45  ^GDAXI_Low                     540 non-null    float64\n",
            " 46  ^GDAXI_Open                    540 non-null    float64\n",
            " 47  is_earnings_day                540 non-null    int8   \n",
            " 48  eps_surprise_pct_yahoo         540 non-null    float64\n",
            " 49  has_eps_surprise_yahoo         540 non-null    int8   \n",
            " 50  eps_surprise_pct_calc          540 non-null    float64\n",
            " 51  has_eps_surprise_calc          540 non-null    int8   \n",
            " 52  EU_break_close_flag            540 non-null    int8   \n",
            " 53  EU_break_close_up              540 non-null    int8   \n",
            " 54  EU_break_close_down            540 non-null    int8   \n",
            " 55  CPI_pct_mom                    540 non-null    float64\n",
            " 56  CPI_accel_pct_mom              540 non-null    float64\n",
            " 57  FEDFUNDS_delta_mom             540 non-null    float64\n",
            " 58  FEDFUNDS_changed               540 non-null    int8   \n",
            " 59  FEDFUNDS_level                 532 non-null    float64\n",
            " 60  CPI_pct_mom_is_missing         540 non-null    int8   \n",
            " 61  CPI_accel_pct_mom_is_missing   540 non-null    int8   \n",
            " 62  FEDFUNDS_delta_mom_is_missing  540 non-null    int8   \n",
            " 63  CPI_release_day                540 non-null    int8   \n",
            " 64  FEDFUNDS_release_day           540 non-null    int8   \n",
            "dtypes: float64(53), int8(12)\n",
            "memory usage: 234.1 KB\n",
            "None\n",
            "[OK] BLOCK 3 complete. full_df shape: (540, 65)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preconditions\n",
        "assert \"full_df\" in globals(), \"[ERROR] full_df is not defined.\"\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "# Use RUN_PARAMS dates (set in Block 2)\n",
        "start = pd.to_datetime(RUN_PARAMS[\"data\"][\"start_date\"])\n",
        "end = pd.to_datetime(RUN_PARAMS[\"data\"][\"end_date\"])\n",
        "\n",
        "print(\"[INFO] full_df range:\", full_df.index.min(), \"->\", full_df.index.max(), \"| rows:\", len(full_df))\n",
        "print(\"[INFO] FRED pull range:\", start, \"->\", end)\n",
        "\n",
        "# --- EPS surprise: flag + fill-0 ---\n",
        "eps_pairs = [\n",
        "    (\"eps_surprise_pct_yahoo\", \"has_eps_surprise_yahoo\"),\n",
        "    (\"eps_surprise_pct_calc\", \"has_eps_surprise_calc\"),\n",
        "]\n",
        "for val_col, flag_col in eps_pairs:\n",
        "    if val_col in full_df.columns and flag_col in full_df.columns:\n",
        "        mask_fill0 = (full_df[flag_col] == 0)\n",
        "        full_df.loc[mask_fill0, val_col] = full_df.loc[mask_fill0, val_col].fillna(0.0)\n",
        "\n",
        "        still_na_when_flag0 = int(full_df.loc[full_df[flag_col] == 0, val_col].isna().sum())\n",
        "        assert still_na_when_flag0 == 0, f\"[ERROR] {val_col} still has NaN where {flag_col}==0\"\n",
        "\n",
        "print(\"[OK] EPS surprise handled: eps_surprise_pct_* filled with 0 where has_* == 0 (flags preserved).\")\n",
        "\n",
        "# --- Pull monthly series from FRED ---\n",
        "cpi = pdr.DataReader(\"CPIAUCSL\", \"fred\", start, end).rename(columns={\"CPIAUCSL\": \"CPI\"})\n",
        "rate = pdr.DataReader(\"FEDFUNDS\", \"fred\", start, end).rename(columns={\"FEDFUNDS\": \"FEDFUNDS\"})\n",
        "\n",
        "cpi.index = pd.to_datetime(cpi.index)\n",
        "rate.index = pd.to_datetime(rate.index)\n",
        "\n",
        "# Compute MONTHLY features (BEFORE daily ffill)\n",
        "\n",
        "# CPI features\n",
        "cpi[\"CPI_pct_mom\"] = cpi[\"CPI\"].pct_change(1, fill_method=None)\n",
        "cpi[\"CPI_accel_pct_mom\"] = cpi[\"CPI_pct_mom\"] - cpi[\"CPI_pct_mom\"].shift(1)\n",
        "cpi_feats_monthly = cpi[[\"CPI_pct_mom\", \"CPI_accel_pct_mom\"]].copy()\n",
        "\n",
        "# FEDFUNDS features\n",
        "rate[\"FEDFUNDS_delta_mom\"] = rate[\"FEDFUNDS\"].diff(1)\n",
        "rate[\"FEDFUNDS_changed\"] = (rate[\"FEDFUNDS_delta_mom\"].fillna(0) != 0).astype(\"int8\")\n",
        "rate[\"FEDFUNDS_level\"] = rate[\"FEDFUNDS\"].copy()\n",
        "rate_feats_monthly = rate[[\"FEDFUNDS_delta_mom\", \"FEDFUNDS_changed\", \"FEDFUNDS_level\"]].copy()\n",
        "\n",
        "print(\"\\n[INFO] Monthly CPI feats head:\\n\", cpi_feats_monthly.head(6))\n",
        "print(\"\\n[INFO] Monthly FEDFUNDS feats head:\\n\", rate_feats_monthly.head(6))\n",
        "\n",
        "# Upsample FEATURES to daily and forward-fill (calendar daily)\n",
        "cpi_feats_daily = cpi_feats_monthly.resample(\"D\").ffill()\n",
        "rate_feats_daily = rate_feats_monthly.resample(\"D\").ffill()\n",
        "macro_daily = pd.concat([cpi_feats_daily, rate_feats_daily], axis=1)\n",
        "\n",
        "# Align macro index to full_df (exact same trading dates)\n",
        "macro_daily.index = pd.to_datetime(macro_daily.index)\n",
        "macro_daily.index.name = full_df.index.name\n",
        "macro_aligned = macro_daily.reindex(full_df.index)\n",
        "\n",
        "# Missingness flags BEFORE any fill (macro only)\n",
        "FLAG_SUFFIX = \"_is_missing\"\n",
        "macro_numeric_to_fill0 = [\"CPI_pct_mom\", \"CPI_accel_pct_mom\", \"FEDFUNDS_delta_mom\"]\n",
        "\n",
        "for col in macro_numeric_to_fill0:\n",
        "    if col in macro_aligned.columns:\n",
        "        macro_aligned[f\"{col}{FLAG_SUFFIX}\"] = macro_aligned[col].isna().astype(\"int8\")\n",
        "\n",
        "# Fill only macro gaps (no global full_df ffill)\n",
        "macro_cols = macro_aligned.columns.tolist()\n",
        "macro_aligned[macro_cols] = macro_aligned[macro_cols].ffill()\n",
        "\n",
        "# After ffill, leading NaNs may remain. Fill with 0 for selected numeric cols.\n",
        "for col in macro_numeric_to_fill0:\n",
        "    if col in macro_aligned.columns:\n",
        "        macro_aligned[col] = macro_aligned[col].fillna(0.0)\n",
        "\n",
        "# Enforce FEDFUNDS_changed to stay binary int8 after reindex/ffill\n",
        "FLAG_COL = \"FEDFUNDS_changed\"\n",
        "if FLAG_COL in macro_aligned.columns:\n",
        "    macro_aligned[FLAG_COL] = (\n",
        "        macro_aligned[FLAG_COL]\n",
        "        .fillna(0)\n",
        "        .clip(0, 1)\n",
        "        .astype(\"int8\")\n",
        "    )\n",
        "\n",
        "# Release-day flags (on trading-day index)\n",
        "if \"CPI_pct_mom\" in macro_aligned.columns:\n",
        "    cpi_series = macro_aligned[\"CPI_pct_mom\"].astype(\"float64\")\n",
        "    macro_aligned[\"CPI_release_day\"] = (cpi_series.notna() & cpi_series.ne(cpi_series.shift(1))).astype(\"int8\")\n",
        "\n",
        "if \"FEDFUNDS_level\" in macro_aligned.columns:\n",
        "    ff_series = macro_aligned[\"FEDFUNDS_level\"].astype(\"float64\")\n",
        "    macro_aligned[\"FEDFUNDS_release_day\"] = (ff_series.notna() & ff_series.ne(ff_series.shift(1))).astype(\"int8\")\n",
        "\n",
        "# Update macro_cols after adding flags\n",
        "macro_cols = macro_aligned.columns.tolist()\n",
        "\n",
        "# Remove existing macro columns from full_df to avoid duplicates\n",
        "existing_macro_cols = [c for c in macro_cols if c in full_df.columns]\n",
        "if existing_macro_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_macro_cols)} existing macro cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_macro_cols)\n",
        "\n",
        "# Merge into full_df\n",
        "full_df_merged = pd.concat([full_df, macro_aligned], axis=1)\n",
        "\n",
        "# Enforce missingness flags + release flags to int8\n",
        "for col in [c for c in full_df_merged.columns if c.endswith(FLAG_SUFFIX)]:\n",
        "    full_df_merged[col] = full_df_merged[col].fillna(0).clip(0, 1).astype(\"int8\")\n",
        "\n",
        "for col in [\"CPI_release_day\", \"FEDFUNDS_release_day\"]:\n",
        "    if col in full_df_merged.columns:\n",
        "        full_df_merged[col] = full_df_merged[col].fillna(0).clip(0, 1).astype(\"int8\")\n",
        "\n",
        "if FLAG_COL in full_df_merged.columns:\n",
        "    full_df_merged[FLAG_COL] = (\n",
        "        full_df_merged[FLAG_COL]\n",
        "        .fillna(0)\n",
        "        .clip(0, 1)\n",
        "        .astype(\"int8\")\n",
        "    )\n",
        "\n",
        "# --- Diagnostics (detailed) ---\n",
        "print(\"\\n[CHECK] Macro columns (including flags + release):\", macro_cols)\n",
        "print(\"\\n[CHECK] Macro head (aligned to trading dates):\\n\", full_df_merged[macro_cols].head(15))\n",
        "print(\"\\n[CHECK] Macro NaNs count:\\n\", full_df_merged[macro_cols].isna().sum())\n",
        "\n",
        "if FLAG_COL in full_df_merged.columns:\n",
        "    print(\"\\n[CHECK] FEDFUNDS_changed dtype:\", full_df_merged[FLAG_COL].dtype)\n",
        "    print(\"\\n[CHECK] FEDFUNDS_changed value counts:\\n\", full_df_merged[FLAG_COL].value_counts(dropna=False))\n",
        "\n",
        "# Release-day quick counts\n",
        "for col in [\"CPI_release_day\", \"FEDFUNDS_release_day\"]:\n",
        "    if col in full_df_merged.columns:\n",
        "        print(f\"\\n[CHECK] {col} value counts:\\n\", full_df_merged[col].value_counts(dropna=False))\n",
        "\n",
        "# EPS diagnostics (post-fill)\n",
        "for val_col, flag_col in eps_pairs:\n",
        "    if val_col in full_df_merged.columns and flag_col in full_df_merged.columns:\n",
        "        na_total = int(full_df_merged[val_col].isna().sum())\n",
        "        na_flag0 = int(full_df_merged.loc[full_df_merged[flag_col] == 0, val_col].isna().sum())\n",
        "        print(f\"\\n[CHECK] EPS {val_col}: na_total={na_total} | na_when_{flag_col}==0 => {na_flag0}\")\n",
        "\n",
        "print(\"\\n[CHECK] full_df_merged info():\")\n",
        "print(full_df_merged.info())\n",
        "\n",
        "# Continue downstream\n",
        "full_df = full_df_merged\n",
        "\n",
        "print(\"[OK] BLOCK 3 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30169d8",
      "metadata": {
        "id": "a30169d8"
      },
      "source": [
        "## BLOCK 4 â€” TIME FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "49101e49",
      "metadata": {
        "id": "49101e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5c961d-12ac-4ea4-9c67-3497ac9f4762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New time features added to full_df:\n",
            "  - day_of_week\n",
            "  - day_of_year_cos\n",
            "  - day_of_year_sin\n",
            "  - is_q1\n",
            "  - is_q2\n",
            "  - is_q3\n",
            "  - is_q4\n",
            "  - month\n",
            "  - quarter\n",
            "  - weekday_cos_5\n",
            "  - weekday_sin_5\n",
            "\n",
            "Preview:\n",
            "            day_of_week  month  quarter  is_q1  is_q2  is_q3  is_q4  \\\n",
            "Date                                                                  \n",
            "2023-11-20            0     11        4      0      0      0      1   \n",
            "2023-11-21            1     11        4      0      0      0      1   \n",
            "2023-11-22            2     11        4      0      0      0      1   \n",
            "2023-11-24            4     11        4      0      0      0      1   \n",
            "2023-11-27            0     11        4      0      0      0      1   \n",
            "2023-11-28            1     11        4      0      0      0      1   \n",
            "2023-11-29            2     11        4      0      0      0      1   \n",
            "2023-11-30            3     11        4      0      0      0      1   \n",
            "2023-12-01            4     12        4      0      0      0      1   \n",
            "2023-12-04            0     12        4      0      0      0      1   \n",
            "\n",
            "            weekday_sin_5  weekday_cos_5  day_of_year_sin  day_of_year_cos  \n",
            "Date                                                                        \n",
            "2023-11-20       0.000000       1.000000        -0.661635         0.749826  \n",
            "2023-11-21       0.951057       0.309017        -0.648630         0.761104  \n",
            "2023-11-22       0.587785      -0.809017        -0.635432         0.772157  \n",
            "2023-11-24      -0.951057       0.309017        -0.608477         0.793572  \n",
            "2023-11-27       0.000000       1.000000        -0.566702         0.823923  \n",
            "2023-11-28       0.951057       0.309017        -0.552435         0.833556  \n",
            "2023-11-29       0.587785      -0.809017        -0.538005         0.842942  \n",
            "2023-11-30      -0.587785      -0.809017        -0.523416         0.852078  \n",
            "2023-12-01      -0.951057       0.309017        -0.508671         0.860961  \n",
            "2023-12-04       0.000000       1.000000        -0.463550         0.886071  \n",
            "\n",
            "NaN check (time features):\n",
            "day_of_week        0\n",
            "month              0\n",
            "quarter            0\n",
            "is_q1              0\n",
            "is_q2              0\n",
            "is_q3              0\n",
            "is_q4              0\n",
            "weekday_sin_5      0\n",
            "weekday_cos_5      0\n",
            "day_of_year_sin    0\n",
            "day_of_year_cos    0\n",
            "dtype: int64\n",
            "[OK] BLOCK 4 complete. full_df shape: (540, 76)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cols_before = set(full_df.columns)\n",
        "\n",
        "# Ensure clean datetime index\n",
        "full_df.index = pd.to_datetime(full_df.index)\n",
        "full_df.index.name = \"Date\"\n",
        "full_df = full_df.sort_index()\n",
        "idx = full_df.index\n",
        "\n",
        "# Discrete time features\n",
        "full_df[\"day_of_week\"] = idx.weekday.astype(\"int8\")   # 0=Mon ... 6=Sun\n",
        "full_df[\"month\"] = idx.month.astype(\"int8\")           # 1..12\n",
        "full_df[\"quarter\"] = idx.quarter.astype(\"int8\")       # 1..4\n",
        "\n",
        "# Quarter binary dummies\n",
        "q = full_df[\"quarter\"].astype(\"int8\")\n",
        "full_df[\"is_q1\"] = (q == 1).astype(\"int8\")\n",
        "full_df[\"is_q2\"] = (q == 2).astype(\"int8\")\n",
        "full_df[\"is_q3\"] = (q == 3).astype(\"int8\")\n",
        "full_df[\"is_q4\"] = (q == 4).astype(\"int8\")\n",
        "\n",
        "# Cyclical features\n",
        "weekday = idx.weekday.astype(int)\n",
        "period_week = 5\n",
        "full_df[\"weekday_sin_5\"] = np.sin(2 * np.pi * (weekday % period_week) / period_week)\n",
        "full_df[\"weekday_cos_5\"] = np.cos(2 * np.pi * (weekday % period_week) / period_week)\n",
        "\n",
        "day_of_year = idx.dayofyear.astype(int)  # 1..365/366\n",
        "year_len = np.where(idx.is_leap_year, 366, 365)\n",
        "full_df[\"day_of_year_sin\"] = np.sin(2 * np.pi * (day_of_year - 1) / year_len)\n",
        "full_df[\"day_of_year_cos\"] = np.cos(2 * np.pi * (day_of_year - 1) / year_len)\n",
        "\n",
        "# Report newly added columns\n",
        "cols_after = set(full_df.columns)\n",
        "new_cols = sorted(cols_after - cols_before)\n",
        "\n",
        "print(\"New time features added to full_df:\")\n",
        "for c in new_cols:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "preview_cols = [\n",
        "    \"day_of_week\", \"month\", \"quarter\", \"is_q1\", \"is_q2\", \"is_q3\", \"is_q4\",\n",
        "    \"weekday_sin_5\", \"weekday_cos_5\", \"day_of_year_sin\", \"day_of_year_cos\"\n",
        "]\n",
        "\n",
        "print(\"\\nPreview:\")\n",
        "print(full_df[preview_cols].head(10))\n",
        "\n",
        "print(\"\\nNaN check (time features):\")\n",
        "print(full_df[preview_cols].isna().sum())\n",
        "\n",
        "print(\"[OK] BLOCK 4 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b34fc6cf",
      "metadata": {
        "id": "b34fc6cf"
      },
      "source": [
        "## BLOCK 5 â€” CONTROL CHECKS (leakage/time integrity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "cab3b3b7",
      "metadata": {
        "id": "cab3b3b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76895283-3a03-4988-8e83-7d21f10a48cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "[CONTROL] Starting control checks for leakage/time integrity...\n",
            "[OK] Index: DatetimeIndex | sorted | unique | range=2023-11-20 00:00:00 -> 2026-01-15 00:00:00 | rows=540\n",
            "[OK] Columns: 76 total | no duplicate column names\n",
            "[OK] No suspicious lead/shift(-1) patterns found in column names\n",
            "[OK] Leakage guard (name-based) passed: no *_t1 columns\n",
            "\n",
            "[CONTROL] Missingness report (focus: Earnings + Macro)\n",
            "                               na_count  na_pct\n",
            "FEDFUNDS_level                        8    1.48\n",
            "eps_surprise_pct_yahoo                0    0.00\n",
            "is_earnings_day                       0    0.00\n",
            "eps_surprise_pct_calc                 0    0.00\n",
            "has_eps_surprise_calc                 0    0.00\n",
            "CPI_pct_mom                           0    0.00\n",
            "has_eps_surprise_yahoo                0    0.00\n",
            "CPI_accel_pct_mom                     0    0.00\n",
            "FEDFUNDS_delta_mom                    0    0.00\n",
            "FEDFUNDS_changed                      0    0.00\n",
            "CPI_pct_mom_is_missing                0    0.00\n",
            "CPI_accel_pct_mom_is_missing          0    0.00\n",
            "FEDFUNDS_delta_mom_is_missing         0    0.00\n",
            "CPI_release_day                       0    0.00\n",
            "FEDFUNDS_release_day                  0    0.00\n",
            "\n",
            "[CONTROL] Global NaNs (top 15 cols with NA)\n",
            "                na_count  na_pct\n",
            "FEDFUNDS_level         8    1.48\n",
            "GOOGL_Volume           1    0.19\n",
            "NVDA_Volume            1    0.19\n",
            "MSFT_Volume            1    0.19\n",
            "^IXIC_Volume           1    0.19\n",
            "SPY_Volume             1    0.19\n",
            "QQQ_Volume             1    0.19\n",
            "XLK_Volume             1    0.19\n",
            "\n",
            "[CONTROL] Event-like column tagging:\n",
            "  - Earnings cols: 5\n",
            "  - Macro cols   : 10\n",
            "  - Total event  : 15\n",
            "\n",
            "[CONTROL] Flag dtype + value counts (quick):\n",
            "  - is_earnings_day: dtype=int8 | values={0: 532, 1: 8}\n",
            "  - has_eps_surprise_yahoo: dtype=int8 | values={0: 532, 1: 8}\n",
            "  - has_eps_surprise_calc: dtype=int8 | values={0: 532, 1: 8}\n",
            "  - FEDFUNDS_changed: dtype=int8 | values={0: 341, 1: 199}\n",
            "\n",
            "[CONTROL] DONE. No features created. No missingness fixed. Leakage name-guards applied.\n",
            "======================================================================\n",
            "[OK] BLOCK 5 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"[CONTROL] Starting control checks for leakage/time integrity...\")\n",
        "\n",
        "# Basic index integrity\n",
        "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a pandas DataFrame.\"\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "\n",
        "full_df = full_df.sort_index()\n",
        "full_df.index.name = full_df.index.name or \"Date\"\n",
        "\n",
        "assert full_df.index.is_monotonic_increasing, \"[ERROR] Index is not monotonic increasing after sort.\"\n",
        "assert full_df.index.is_unique, \"[ERROR] Index has duplicate timestamps.\"\n",
        "\n",
        "print(\n",
        "    f\"[OK] Index: DatetimeIndex | sorted | unique | range={full_df.index.min()} -> {full_df.index.max()} | rows={len(full_df)}\"\n",
        ")\n",
        "\n",
        "# Column sanity / duplicates\n",
        "cols = list(full_df.columns)\n",
        "dup_cols = pd.Index(cols)[pd.Index(cols).duplicated()].tolist()\n",
        "assert len(dup_cols) == 0, f\"[ERROR] Duplicate column names detected: {dup_cols}\"\n",
        "print(f\"[OK] Columns: {len(cols)} total | no duplicate column names\")\n",
        "\n",
        "# Leakage guard by naming conventions\n",
        "t1_cols = [c for c in cols if re.search(r\"(_t1\\b|_t\\+1\\b|t_plus_1\\b)\", c)]\n",
        "assert len(t1_cols) == 0, f\"[LEAKAGE ERROR] Found t+1 style columns in full_df: {t1_cols[:20]} (showing up to 20)\"\n",
        "\n",
        "suspicious_patterns = [\n",
        "    r\"\\bshift\\(\\s*-1\\s*\\)\", r\"\\blead\\b\", r\"\\bforward\\b\", r\"\\bfwd\\b\",\n",
        "    r\"\\bnext_day\\b\", r\"\\btomorrow\\b\", r\"\\bt\\+1\\b\"\n",
        "]\n",
        "sus_cols = [c for c in cols if any(re.search(p, c.lower()) for p in suspicious_patterns)]\n",
        "if len(sus_cols) > 0:\n",
        "    print(f\"[WARN] Suspicious potential lead columns by NAME: {sus_cols[:25]} (showing up to 25)\")\n",
        "else:\n",
        "    print(\"[OK] No suspicious lead/shift(-1) patterns found in column names\")\n",
        "\n",
        "print(\"[OK] Leakage guard (name-based) passed: no *_t1 columns\")\n",
        "\n",
        "# Missingness report\n",
        "earnings_cols = [\n",
        "    c for c in cols\n",
        "    if c.startswith(\"eps_surprise\")\n",
        "    or c.startswith(\"has_eps_surprise\")\n",
        "    or c == \"is_earnings_day\"\n",
        "]\n",
        "macro_cols = [c for c in cols if c.startswith(\"CPI_\") or c.startswith(\"FEDFUNDS_\")]\n",
        "\n",
        "focus_cols = [c for c in (earnings_cols + macro_cols) if c in cols]\n",
        "\n",
        "print(\"\\n[CONTROL] Missingness report (focus: Earnings + Macro)\")\n",
        "if len(focus_cols) == 0:\n",
        "    print(\"[INFO] No focus columns found (earnings/macro) â€” skipping focus missingness table.\")\n",
        "else:\n",
        "    miss = full_df[focus_cols].isna().sum().sort_values(ascending=False)\n",
        "    miss_pct = (miss / len(full_df) * 100).round(2)\n",
        "    miss_tbl = pd.DataFrame({\"na_count\": miss, \"na_pct\": miss_pct})\n",
        "    print(miss_tbl)\n",
        "\n",
        "na_any = full_df.isna().sum()\n",
        "top_na = na_any[na_any > 0].sort_values(ascending=False).head(15)\n",
        "print(\"\\n[CONTROL] Global NaNs (top 15 cols with NA)\")\n",
        "if len(top_na) == 0:\n",
        "    print(\"[OK] No NaNs in full_df.\")\n",
        "else:\n",
        "    top_na_pct = (top_na / len(full_df) * 100).round(2)\n",
        "    print(pd.DataFrame({\"na_count\": top_na, \"na_pct\": top_na_pct}))\n",
        "\n",
        "# Event-like columns tags (governance lists)\n",
        "EVENT_COLS_EARNINGS = earnings_cols\n",
        "EVENT_COLS_MACRO = macro_cols\n",
        "EVENT_COLS_ALL = sorted(set(EVENT_COLS_EARNINGS + EVENT_COLS_MACRO))\n",
        "\n",
        "print(\"\\n[CONTROL] Event-like column tagging:\")\n",
        "print(f\"  - Earnings cols: {len(EVENT_COLS_EARNINGS)}\")\n",
        "print(f\"  - Macro cols   : {len(EVENT_COLS_MACRO)}\")\n",
        "print(f\"  - Total event  : {len(EVENT_COLS_ALL)}\")\n",
        "\n",
        "flag_checks = [c for c in [\"is_earnings_day\", \"has_eps_surprise_yahoo\", \"has_eps_surprise_calc\", \"FEDFUNDS_changed\"] if c in cols]\n",
        "if len(flag_checks) > 0:\n",
        "    print(\"\\n[CONTROL] Flag dtype + value counts (quick):\")\n",
        "    for c in flag_checks:\n",
        "        vc = full_df[c].value_counts(dropna=False)\n",
        "        print(f\"  - {c}: dtype={full_df[c].dtype} | values={vc.to_dict()}\")\n",
        "\n",
        "# Save governance artifacts\n",
        "governance = {\n",
        "    \"event_cols_earnings\": EVENT_COLS_EARNINGS,\n",
        "    \"event_cols_macro\": EVENT_COLS_MACRO,\n",
        "    \"event_cols_all\": EVENT_COLS_ALL,\n",
        "    \"focus_cols_missingness\": focus_cols,\n",
        "    \"flag_checks\": flag_checks,\n",
        "}\n",
        "\n",
        "print(\"\\n[CONTROL] DONE. No features created. No missingness fixed. Leakage name-guards applied.\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"[OK] BLOCK 5 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b014e591",
      "metadata": {
        "id": "b014e591"
      },
      "source": [
        "## BLOCK 6 â€” RAW TRANSFORMATIONS (NO ROLLING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "c0cdbeda",
      "metadata": {
        "id": "c0cdbeda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86db2d96-92ef-4998-d061-293c0b665bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 7 OHLC tickers (excluded ['^GDAXI', '^TNX', '^VIX']): ['GOOGL', 'MSFT', 'NVDA', 'QQQ', 'SPY', 'XLK', '^IXIC']\n",
            "[INFO] Found 7 OHLCV tickers (with Volume, excluded ['^GDAXI', '^TNX', '^VIX']): ['GOOGL', 'MSFT', 'NVDA', 'QQQ', 'SPY', 'XLK', '^IXIC']\n",
            "\n",
            "[OK] BLOCK 6 added 105 raw feature columns.\n",
            "[INFO] Sample of added columns (first 40):\n",
            "  - GOOGL_abs_logret_cc\n",
            "  - GOOGL_abs_logret_gap_co\n",
            "  - GOOGL_abs_logret_oc\n",
            "  - GOOGL_close_pos_hl\n",
            "  - GOOGL_close_pos_hl_centered\n",
            "  - GOOGL_log_dollar_vol\n",
            "  - GOOGL_log_hl\n",
            "  - GOOGL_log_vol\n",
            "  - GOOGL_log_vol_chg_1d\n",
            "  - GOOGL_logret_cc\n",
            "  - GOOGL_logret_cc_lag1\n",
            "  - GOOGL_logret_cc_lag21\n",
            "  - GOOGL_logret_cc_lag5\n",
            "  - GOOGL_logret_gap_co\n",
            "  - GOOGL_logret_oc\n",
            "  - MSFT_abs_logret_cc\n",
            "  - MSFT_abs_logret_gap_co\n",
            "  - MSFT_abs_logret_oc\n",
            "  - MSFT_close_pos_hl\n",
            "  - MSFT_close_pos_hl_centered\n",
            "  - MSFT_log_dollar_vol\n",
            "  - MSFT_log_hl\n",
            "  - MSFT_log_vol\n",
            "  - MSFT_log_vol_chg_1d\n",
            "  - MSFT_logret_cc\n",
            "  - MSFT_logret_cc_lag1\n",
            "  - MSFT_logret_cc_lag21\n",
            "  - MSFT_logret_cc_lag5\n",
            "  - MSFT_logret_gap_co\n",
            "  - MSFT_logret_oc\n",
            "  - NVDA_abs_logret_cc\n",
            "  - NVDA_abs_logret_gap_co\n",
            "  - NVDA_abs_logret_oc\n",
            "  - NVDA_close_pos_hl\n",
            "  - NVDA_close_pos_hl_centered\n",
            "  - NVDA_log_dollar_vol\n",
            "  - NVDA_log_hl\n",
            "  - NVDA_log_vol\n",
            "  - NVDA_log_vol_chg_1d\n",
            "  - NVDA_logret_cc\n",
            "\n",
            "[CHECK] NaNs in NEW raw features (top 15):\n",
            "GOOGL_logret_cc_lag21    22\n",
            "MSFT_logret_cc_lag21     22\n",
            "SPY_logret_cc_lag21      22\n",
            "QQQ_logret_cc_lag21      22\n",
            "^IXIC_logret_cc_lag21    22\n",
            "XLK_logret_cc_lag21      22\n",
            "NVDA_logret_cc_lag21     22\n",
            "GOOGL_logret_cc_lag5      6\n",
            "MSFT_logret_cc_lag5       6\n",
            "NVDA_logret_cc_lag5       6\n",
            "QQQ_logret_cc_lag5        6\n",
            "SPY_logret_cc_lag5        6\n",
            "XLK_logret_cc_lag5        6\n",
            "^IXIC_logret_cc_lag5      6\n",
            "^IXIC_logret_cc_lag1      2\n",
            "dtype: int64\n",
            "[OK] BLOCK 6 complete. full_df shape: (540, 181)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "eps = float(RUN_PARAMS[\"features\"][\"eps\"])  # numeric safety for logs/divisions\n",
        "\n",
        "# Preconditions\n",
        "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "\n",
        "\n",
        "# Identify OHLCV ticker prefixes\n",
        "def has_cols(prefix: str, required: list) -> bool:\n",
        "    \"\"\"Check if all required columns exist for a ticker prefix.\"\"\"\n",
        "    return all(f\"{prefix}_{c}\" in full_df.columns for c in required)\n",
        "\n",
        "\n",
        "required_ohlc = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
        "required_ohlcv = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "\n",
        "prefixes = sorted({c.rsplit(\"_\", 1)[0] for c in full_df.columns if c.endswith(\"_Close\")})\n",
        "\n",
        "# EXCLUDE market indicators from OHLC-style transforms\n",
        "EXCLUDE_RAW_OHLC = set(RUN_PARAMS[\"features\"][\"exclude_raw_ohlc\"])\n",
        "\n",
        "ohlc_prefixes = [p for p in prefixes if has_cols(p, required_ohlc) and p not in EXCLUDE_RAW_OHLC]\n",
        "ohlcv_prefixes = [p for p in prefixes if has_cols(p, required_ohlcv) and p not in EXCLUDE_RAW_OHLC]\n",
        "\n",
        "print(f\"[INFO] Found {len(ohlc_prefixes)} OHLC tickers (excluded {sorted(EXCLUDE_RAW_OHLC)}): {ohlc_prefixes}\")\n",
        "print(f\"[INFO] Found {len(ohlcv_prefixes)} OHLCV tickers (with Volume, excluded {sorted(EXCLUDE_RAW_OHLC)}): {ohlcv_prefixes}\")\n",
        "\n",
        "# Raw transforms per ticker (NO rolling)\n",
        "new_cols = {}\n",
        "\n",
        "for p in ohlc_prefixes:\n",
        "    o = full_df[f\"{p}_Open\"].astype(\"float64\")\n",
        "    h = full_df[f\"{p}_High\"].astype(\"float64\")\n",
        "    l = full_df[f\"{p}_Low\"].astype(\"float64\")\n",
        "    c = full_df[f\"{p}_Close\"].astype(\"float64\")\n",
        "\n",
        "    # log returns\n",
        "    new_cols[f\"{p}_logret_cc\"] = np.log((c + eps) / (c.shift(1) + eps))\n",
        "    new_cols[f\"{p}_logret_oc\"] = np.log((c + eps) / (o + eps))\n",
        "    new_cols[f\"{p}_logret_gap_co\"] = np.log((o + eps) / (c.shift(1) + eps))\n",
        "\n",
        "    # abs variants\n",
        "    new_cols[f\"{p}_abs_logret_cc\"] = new_cols[f\"{p}_logret_cc\"].abs()\n",
        "    new_cols[f\"{p}_abs_logret_oc\"] = new_cols[f\"{p}_logret_oc\"].abs()\n",
        "    new_cols[f\"{p}_abs_logret_gap_co\"] = new_cols[f\"{p}_logret_gap_co\"].abs()\n",
        "\n",
        "    # intraday range\n",
        "    new_cols[f\"{p}_log_hl\"] = np.log((h + eps) / (l + eps))\n",
        "\n",
        "    # close position within High-Low range\n",
        "    denom_hl = (h - l).replace(0.0, np.nan)\n",
        "    close_pos = (c - l) / denom_hl\n",
        "    new_cols[f\"{p}_close_pos_hl\"] = close_pos\n",
        "    new_cols[f\"{p}_close_pos_hl_centered\"] = close_pos - 0.5\n",
        "\n",
        "    # lags for logret_cc\n",
        "    for lag in [1, 5, 21]:\n",
        "        new_cols[f\"{p}_logret_cc_lag{lag}\"] = new_cols[f\"{p}_logret_cc\"].shift(lag)\n",
        "\n",
        "# Volume features (only where Volume exists)\n",
        "for p in ohlcv_prefixes:\n",
        "    v = full_df[f\"{p}_Volume\"].astype(\"float64\")\n",
        "    c = full_df[f\"{p}_Close\"].astype(\"float64\")\n",
        "\n",
        "    new_cols[f\"{p}_log_vol\"] = np.log(v + 1.0)\n",
        "    new_cols[f\"{p}_log_vol_chg_1d\"] = new_cols[f\"{p}_log_vol\"] - new_cols[f\"{p}_log_vol\"].shift(1)\n",
        "\n",
        "    dollar_vol = (c * v).astype(\"float64\")\n",
        "    new_cols[f\"{p}_log_dollar_vol\"] = np.log(dollar_vol + 1.0)\n",
        "\n",
        "# Attach to full_df (remove existing to avoid duplicates on re-run)\n",
        "new_df = pd.DataFrame(new_cols, index=full_df.index)\n",
        "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
        "if existing_new_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_new_cols)\n",
        "full_df = pd.concat([full_df, new_df], axis=1)\n",
        "\n",
        "# Diagnostics\n",
        "cols_after = set(full_df.columns)\n",
        "added = sorted(cols_after - cols_before)\n",
        "\n",
        "print(f\"\\n[OK] BLOCK 6 added {len(added)} raw feature columns.\")\n",
        "print(\"[INFO] Sample of added columns (first 40):\")\n",
        "for c in added[:40]:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "nan_counts = full_df[added].isna().sum().sort_values(ascending=False).head(15)\n",
        "print(\"\\n[CHECK] NaNs in NEW raw features (top 15):\")\n",
        "print(nan_counts)\n",
        "\n",
        "print(\"[OK] BLOCK 6 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c218968",
      "metadata": {
        "id": "3c218968"
      },
      "source": [
        "## BLOCK 7 â€” ROLLING STATISTICS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "994b6dca",
      "metadata": {
        "id": "994b6dca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0be082-8be6-480e-bbea-15358b009173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Rolling stats tickers detected from *_logret_cc (excluding *_abs_*): ['GOOGL', 'MSFT', 'NVDA', 'QQQ', 'SPY', 'XLK', '^IXIC']\n",
            "\n",
            "[OK] BLOCK 7 added 133 rolling-stat columns (W_SHORT=5, W_LONG=21, volume_rolling=True).\n",
            "[INFO] Sample added cols (first 40):\n",
            "  - GOOGL_abs_logret_cc_mean_21\n",
            "  - GOOGL_abs_logret_cc_mean_5\n",
            "  - GOOGL_abs_logret_cc_std_21\n",
            "  - GOOGL_abs_logret_cc_std_5\n",
            "  - GOOGL_log_hl_mean_21\n",
            "  - GOOGL_log_hl_mean_5\n",
            "  - GOOGL_log_hl_std_21\n",
            "  - GOOGL_log_hl_std_5\n",
            "  - GOOGL_log_hl_z_21\n",
            "  - GOOGL_log_vol_mean_21\n",
            "  - GOOGL_log_vol_mean_5\n",
            "  - GOOGL_log_vol_std_21\n",
            "  - GOOGL_log_vol_std_5\n",
            "  - GOOGL_log_vol_z_21\n",
            "  - GOOGL_logret_cc_mean_21\n",
            "  - GOOGL_logret_cc_mean_5\n",
            "  - GOOGL_logret_cc_std_21\n",
            "  - GOOGL_logret_cc_std_5\n",
            "  - GOOGL_logret_cc_z_21\n",
            "  - MSFT_abs_logret_cc_mean_21\n",
            "  - MSFT_abs_logret_cc_mean_5\n",
            "  - MSFT_abs_logret_cc_std_21\n",
            "  - MSFT_abs_logret_cc_std_5\n",
            "  - MSFT_log_hl_mean_21\n",
            "  - MSFT_log_hl_mean_5\n",
            "  - MSFT_log_hl_std_21\n",
            "  - MSFT_log_hl_std_5\n",
            "  - MSFT_log_hl_z_21\n",
            "  - MSFT_log_vol_mean_21\n",
            "  - MSFT_log_vol_mean_5\n",
            "  - MSFT_log_vol_std_21\n",
            "  - MSFT_log_vol_std_5\n",
            "  - MSFT_log_vol_z_21\n",
            "  - MSFT_logret_cc_mean_21\n",
            "  - MSFT_logret_cc_mean_5\n",
            "  - MSFT_logret_cc_std_21\n",
            "  - MSFT_logret_cc_std_5\n",
            "  - MSFT_logret_cc_z_21\n",
            "  - NVDA_abs_logret_cc_mean_21\n",
            "  - NVDA_abs_logret_cc_mean_5\n",
            "\n",
            "[CHECK] NaNs in NEW rolling features (top 15):\n",
            "GOOGL_abs_logret_cc_mean_21    21\n",
            "GOOGL_abs_logret_cc_std_21     21\n",
            "GOOGL_logret_cc_mean_21        21\n",
            "GOOGL_log_vol_mean_21          21\n",
            "GOOGL_log_vol_std_21           21\n",
            "XLK_log_vol_mean_21            21\n",
            "XLK_log_vol_std_21             21\n",
            "SPY_logret_cc_mean_21          21\n",
            "MSFT_abs_logret_cc_mean_21     21\n",
            "GOOGL_logret_cc_z_21           21\n",
            "GOOGL_logret_cc_std_21         21\n",
            "GOOGL_log_vol_z_21             21\n",
            "MSFT_abs_logret_cc_std_21      21\n",
            "MSFT_log_vol_std_21            21\n",
            "MSFT_log_vol_z_21              21\n",
            "dtype: int64\n",
            "[OK] BLOCK 7 complete. full_df shape: (540, 314)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "# Windows from RUN_PARAMS\n",
        "W_SHORT = int(RUN_PARAMS[\"features\"][\"rolling_w_short\"])\n",
        "W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
        "DO_VOLUME_ROLLING = bool(RUN_PARAMS[\"features\"][\"do_volume_rolling\"])\n",
        "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "\n",
        "# Determine tickers that have raw series\n",
        "tickers = sorted({\n",
        "    c.replace(\"_logret_cc\", \"\")\n",
        "    for c in full_df.columns\n",
        "    if c.endswith(\"_logret_cc\") and not c.endswith(\"_abs_logret_cc\")\n",
        "})\n",
        "print(f\"[INFO] Rolling stats tickers detected from *_logret_cc (excluding *_abs_*): {tickers}\")\n",
        "\n",
        "new_cols = {}\n",
        "\n",
        "# Rolling stats for returns / abs returns / HL\n",
        "for p in tickers:\n",
        "    col_r = f\"{p}_logret_cc\"\n",
        "    col_ar = f\"{p}_abs_logret_cc\"\n",
        "    col_hl = f\"{p}_log_hl\"\n",
        "\n",
        "    if col_r not in full_df.columns:\n",
        "        continue\n",
        "\n",
        "    r = full_df[col_r].astype(\"float64\")\n",
        "\n",
        "    # rolling mean/std of logret_cc\n",
        "    r_m_s = r.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
        "    r_s_s = r.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
        "\n",
        "    r_m_l = r.rolling(W_LONG, min_periods=W_LONG).mean()\n",
        "    r_s_l = r.rolling(W_LONG, min_periods=W_LONG).std()\n",
        "\n",
        "    new_cols[f\"{p}_logret_cc_mean_{W_SHORT}\"] = r_m_s\n",
        "    new_cols[f\"{p}_logret_cc_std_{W_SHORT}\"] = r_s_s\n",
        "    new_cols[f\"{p}_logret_cc_mean_{W_LONG}\"] = r_m_l\n",
        "    new_cols[f\"{p}_logret_cc_std_{W_LONG}\"] = r_s_l\n",
        "\n",
        "    # z-score vs long window\n",
        "    new_cols[f\"{p}_logret_cc_z_{W_LONG}\"] = (r - r_m_l) / (r_s_l + eps)\n",
        "\n",
        "    # abs_logret_cc mean/std\n",
        "    if col_ar in full_df.columns:\n",
        "        ar = full_df[col_ar].astype(\"float64\")\n",
        "        new_cols[f\"{p}_abs_logret_cc_mean_{W_SHORT}\"] = ar.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
        "        new_cols[f\"{p}_abs_logret_cc_std_{W_SHORT}\"] = ar.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
        "        new_cols[f\"{p}_abs_logret_cc_mean_{W_LONG}\"] = ar.rolling(W_LONG, min_periods=W_LONG).mean()\n",
        "        new_cols[f\"{p}_abs_logret_cc_std_{W_LONG}\"] = ar.rolling(W_LONG, min_periods=W_LONG).std()\n",
        "\n",
        "    # HL rolling mean/std + z-score\n",
        "    if col_hl in full_df.columns:\n",
        "        hl = full_df[col_hl].astype(\"float64\")\n",
        "        hl_m_s = hl.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
        "        hl_s_s = hl.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
        "\n",
        "        hl_m_l = hl.rolling(W_LONG, min_periods=W_LONG).mean()\n",
        "        hl_s_l = hl.rolling(W_LONG, min_periods=W_LONG).std()\n",
        "\n",
        "        new_cols[f\"{p}_log_hl_mean_{W_SHORT}\"] = hl_m_s\n",
        "        new_cols[f\"{p}_log_hl_std_{W_SHORT}\"] = hl_s_s\n",
        "        new_cols[f\"{p}_log_hl_mean_{W_LONG}\"] = hl_m_l\n",
        "        new_cols[f\"{p}_log_hl_std_{W_LONG}\"] = hl_s_l\n",
        "        new_cols[f\"{p}_log_hl_z_{W_LONG}\"] = (hl - hl_m_l) / (hl_s_l + eps)\n",
        "\n",
        "# Optional: Volume rolling stats\n",
        "if DO_VOLUME_ROLLING:\n",
        "    for p in tickers:\n",
        "        col_lv = f\"{p}_log_vol\"\n",
        "        if col_lv not in full_df.columns:\n",
        "            continue\n",
        "\n",
        "        lv = full_df[col_lv].astype(\"float64\")\n",
        "\n",
        "        lv_m_s = lv.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
        "        lv_s_s = lv.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
        "\n",
        "        lv_m_l = lv.rolling(W_LONG, min_periods=W_LONG).mean()\n",
        "        lv_s_l = lv.rolling(W_LONG, min_periods=W_LONG).std()\n",
        "\n",
        "        new_cols[f\"{p}_log_vol_mean_{W_SHORT}\"] = lv_m_s\n",
        "        new_cols[f\"{p}_log_vol_std_{W_SHORT}\"] = lv_s_s\n",
        "        new_cols[f\"{p}_log_vol_mean_{W_LONG}\"] = lv_m_l\n",
        "        new_cols[f\"{p}_log_vol_std_{W_LONG}\"] = lv_s_l\n",
        "        new_cols[f\"{p}_log_vol_z_{W_LONG}\"] = (lv - lv_m_l) / (lv_s_l + eps)\n",
        "\n",
        "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
        "roll_df = pd.DataFrame(new_cols, index=full_df.index)\n",
        "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
        "if existing_new_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_new_cols)\n",
        "full_df = pd.concat([full_df, roll_df], axis=1)\n",
        "\n",
        "cols_after = set(full_df.columns)\n",
        "added = sorted(cols_after - cols_before)\n",
        "\n",
        "print(\n",
        "    f\"\\n[OK] BLOCK 7 added {len(added)} rolling-stat columns \"\n",
        "    f\"(W_SHORT={W_SHORT}, W_LONG={W_LONG}, volume_rolling={DO_VOLUME_ROLLING}).\"\n",
        ")\n",
        "print(\"[INFO] Sample added cols (first 40):\")\n",
        "for c in added[:40]:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(15)\n",
        "print(\"\\n[CHECK] NaNs in NEW rolling features (top 15):\")\n",
        "print(nan_top)\n",
        "\n",
        "print(\"[OK] BLOCK 7 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9341c74",
      "metadata": {
        "id": "b9341c74"
      },
      "source": [
        "## BLOCK 8 â€” CROSS-ASSET RELATIONSHIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "ab58f88c",
      "metadata": {
        "id": "ab58f88c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33756e18-38eb-49f8-8439-505ff524cc3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] BLOCK 8 added 16 cross-asset rolling columns (BASE=GOOGL, windows=[5, 21]).\n",
            "[INFO] Added columns:\n",
            "  - GOOGL_beta_QQQ_21\n",
            "  - GOOGL_beta_QQQ_5\n",
            "  - GOOGL_beta_SPY_21\n",
            "  - GOOGL_beta_SPY_5\n",
            "  - GOOGL_beta_XLK_21\n",
            "  - GOOGL_beta_XLK_5\n",
            "  - GOOGL_beta_^IXIC_21\n",
            "  - GOOGL_beta_^IXIC_5\n",
            "  - GOOGL_corr_QQQ_21\n",
            "  - GOOGL_corr_QQQ_5\n",
            "  - GOOGL_corr_SPY_21\n",
            "  - GOOGL_corr_SPY_5\n",
            "  - GOOGL_corr_XLK_21\n",
            "  - GOOGL_corr_XLK_5\n",
            "  - GOOGL_corr_^IXIC_21\n",
            "  - GOOGL_corr_^IXIC_5\n",
            "\n",
            "[CHECK] NaNs in NEW cross-asset features (top 10):\n",
            "GOOGL_beta_QQQ_21      21\n",
            "GOOGL_beta_SPY_21      21\n",
            "GOOGL_beta_^IXIC_21    21\n",
            "GOOGL_beta_XLK_21      21\n",
            "GOOGL_corr_XLK_21      21\n",
            "GOOGL_corr_^IXIC_21    21\n",
            "GOOGL_corr_SPY_21      21\n",
            "GOOGL_corr_QQQ_21      21\n",
            "GOOGL_beta_^IXIC_5      5\n",
            "GOOGL_beta_XLK_5        5\n",
            "dtype: int64\n",
            "[OK] BLOCK 8 complete. full_df shape: (540, 330)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "BASE = str(RUN_PARAMS[\"features\"][\"cross_asset_base\"])\n",
        "PEERS = list(RUN_PARAMS[\"features\"][\"cross_asset_peers\"])\n",
        "WINDOWS = list(RUN_PARAMS[\"features\"][\"cross_asset_windows\"])\n",
        "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "\n",
        "# Preconditions\n",
        "base_col = f\"{BASE}_logret_cc\"\n",
        "assert base_col in full_df.columns, f\"[ERROR] Missing base return column: {base_col}\"\n",
        "\n",
        "for p in PEERS:\n",
        "    col = f\"{p}_logret_cc\"\n",
        "    assert col in full_df.columns, f\"[ERROR] Missing peer return column: {col}\"\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "\n",
        "# Rolling correlation & beta\n",
        "new_cols = {}\n",
        "r_base = full_df[base_col].astype(\"float64\")\n",
        "\n",
        "for p in PEERS:\n",
        "    r_peer = full_df[f\"{p}_logret_cc\"].astype(\"float64\")\n",
        "\n",
        "    for w in WINDOWS:\n",
        "        w = int(w)\n",
        "        new_cols[f\"{BASE}_corr_{p}_{w}\"] = r_base.rolling(w, min_periods=w).corr(r_peer)\n",
        "\n",
        "        cov = r_base.rolling(w, min_periods=w).cov(r_peer)\n",
        "        var = r_peer.rolling(w, min_periods=w).var()\n",
        "        new_cols[f\"{BASE}_beta_{p}_{w}\"] = cov / (var + eps)\n",
        "\n",
        "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
        "cross_df = pd.DataFrame(new_cols, index=full_df.index)\n",
        "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
        "if existing_new_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_new_cols)\n",
        "full_df = pd.concat([full_df, cross_df], axis=1)\n",
        "\n",
        "added = sorted(set(full_df.columns) - cols_before)\n",
        "\n",
        "print(f\"\\n[OK] BLOCK 8 added {len(added)} cross-asset rolling columns (BASE={BASE}, windows={WINDOWS}).\")\n",
        "print(\"[INFO] Added columns:\")\n",
        "for c in added:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(10)\n",
        "print(\"\\n[CHECK] NaNs in NEW cross-asset features (top 10):\")\n",
        "print(nan_top)\n",
        "\n",
        "print(\"[OK] BLOCK 8 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1f7c58",
      "metadata": {
        "id": "eb1f7c58"
      },
      "source": [
        "## BLOCK 9 â€” REGIME & INTERACTION LAYER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "aadb8c14",
      "metadata": {
        "id": "aadb8c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c65a745-135a-4dc4-c10e-3531c97e9961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] BLOCK 9 added 16 regime/interaction columns (macro now event-aware).\n",
            "[INFO] Added columns:\n",
            "  - GOOGL_cpi_trend_interact\n",
            "  - GOOGL_ctx_beta_vol_QQQ\n",
            "  - GOOGL_ctx_beta_vol_SPY\n",
            "  - GOOGL_ctx_beta_vol_XLK\n",
            "  - GOOGL_ctx_beta_vol_^IXIC\n",
            "  - GOOGL_ctx_corr_volratio_QQQ\n",
            "  - GOOGL_ctx_corr_volratio_SPY\n",
            "  - GOOGL_ctx_corr_volratio_XLK\n",
            "  - GOOGL_ctx_corr_volratio_^IXIC\n",
            "  - GOOGL_fedfunds_trend_interact\n",
            "  - GOOGL_price_struct_1\n",
            "  - GOOGL_price_struct_2\n",
            "  - GOOGL_vix_vol_interact\n",
            "  - GOOGL_vol_diff_5_21\n",
            "  - GOOGL_vol_ratio_5_21\n",
            "  - GOOGL_vol_regime_score\n",
            "\n",
            "[CHECK] NaNs in NEW Block-9 features (top 10):\n",
            "GOOGL_cpi_trend_interact         21\n",
            "GOOGL_ctx_beta_vol_QQQ           21\n",
            "GOOGL_ctx_beta_vol_SPY           21\n",
            "GOOGL_ctx_beta_vol_XLK           21\n",
            "GOOGL_ctx_beta_vol_^IXIC         21\n",
            "GOOGL_ctx_corr_volratio_QQQ      21\n",
            "GOOGL_ctx_corr_volratio_SPY      21\n",
            "GOOGL_ctx_corr_volratio_XLK      21\n",
            "GOOGL_ctx_corr_volratio_^IXIC    21\n",
            "GOOGL_fedfunds_trend_interact    21\n",
            "dtype: int64\n",
            "[OK] BLOCK 9 complete. full_df shape: (540, 346)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "BASE = str(RUN_PARAMS[\"features\"][\"regime_base\"])\n",
        "W_SHORT = int(RUN_PARAMS[\"features\"][\"rolling_w_short\"])\n",
        "W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
        "PEERS = list(RUN_PARAMS[\"features\"][\"cross_asset_peers\"])\n",
        "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "new_cols = {}\n",
        "\n",
        "# Preconditions for needed rolling columns\n",
        "req_cols = [\n",
        "    f\"{BASE}_logret_cc_std_{W_SHORT}\",\n",
        "    f\"{BASE}_logret_cc_std_{W_LONG}\",\n",
        "    f\"{BASE}_logret_cc_z_{W_LONG}\",\n",
        "    f\"{BASE}_abs_logret_cc\",\n",
        "    f\"{BASE}_abs_logret_cc_mean_{W_LONG}\",\n",
        "    f\"{BASE}_abs_logret_cc_std_{W_LONG}\",\n",
        "    f\"{BASE}_close_pos_hl\",\n",
        "    f\"{BASE}_log_hl_z_{W_LONG}\",\n",
        "    f\"{BASE}_logret_cc_mean_{W_LONG}\",\n",
        "]\n",
        "missing_req = [c for c in req_cols if c not in full_df.columns]\n",
        "assert len(missing_req) == 0, f\"[ERROR] Missing required columns for regime layer: {missing_req}\"\n",
        "\n",
        "# Volatility / Regime (BASE)\n",
        "std_s = full_df[f\"{BASE}_logret_cc_std_{W_SHORT}\"].astype(\"float64\")\n",
        "std_l = full_df[f\"{BASE}_logret_cc_std_{W_LONG}\"].astype(\"float64\")\n",
        "\n",
        "new_cols[f\"{BASE}_vol_ratio_{W_SHORT}_{W_LONG}\"] = std_s / (std_l + eps)\n",
        "new_cols[f\"{BASE}_vol_diff_{W_SHORT}_{W_LONG}\"] = std_s - std_l\n",
        "\n",
        "z_ret = full_df[f\"{BASE}_logret_cc_z_{W_LONG}\"].astype(\"float64\")\n",
        "new_cols[f\"{BASE}_vol_regime_score\"] = z_ret * (std_s / (std_l + eps))\n",
        "\n",
        "# Price Structure (BASE)\n",
        "abs_ret = full_df[f\"{BASE}_abs_logret_cc\"].astype(\"float64\")\n",
        "abs_ret_mean_l = full_df[f\"{BASE}_abs_logret_cc_mean_{W_LONG}\"].astype(\"float64\")\n",
        "abs_ret_std_l = full_df[f\"{BASE}_abs_logret_cc_std_{W_LONG}\"].astype(\"float64\")\n",
        "abs_ret_z = (abs_ret - abs_ret_mean_l) / (abs_ret_std_l + eps)\n",
        "\n",
        "new_cols[f\"{BASE}_price_struct_1\"] = (\n",
        "    full_df[f\"{BASE}_close_pos_hl\"].astype(\"float64\") * abs_ret_z\n",
        ")\n",
        "\n",
        "new_cols[f\"{BASE}_price_struct_2\"] = (\n",
        "    full_df[f\"{BASE}_log_hl_z_{W_LONG}\"].astype(\"float64\") *\n",
        "    (std_s / (std_l + eps))\n",
        ")\n",
        "\n",
        "# Market Context (BASE vs peers)\n",
        "for p in PEERS:\n",
        "    beta_col = f\"{BASE}_beta_{p}_{W_LONG}\"\n",
        "    corr_col = f\"{BASE}_corr_{p}_{W_LONG}\"\n",
        "\n",
        "    if beta_col in full_df.columns:\n",
        "        new_cols[f\"{BASE}_ctx_beta_vol_{p}\"] = full_df[beta_col].astype(\"float64\") * std_l\n",
        "\n",
        "    if corr_col in full_df.columns:\n",
        "        new_cols[f\"{BASE}_ctx_corr_volratio_{p}\"] = (\n",
        "            full_df[corr_col].astype(\"float64\") * (std_s / (std_l + eps))\n",
        "        )\n",
        "\n",
        "# Macro context (event-aware)\n",
        "trend_l = full_df[f\"{BASE}_logret_cc_mean_{W_LONG}\"].astype(\"float64\")\n",
        "\n",
        "# VIX level Ã— volatility (daily series)\n",
        "if \"^VIX_Close\" in full_df.columns:\n",
        "    new_cols[f\"{BASE}_vix_vol_interact\"] = full_df[\"^VIX_Close\"].astype(\"float64\") * std_l\n",
        "\n",
        "# FEDFUNDS delta Ã— trend, ONLY on release day\n",
        "if \"FEDFUNDS_delta_mom\" in full_df.columns and \"FEDFUNDS_release_day\" in full_df.columns:\n",
        "    new_cols[f\"{BASE}_fedfunds_trend_interact\"] = (\n",
        "        full_df[\"FEDFUNDS_delta_mom\"].astype(\"float64\") *\n",
        "        full_df[\"FEDFUNDS_release_day\"].astype(\"float64\") *\n",
        "        trend_l\n",
        "    )\n",
        "\n",
        "# CPI pct_mom Ã— trend, ONLY on release day\n",
        "if \"CPI_pct_mom\" in full_df.columns and \"CPI_release_day\" in full_df.columns:\n",
        "    new_cols[f\"{BASE}_cpi_trend_interact\"] = (\n",
        "        full_df[\"CPI_pct_mom\"].astype(\"float64\") *\n",
        "        full_df[\"CPI_release_day\"].astype(\"float64\") *\n",
        "        trend_l\n",
        "    )\n",
        "\n",
        "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
        "inter_df = pd.DataFrame(new_cols, index=full_df.index)\n",
        "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
        "if existing_new_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_new_cols)\n",
        "full_df = pd.concat([full_df, inter_df], axis=1)\n",
        "\n",
        "added = sorted(set(full_df.columns) - cols_before)\n",
        "\n",
        "print(f\"\\n[OK] BLOCK 9 added {len(added)} regime/interaction columns (macro now event-aware).\")\n",
        "print(\"[INFO] Added columns:\")\n",
        "for c in added:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(10)\n",
        "print(\"\\n[CHECK] NaNs in NEW Block-9 features (top 10):\")\n",
        "print(nan_top)\n",
        "\n",
        "print(\"[OK] BLOCK 9 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1b6149",
      "metadata": {
        "id": "5b1b6149"
      },
      "source": [
        "## BLOCK 10 â€” VIX/TNX SPECIAL FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "b20b9e06",
      "metadata": {
        "id": "b20b9e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e20a981-097f-4162-94ae-31ac3e297156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] BLOCK 10 added 25 special VIX/TNX columns.\n",
            "[INFO] Added columns:\n",
            "  - TNX_abs_delta_1d\n",
            "  - TNX_abs_delta_1d_lag1\n",
            "  - TNX_abs_delta_1d_lag21\n",
            "  - TNX_abs_delta_1d_lag5\n",
            "  - TNX_delta_1d\n",
            "  - TNX_delta_1d_lag1\n",
            "  - TNX_delta_1d_lag21\n",
            "  - TNX_delta_1d_lag5\n",
            "  - TNX_delta_5d\n",
            "  - TNX_gap_oc\n",
            "  - TNX_level\n",
            "  - TNX_log_hl\n",
            "  - TNX_range\n",
            "  - VIX_abs_delta_1d\n",
            "  - VIX_abs_delta_1d_lag1\n",
            "  - VIX_abs_delta_1d_lag21\n",
            "  - VIX_abs_delta_1d_lag5\n",
            "  - VIX_delta_1d\n",
            "  - VIX_delta_1d_lag1\n",
            "  - VIX_delta_1d_lag21\n",
            "  - VIX_delta_1d_lag5\n",
            "  - VIX_gap_oc\n",
            "  - VIX_log_hl\n",
            "  - VIX_log_level\n",
            "  - VIX_range_frac\n",
            "\n",
            "[CHECK] NaNs in NEW VIX/TNX features (top 12):\n",
            "TNX_abs_delta_1d_lag21    22\n",
            "TNX_delta_1d_lag21        22\n",
            "VIX_abs_delta_1d_lag21    22\n",
            "VIX_delta_1d_lag21        22\n",
            "TNX_delta_1d_lag5          6\n",
            "VIX_abs_delta_1d_lag5      6\n",
            "VIX_delta_1d_lag5          6\n",
            "TNX_abs_delta_1d_lag5      6\n",
            "TNX_delta_5d               5\n",
            "VIX_abs_delta_1d_lag1      2\n",
            "VIX_delta_1d_lag1          2\n",
            "TNX_abs_delta_1d_lag1      2\n",
            "dtype: int64\n",
            "[OK] BLOCK 10 complete. full_df shape: (540, 371)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "full_df = full_df.sort_index()\n",
        "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "new_cols = {}\n",
        "\n",
        "\n",
        "# Preconditions\n",
        "def _need_cols(cols):\n",
        "    \"\"\"Check required columns exist.\"\"\"\n",
        "    missing = [c for c in cols if c not in full_df.columns]\n",
        "    assert len(missing) == 0, f\"[ERROR] Missing required columns: {missing}\"\n",
        "\n",
        "\n",
        "_need_cols([\n",
        "    \"^VIX_Open\", \"^VIX_High\", \"^VIX_Low\", \"^VIX_Close\",\n",
        "    \"^TNX_Open\", \"^TNX_High\", \"^TNX_Low\", \"^TNX_Close\"\n",
        "])\n",
        "\n",
        "\n",
        "def _log_ratio(num, den):\n",
        "    \"\"\"Safe log ratio.\"\"\"\n",
        "    return np.log((num + eps) / (den + eps))\n",
        "\n",
        "\n",
        "# VIX features\n",
        "vix_o = full_df[\"^VIX_Open\"].astype(\"float64\")\n",
        "vix_h = full_df[\"^VIX_High\"].astype(\"float64\")\n",
        "vix_l = full_df[\"^VIX_Low\"].astype(\"float64\")\n",
        "vix_c = full_df[\"^VIX_Close\"].astype(\"float64\")\n",
        "\n",
        "new_cols[\"VIX_log_level\"] = np.log(vix_c + eps)\n",
        "new_cols[\"VIX_delta_1d\"] = vix_c.diff(1)\n",
        "new_cols[\"VIX_abs_delta_1d\"] = new_cols[\"VIX_delta_1d\"].abs()\n",
        "new_cols[\"VIX_log_hl\"] = _log_ratio(vix_h, vix_l)\n",
        "new_cols[\"VIX_range_frac\"] = (vix_h - vix_l) / (vix_c.abs() + eps)\n",
        "new_cols[\"VIX_gap_oc\"] = _log_ratio(vix_c, vix_o)\n",
        "\n",
        "for lag in [1, 5, 21]:\n",
        "    new_cols[f\"VIX_delta_1d_lag{lag}\"] = new_cols[\"VIX_delta_1d\"].shift(lag)\n",
        "    new_cols[f\"VIX_abs_delta_1d_lag{lag}\"] = new_cols[\"VIX_abs_delta_1d\"].shift(lag)\n",
        "\n",
        "# TNX features\n",
        "tnx_o = full_df[\"^TNX_Open\"].astype(\"float64\")\n",
        "tnx_h = full_df[\"^TNX_High\"].astype(\"float64\")\n",
        "tnx_l = full_df[\"^TNX_Low\"].astype(\"float64\")\n",
        "tnx_c = full_df[\"^TNX_Close\"].astype(\"float64\")\n",
        "\n",
        "new_cols[\"TNX_level\"] = tnx_c\n",
        "new_cols[\"TNX_delta_1d\"] = tnx_c.diff(1)\n",
        "new_cols[\"TNX_abs_delta_1d\"] = new_cols[\"TNX_delta_1d\"].abs()\n",
        "new_cols[\"TNX_delta_5d\"] = tnx_c.diff(5)\n",
        "new_cols[\"TNX_log_hl\"] = _log_ratio(tnx_h, tnx_l)\n",
        "new_cols[\"TNX_range\"] = (tnx_h - tnx_l)\n",
        "new_cols[\"TNX_gap_oc\"] = _log_ratio(tnx_c, tnx_o)\n",
        "\n",
        "for lag in [1, 5, 21]:\n",
        "    new_cols[f\"TNX_delta_1d_lag{lag}\"] = new_cols[\"TNX_delta_1d\"].shift(lag)\n",
        "    new_cols[f\"TNX_abs_delta_1d_lag{lag}\"] = new_cols[\"TNX_abs_delta_1d\"].shift(lag)\n",
        "\n",
        "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
        "vix_tnx_df = pd.DataFrame(new_cols, index=full_df.index)\n",
        "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
        "if existing_new_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_new_cols)\n",
        "full_df = pd.concat([full_df, vix_tnx_df], axis=1)\n",
        "\n",
        "added = sorted(set(full_df.columns) - cols_before)\n",
        "\n",
        "print(f\"\\n[OK] BLOCK 10 added {len(added)} special VIX/TNX columns.\")\n",
        "print(\"[INFO] Added columns:\")\n",
        "for c in added:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(12)\n",
        "print(\"\\n[CHECK] NaNs in NEW VIX/TNX features (top 12):\")\n",
        "print(nan_top)\n",
        "\n",
        "print(\"[OK] BLOCK 10 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a4e057",
      "metadata": {
        "id": "e8a4e057"
      },
      "source": [
        "## BLOCK 11 â€” EVENTS & SPARSE SIGNALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "6748fe3c",
      "metadata": {
        "id": "6748fe3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c453d31-1de5-48bc-dd40-d994b39b4452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] BLOCK 11 added 17 event/sparse-signal columns.\n",
            "[INFO] Added columns:\n",
            "  - CPI_change_prev_release\n",
            "  - CPI_impulse\n",
            "  - CPI_prev_release\n",
            "  - FEDFUNDS_impulse\n",
            "  - FEDFUNDS_prev_delta\n",
            "  - FEDFUNDS_release_day_x_mkt_vol\n",
            "  - eps_flag_calc_x_vol\n",
            "  - eps_flag_yahoo_x_vol\n",
            "  - eps_surprise_calc_lag1\n",
            "  - eps_surprise_calc_lag2\n",
            "  - eps_surprise_yahoo_lag1\n",
            "  - eps_surprise_yahoo_lag2\n",
            "  - post_earnings_day_1\n",
            "  - post_earnings_day_2\n",
            "  - post_earnings_day_3\n",
            "  - post_earnings_day_4\n",
            "  - post_earnings_day_5\n",
            "\n",
            "[CHECK] NaNs in NEW Block-11 features (top 12):\n",
            "eps_flag_calc_x_vol               21\n",
            "FEDFUNDS_release_day_x_mkt_vol    21\n",
            "eps_flag_yahoo_x_vol              21\n",
            "CPI_change_prev_release            0\n",
            "CPI_impulse                        0\n",
            "FEDFUNDS_prev_delta                0\n",
            "FEDFUNDS_impulse                   0\n",
            "CPI_prev_release                   0\n",
            "eps_surprise_calc_lag1             0\n",
            "eps_surprise_calc_lag2             0\n",
            "eps_surprise_yahoo_lag1            0\n",
            "eps_surprise_yahoo_lag2            0\n",
            "dtype: int64\n",
            "\n",
            "[CHECK] CPI_impulse: nonzero_days=22 | CPI_release_day count=23\n",
            "\n",
            "[CHECK] FEDFUNDS_impulse: nonzero_days=9 | FEDFUNDS_release_day count=10\n",
            "[OK] BLOCK 11 complete. full_df shape: (540, 388)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "full_df = full_df.sort_index()\n",
        "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "\n",
        "# Get params from RUN_PARAMS\n",
        "BASE = str(RUN_PARAMS[\"features\"][\"regime_base\"])\n",
        "W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
        "MARKET_VOL_TICKER = str(RUN_PARAMS[\"features\"][\"market_vol_ticker\"])\n",
        "\n",
        "BASE_VOL_COL = f\"{BASE}_logret_cc_std_{W_LONG}\"\n",
        "MARKET_VOL_COL = f\"{MARKET_VOL_TICKER}_logret_cc_std_{W_LONG}\"\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "new_cols = {}\n",
        "\n",
        "# Preconditions\n",
        "req = [\n",
        "    \"is_earnings_day\",\n",
        "    \"eps_surprise_pct_yahoo\", \"has_eps_surprise_yahoo\",\n",
        "    \"eps_surprise_pct_calc\", \"has_eps_surprise_calc\",\n",
        "    \"CPI_pct_mom\", \"CPI_release_day\",\n",
        "    \"FEDFUNDS_delta_mom\", \"FEDFUNDS_release_day\",\n",
        "]\n",
        "missing = [c for c in req if c not in full_df.columns]\n",
        "assert len(missing) == 0, f\"[ERROR] Missing required columns: {missing}\"\n",
        "assert BASE_VOL_COL in full_df.columns, f\"[ERROR] missing {BASE_VOL_COL}\"\n",
        "assert MARKET_VOL_COL in full_df.columns, f\"[ERROR] missing {MARKET_VOL_COL}\"\n",
        "\n",
        "\n",
        "# Helper: event-based previous release (lag1/lag2) mapped to daily index\n",
        "def prev_event_value_to_daily(\n",
        "    df: pd.DataFrame,\n",
        "    value_col: str,\n",
        "    release_flag_col: str,\n",
        "    lag_k: int = 1,\n",
        "    fill_before_first: float = 0.0,\n",
        ") -> pd.Series:\n",
        "    \"\"\"Returns a daily series where each day carries the value from the previous (lag_k) RELEASE EVENT.\"\"\"\n",
        "    events = df.loc[df[release_flag_col] == 1, value_col].astype(\"float64\").copy()\n",
        "    shifted = events.shift(lag_k)\n",
        "\n",
        "    out = pd.Series(index=df.index, dtype=\"float64\")\n",
        "    out.loc[shifted.index] = shifted.values\n",
        "    out = out.ffill().fillna(fill_before_first)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Earnings: EPS surprise lags by EARNINGS EVENTS (lag1/lag2 = last/prev earnings)\n",
        "def eps_event_lags_to_daily(df: pd.DataFrame, val_col: str, flag_col: str, lag_k: int) -> pd.Series:\n",
        "    \"\"\"Event-based lag for EPS: values only when has_eps_surprise_* == 1, shift by EVENTS.\"\"\"\n",
        "    event_series = df.loc[df[flag_col] == 1, val_col].astype(\"float64\").copy()\n",
        "    shifted = event_series.shift(lag_k)\n",
        "\n",
        "    out = pd.Series(index=df.index, dtype=\"float64\")\n",
        "    out.loc[shifted.index] = shifted.values\n",
        "    out = out.ffill().fillna(0.0)\n",
        "    return out\n",
        "\n",
        "\n",
        "for src in [\"yahoo\", \"calc\"]:\n",
        "    val_col = f\"eps_surprise_pct_{src}\"\n",
        "    flag_col = f\"has_eps_surprise_{src}\"\n",
        "    new_cols[f\"eps_surprise_{src}_lag1\"] = eps_event_lags_to_daily(full_df, val_col, flag_col, lag_k=1)\n",
        "    new_cols[f\"eps_surprise_{src}_lag2\"] = eps_event_lags_to_daily(full_df, val_col, flag_col, lag_k=2)\n",
        "\n",
        "# post-earnings day 1..5 dummies\n",
        "earn = full_df[\"is_earnings_day\"].astype(\"int8\")\n",
        "for k in range(1, 6):\n",
        "    new_cols[f\"post_earnings_day_{k}\"] = earn.shift(k).fillna(0).astype(\"int8\")\n",
        "\n",
        "# EPS flag Ã— vol\n",
        "vol_base = full_df[BASE_VOL_COL].astype(\"float64\")\n",
        "new_cols[\"eps_flag_yahoo_x_vol\"] = full_df[\"has_eps_surprise_yahoo\"].astype(\"float64\") * vol_base\n",
        "new_cols[\"eps_flag_calc_x_vol\"] = full_df[\"has_eps_surprise_calc\"].astype(\"float64\") * vol_base\n",
        "\n",
        "# Macro: CPI (impulse + previous release)\n",
        "cpi_release = full_df[\"CPI_release_day\"].astype(\"int8\")\n",
        "cpi_val = full_df[\"CPI_pct_mom\"].astype(\"float64\")\n",
        "\n",
        "# impulse only on release day\n",
        "new_cols[\"CPI_impulse\"] = cpi_val * cpi_release.astype(\"float64\")\n",
        "\n",
        "# previous release value carried forward (event-lag1)\n",
        "new_cols[\"CPI_prev_release\"] = prev_event_value_to_daily(\n",
        "    full_df, value_col=\"CPI_pct_mom\", release_flag_col=\"CPI_release_day\", lag_k=1, fill_before_first=0.0\n",
        ")\n",
        "\n",
        "# change vs previous release, only meaningful on release days\n",
        "new_cols[\"CPI_change_prev_release\"] = (cpi_val - new_cols[\"CPI_prev_release\"]) * cpi_release.astype(\"float64\")\n",
        "\n",
        "# Macro: FEDFUNDS (impulse + previous decision delta)\n",
        "ff_release = full_df[\"FEDFUNDS_release_day\"].astype(\"int8\")\n",
        "ff_delta = full_df[\"FEDFUNDS_delta_mom\"].astype(\"float64\")\n",
        "\n",
        "new_cols[\"FEDFUNDS_impulse\"] = ff_delta * ff_release.astype(\"float64\")\n",
        "\n",
        "new_cols[\"FEDFUNDS_prev_delta\"] = prev_event_value_to_daily(\n",
        "    full_df, value_col=\"FEDFUNDS_delta_mom\", release_flag_col=\"FEDFUNDS_release_day\", lag_k=1, fill_before_first=0.0\n",
        ")\n",
        "\n",
        "# release-day Ã— market vol\n",
        "mkt_vol = full_df[MARKET_VOL_COL].astype(\"float64\")\n",
        "new_cols[\"FEDFUNDS_release_day_x_mkt_vol\"] = ff_release.astype(\"float64\") * mkt_vol\n",
        "\n",
        "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
        "blk11_df = pd.DataFrame(new_cols, index=full_df.index)\n",
        "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
        "if existing_new_cols:\n",
        "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
        "    full_df = full_df.drop(columns=existing_new_cols)\n",
        "full_df = pd.concat([full_df, blk11_df], axis=1)\n",
        "\n",
        "added = sorted(set(full_df.columns) - cols_before)\n",
        "\n",
        "print(f\"\\n[OK] BLOCK 11 added {len(added)} event/sparse-signal columns.\")\n",
        "print(\"[INFO] Added columns:\")\n",
        "for c in added:\n",
        "    print(\"  -\", c)\n",
        "\n",
        "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(12)\n",
        "print(\"\\n[CHECK] NaNs in NEW Block-11 features (top 12):\")\n",
        "print(nan_top)\n",
        "\n",
        "# Sanity: impulse should be non-zero only on release days\n",
        "for col_imp, flag in [(\"CPI_impulse\", \"CPI_release_day\"), (\"FEDFUNDS_impulse\", \"FEDFUNDS_release_day\")]:\n",
        "    if col_imp in full_df.columns and flag in full_df.columns:\n",
        "        nz_days = int((full_df[col_imp].abs() > 0).sum())\n",
        "        flag_days = int(full_df[flag].sum())\n",
        "        print(f\"\\n[CHECK] {col_imp}: nonzero_days={nz_days} | {flag} count={flag_days}\")\n",
        "\n",
        "print(\"[OK] BLOCK 11 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829e7273",
      "metadata": {
        "id": "829e7273"
      },
      "source": [
        "## BLOCK 12 â€” CRISIS PERIOD FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "4f3fd6cc",
      "metadata": {
        "id": "4f3fd6cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3638e73e-d51b-431f-d071-f299158d59ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] BLOCK 12 crisis flags added/updated. Newly added cols: 6\n",
            "[INFO] Counts: {'pre_covid': 0, 'covid_period': 0, 'post_covid': 540, 'pre_crisis_2008': 0, 'crisis_2008': 0, 'post_crisis_2008': 540}\n",
            "[OK] BLOCK 12 complete. full_df shape: (540, 394)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "cols_before = set(full_df.columns)\n",
        "\n",
        "# Date windows from RUN_PARAMS\n",
        "covid_start = pd.Timestamp(RUN_PARAMS[\"features\"][\"covid_start\"])\n",
        "covid_end = pd.Timestamp(RUN_PARAMS[\"features\"][\"covid_end\"])\n",
        "\n",
        "crisis_2008_start = pd.Timestamp(RUN_PARAMS[\"features\"][\"crisis_2008_start\"])\n",
        "crisis_2008_end = pd.Timestamp(RUN_PARAMS[\"features\"][\"crisis_2008_end\"])\n",
        "\n",
        "# Build flags (overwrite-safe)\n",
        "full_df[\"covid_period\"] = ((full_df.index >= covid_start) & (full_df.index <= covid_end)).astype(\"int8\")\n",
        "full_df[\"pre_covid\"] = (full_df.index < covid_start).astype(\"int8\")\n",
        "full_df[\"post_covid\"] = (full_df.index > covid_end).astype(\"int8\")\n",
        "\n",
        "full_df[\"crisis_2008\"] = ((full_df.index >= crisis_2008_start) & (full_df.index <= crisis_2008_end)).astype(\"int8\")\n",
        "full_df[\"pre_crisis_2008\"] = (full_df.index < crisis_2008_start).astype(\"int8\")\n",
        "full_df[\"post_crisis_2008\"] = (full_df.index > crisis_2008_end).astype(\"int8\")\n",
        "\n",
        "# Sanity checks: mutually exclusive within each regime triad (pre / in / post)\n",
        "bad_covid = int(((full_df[\"pre_covid\"] + full_df[\"covid_period\"] + full_df[\"post_covid\"]) != 1).sum())\n",
        "bad_2008 = int(((full_df[\"pre_crisis_2008\"] + full_df[\"crisis_2008\"] + full_df[\"post_crisis_2008\"]) != 1).sum())\n",
        "assert bad_covid == 0, \"[ERROR] COVID flags are not mutually exclusive.\"\n",
        "assert bad_2008 == 0, \"[ERROR] Crisis flags are not mutually exclusive.\"\n",
        "\n",
        "cols_after = set(full_df.columns)\n",
        "added = sorted(cols_after - cols_before)\n",
        "\n",
        "counts = {\n",
        "    \"pre_covid\": int(full_df[\"pre_covid\"].sum()),\n",
        "    \"covid_period\": int(full_df[\"covid_period\"].sum()),\n",
        "    \"post_covid\": int(full_df[\"post_covid\"].sum()),\n",
        "    \"pre_crisis_2008\": int(full_df[\"pre_crisis_2008\"].sum()),\n",
        "    \"crisis_2008\": int(full_df[\"crisis_2008\"].sum()),\n",
        "    \"post_crisis_2008\": int(full_df[\"post_crisis_2008\"].sum()),\n",
        "}\n",
        "\n",
        "print(f\"[OK] BLOCK 12 crisis flags added/updated. Newly added cols: {len(added)}\")\n",
        "print(\"[INFO] Counts:\", counts)\n",
        "\n",
        "print(\"[OK] BLOCK 12 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2553698",
      "metadata": {
        "id": "f2553698"
      },
      "source": [
        "## BLOCK 13 â€” DEFINE TARGET (NEXT-DAY LOG RETURN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "863f45cd",
      "metadata": {
        "id": "863f45cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c77b37-0908-4655-f927-2ae2d8ff1a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Target NaNs: 1 out of 540\n",
            "[OK] Target defined: GOOGL_logret_t1\n",
            "[INFO] Target tail preview (before drop):\n",
            "            GOOGL_logret_t1\n",
            "Date                       \n",
            "2026-01-13        -0.000387\n",
            "2026-01-14         0.000000\n",
            "2026-01-15              NaN\n",
            "[OK] Dropped last row with NaN target: 540 -> 539\n",
            "[INFO] New index range: 2023-11-20 00:00:00 -> 2026-01-14 00:00:00\n",
            "[INFO] Target tail preview (after drop):\n",
            "            GOOGL_logret_t1\n",
            "Date                       \n",
            "2026-01-12         0.012309\n",
            "2026-01-13        -0.000387\n",
            "2026-01-14         0.000000\n",
            "[OK] BLOCK 13 complete. full_df shape: (539, 395)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "SRC_COL = str(RUN_PARAMS[\"data\"][\"target_src_col\"])\n",
        "\n",
        "assert SRC_COL in full_df.columns, f\"[ERROR] Missing {SRC_COL}.\"\n",
        "\n",
        "# Deterministic overwrite on rerun\n",
        "if TARGET_T1 in full_df.columns:\n",
        "    print(f\"[WARN] {TARGET_T1} already exists â€” overwriting it deterministically (shift(-1) of {SRC_COL}).\")\n",
        "\n",
        "full_df[TARGET_T1] = full_df[SRC_COL].shift(-1).astype(\"float64\")\n",
        "\n",
        "# Sanity: expect exactly 1 NaN at the end\n",
        "n_nan = int(full_df[TARGET_T1].isna().sum())\n",
        "print(\"[INFO] Target NaNs:\", n_nan, \"out of\", len(full_df))\n",
        "assert n_nan == 1, f\"[ERROR] Expected exactly 1 NaN in {TARGET_T1} (last row). Found {n_nan}.\"\n",
        "\n",
        "print(\"[OK] Target defined:\", TARGET_T1)\n",
        "print(\"[INFO] Target tail preview (before drop):\")\n",
        "print(full_df[[TARGET_T1]].tail(3))\n",
        "\n",
        "# Drop last row (NaN target)\n",
        "before = len(full_df)\n",
        "full_df = full_df.dropna(subset=[TARGET_T1]).copy()\n",
        "after = len(full_df)\n",
        "\n",
        "print(f\"[OK] Dropped last row with NaN target: {before} -> {after}\")\n",
        "print(\"[INFO] New index range:\", full_df.index.min(), \"->\", full_df.index.max())\n",
        "print(\"[INFO] Target tail preview (after drop):\")\n",
        "print(full_df[[TARGET_T1]].tail(3))\n",
        "\n",
        "# Persist target metadata into RUN_PARAMS\n",
        "RUN_PARAMS.setdefault(\"data\", {})\n",
        "RUN_PARAMS[\"data\"][\"target_src_col\"] = SRC_COL\n",
        "RUN_PARAMS[\"data\"][\"target_col\"] = TARGET_T1\n",
        "\n",
        "# Save target metadata\n",
        "meta = {\n",
        "    \"target_src_col\": SRC_COL,\n",
        "    \"target_col\": TARGET_T1,\n",
        "    \"rows_before_drop\": int(before),\n",
        "    \"rows_after_drop\": int(after),\n",
        "    \"min_date\": str(full_df.index.min()),\n",
        "    \"max_date\": str(full_df.index.max()),\n",
        "}\n",
        "save_json(meta, OUTPUTS_DIR / \"target_meta.json\")\n",
        "save_json(meta, DRIVE_PATHS[\"outputs_dir\"] / \"target_meta.json\")\n",
        "\n",
        "print(\"[OK] BLOCK 13 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9513f150",
      "metadata": {
        "id": "9513f150"
      },
      "source": [
        "## BLOCK 14 â€” DROP DUPLICATE FEATURE COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "6abdf2fd",
      "metadata": {
        "id": "6abdf2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664fa72d-799b-4b3f-b091-cf07fdbb0190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] No duplicate column names found. Nothing to drop.\n",
            "[OK] BLOCK 14 complete. full_df shape: (539, 395)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
        "\n",
        "dup_mask = full_df.columns.duplicated(keep=\"last\")\n",
        "dup_cols = full_df.columns[dup_mask].tolist()\n",
        "\n",
        "if len(dup_cols) == 0:\n",
        "    print(\"[OK] No duplicate column names found. Nothing to drop.\")\n",
        "else:\n",
        "    before_shape = full_df.shape\n",
        "    full_df = full_df.loc[:, ~dup_mask].copy()\n",
        "    after_shape = full_df.shape\n",
        "\n",
        "    print(f\"[WARN] Dropped {len(dup_cols)} duplicate columns (keep_last).\")\n",
        "    print(\"[INFO] Shape:\", before_shape, \"->\", after_shape)\n",
        "    print(\"[INFO] Example dropped duplicates (first 30):\", dup_cols[:30])\n",
        "\n",
        "print(\"[OK] BLOCK 14 complete. full_df shape:\", full_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d31044",
      "metadata": {
        "id": "68d31044"
      },
      "source": [
        "## BLOCK 15 â€” LIMIT PERIOD + SAVE SNAPSHOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "c2937331",
      "metadata": {
        "id": "c2937331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad5a34c-e000-4662-d441-fa0f2d303bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved full_df snapshot to:\n",
            "  - local: /content/my_project/data/interim/full_df.pkl\n",
            "  - drive: /content/drive/MyDrive/my_project/data/interim/full_df.pkl\n",
            "[INFO] full_df rows: 511 | range: 2024-01-02 00:00:00 -> 2026-01-14 00:00:00\n",
            "[OK] BLOCK 15 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "# Limit start date from RUN_PARAMS\n",
        "limit_start = str(RUN_PARAMS[\"data\"][\"limit_start_date\"])\n",
        "full_df = full_df.loc[limit_start:].copy()\n",
        "\n",
        "# Save to interim\n",
        "INTERIM_DIR_LOCAL = DATA_DIRS_LOCAL[\"interim\"]\n",
        "INTERIM_DIR_DRIVE = DATA_DIRS_DRIVE[\"interim\"]\n",
        "\n",
        "out_local = INTERIM_DIR_LOCAL / \"full_df.pkl\"\n",
        "out_drive = INTERIM_DIR_DRIVE / \"full_df.pkl\"\n",
        "\n",
        "full_df.to_pickle(out_local)\n",
        "copy_file(out_local, out_drive)\n",
        "\n",
        "print(\"[OK] Saved full_df snapshot to:\")\n",
        "print(\"  - local:\", out_local)\n",
        "print(\"  - drive:\", out_drive)\n",
        "print(\"[INFO] full_df rows:\", len(full_df), \"| range:\", full_df.index.min(), \"->\", full_df.index.max())\n",
        "\n",
        "print(\"[OK] BLOCK 15 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c984abc2",
      "metadata": {
        "id": "c984abc2"
      },
      "source": [
        "## BLOCK 16 â€” FULL COMPLETENESS SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "8b459f8a",
      "metadata": {
        "id": "8b459f8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673eab95-021e-484a-e5a6-cc3b63c7bc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "[full_df (after feature blocks + time limit)]\n",
            "==========================================================================================\n",
            "Total rows: 511\n",
            "Total cols: 395\n",
            "Index range: 2024-01-02 00:00:00 -> 2026-01-14 00:00:00\n",
            "\n",
            "[Column-wise completeness]\n",
            "                                  dtype  non_null  missing  missing_%\n",
            "GOOGL_Close                     float64       511        0        0.0\n",
            "GOOGL_High                      float64       511        0        0.0\n",
            "GOOGL_Low                       float64       511        0        0.0\n",
            "GOOGL_Open                      float64       511        0        0.0\n",
            "GOOGL_Volume                    float64       511        0        0.0\n",
            "MSFT_Close                      float64       511        0        0.0\n",
            "MSFT_High                       float64       511        0        0.0\n",
            "MSFT_Low                        float64       511        0        0.0\n",
            "MSFT_Open                       float64       511        0        0.0\n",
            "MSFT_Volume                     float64       511        0        0.0\n",
            "NVDA_Close                      float64       511        0        0.0\n",
            "NVDA_High                       float64       511        0        0.0\n",
            "NVDA_Low                        float64       511        0        0.0\n",
            "NVDA_Open                       float64       511        0        0.0\n",
            "NVDA_Volume                     float64       511        0        0.0\n",
            "^IXIC_Close                     float64       511        0        0.0\n",
            "^IXIC_High                      float64       511        0        0.0\n",
            "^IXIC_Low                       float64       511        0        0.0\n",
            "^IXIC_Open                      float64       511        0        0.0\n",
            "^IXIC_Volume                    float64       511        0        0.0\n",
            "SPY_Close                       float64       511        0        0.0\n",
            "SPY_High                        float64       511        0        0.0\n",
            "SPY_Low                         float64       511        0        0.0\n",
            "SPY_Open                        float64       511        0        0.0\n",
            "SPY_Volume                      float64       511        0        0.0\n",
            "QQQ_Close                       float64       511        0        0.0\n",
            "QQQ_High                        float64       511        0        0.0\n",
            "QQQ_Low                         float64       511        0        0.0\n",
            "QQQ_Open                        float64       511        0        0.0\n",
            "QQQ_Volume                      float64       511        0        0.0\n",
            "^VIX_Close                      float64       511        0        0.0\n",
            "^VIX_High                       float64       511        0        0.0\n",
            "^VIX_Low                        float64       511        0        0.0\n",
            "^VIX_Open                       float64       511        0        0.0\n",
            "^TNX_Close                      float64       511        0        0.0\n",
            "^TNX_High                       float64       511        0        0.0\n",
            "^TNX_Low                        float64       511        0        0.0\n",
            "^TNX_Open                       float64       511        0        0.0\n",
            "XLK_Close                       float64       511        0        0.0\n",
            "XLK_High                        float64       511        0        0.0\n",
            "XLK_Low                         float64       511        0        0.0\n",
            "XLK_Open                        float64       511        0        0.0\n",
            "XLK_Volume                      float64       511        0        0.0\n",
            "^GDAXI_Close                    float64       511        0        0.0\n",
            "^GDAXI_High                     float64       511        0        0.0\n",
            "^GDAXI_Low                      float64       511        0        0.0\n",
            "^GDAXI_Open                     float64       511        0        0.0\n",
            "is_earnings_day                    int8       511        0        0.0\n",
            "eps_surprise_pct_yahoo          float64       511        0        0.0\n",
            "has_eps_surprise_yahoo             int8       511        0        0.0\n",
            "eps_surprise_pct_calc           float64       511        0        0.0\n",
            "has_eps_surprise_calc              int8       511        0        0.0\n",
            "EU_break_close_flag                int8       511        0        0.0\n",
            "EU_break_close_up                  int8       511        0        0.0\n",
            "EU_break_close_down                int8       511        0        0.0\n",
            "CPI_pct_mom                     float64       511        0        0.0\n",
            "CPI_accel_pct_mom               float64       511        0        0.0\n",
            "FEDFUNDS_delta_mom              float64       511        0        0.0\n",
            "FEDFUNDS_changed                   int8       511        0        0.0\n",
            "FEDFUNDS_level                  float64       511        0        0.0\n",
            "CPI_pct_mom_is_missing             int8       511        0        0.0\n",
            "CPI_accel_pct_mom_is_missing       int8       511        0        0.0\n",
            "FEDFUNDS_delta_mom_is_missing      int8       511        0        0.0\n",
            "CPI_release_day                    int8       511        0        0.0\n",
            "FEDFUNDS_release_day               int8       511        0        0.0\n",
            "day_of_week                        int8       511        0        0.0\n",
            "month                              int8       511        0        0.0\n",
            "quarter                            int8       511        0        0.0\n",
            "is_q1                              int8       511        0        0.0\n",
            "is_q2                              int8       511        0        0.0\n",
            "is_q3                              int8       511        0        0.0\n",
            "is_q4                              int8       511        0        0.0\n",
            "weekday_sin_5                   float64       511        0        0.0\n",
            "weekday_cos_5                   float64       511        0        0.0\n",
            "day_of_year_sin                 float64       511        0        0.0\n",
            "day_of_year_cos                 float64       511        0        0.0\n",
            "GOOGL_logret_cc                 float64       511        0        0.0\n",
            "GOOGL_logret_oc                 float64       511        0        0.0\n",
            "GOOGL_logret_gap_co             float64       511        0        0.0\n",
            "GOOGL_abs_logret_cc             float64       511        0        0.0\n",
            "GOOGL_abs_logret_oc             float64       511        0        0.0\n",
            "GOOGL_abs_logret_gap_co         float64       511        0        0.0\n",
            "GOOGL_log_hl                    float64       511        0        0.0\n",
            "GOOGL_close_pos_hl              float64       511        0        0.0\n",
            "GOOGL_close_pos_hl_centered     float64       511        0        0.0\n",
            "GOOGL_logret_cc_lag1            float64       511        0        0.0\n",
            "GOOGL_logret_cc_lag5            float64       511        0        0.0\n",
            "GOOGL_logret_cc_lag21           float64       511        0        0.0\n",
            "MSFT_logret_cc                  float64       511        0        0.0\n",
            "MSFT_logret_oc                  float64       511        0        0.0\n",
            "MSFT_logret_gap_co              float64       511        0        0.0\n",
            "MSFT_abs_logret_cc              float64       511        0        0.0\n",
            "MSFT_abs_logret_oc              float64       511        0        0.0\n",
            "MSFT_abs_logret_gap_co          float64       511        0        0.0\n",
            "MSFT_log_hl                     float64       511        0        0.0\n",
            "MSFT_close_pos_hl               float64       511        0        0.0\n",
            "MSFT_close_pos_hl_centered      float64       511        0        0.0\n",
            "MSFT_logret_cc_lag1             float64       511        0        0.0\n",
            "MSFT_logret_cc_lag5             float64       511        0        0.0\n",
            "MSFT_logret_cc_lag21            float64       511        0        0.0\n",
            "NVDA_logret_cc                  float64       511        0        0.0\n",
            "NVDA_logret_oc                  float64       511        0        0.0\n",
            "NVDA_logret_gap_co              float64       511        0        0.0\n",
            "NVDA_abs_logret_cc              float64       511        0        0.0\n",
            "NVDA_abs_logret_oc              float64       511        0        0.0\n",
            "NVDA_abs_logret_gap_co          float64       511        0        0.0\n",
            "NVDA_log_hl                     float64       511        0        0.0\n",
            "NVDA_close_pos_hl               float64       511        0        0.0\n",
            "NVDA_close_pos_hl_centered      float64       511        0        0.0\n",
            "NVDA_logret_cc_lag1             float64       511        0        0.0\n",
            "NVDA_logret_cc_lag5             float64       511        0        0.0\n",
            "NVDA_logret_cc_lag21            float64       511        0        0.0\n",
            "QQQ_logret_cc                   float64       511        0        0.0\n",
            "QQQ_logret_oc                   float64       511        0        0.0\n",
            "QQQ_logret_gap_co               float64       511        0        0.0\n",
            "QQQ_abs_logret_cc               float64       511        0        0.0\n",
            "QQQ_abs_logret_oc               float64       511        0        0.0\n",
            "QQQ_abs_logret_gap_co           float64       511        0        0.0\n",
            "QQQ_log_hl                      float64       511        0        0.0\n",
            "QQQ_close_pos_hl                float64       511        0        0.0\n",
            "QQQ_close_pos_hl_centered       float64       511        0        0.0\n",
            "QQQ_logret_cc_lag1              float64       511        0        0.0\n",
            "QQQ_logret_cc_lag5              float64       511        0        0.0\n",
            "QQQ_logret_cc_lag21             float64       511        0        0.0\n",
            "SPY_logret_cc                   float64       511        0        0.0\n",
            "SPY_logret_oc                   float64       511        0        0.0\n",
            "SPY_logret_gap_co               float64       511        0        0.0\n",
            "SPY_abs_logret_cc               float64       511        0        0.0\n",
            "SPY_abs_logret_oc               float64       511        0        0.0\n",
            "SPY_abs_logret_gap_co           float64       511        0        0.0\n",
            "SPY_log_hl                      float64       511        0        0.0\n",
            "SPY_close_pos_hl                float64       511        0        0.0\n",
            "SPY_close_pos_hl_centered       float64       511        0        0.0\n",
            "SPY_logret_cc_lag1              float64       511        0        0.0\n",
            "SPY_logret_cc_lag5              float64       511        0        0.0\n",
            "SPY_logret_cc_lag21             float64       511        0        0.0\n",
            "XLK_logret_cc                   float64       511        0        0.0\n",
            "XLK_logret_oc                   float64       511        0        0.0\n",
            "XLK_logret_gap_co               float64       511        0        0.0\n",
            "XLK_abs_logret_cc               float64       511        0        0.0\n",
            "XLK_abs_logret_oc               float64       511        0        0.0\n",
            "XLK_abs_logret_gap_co           float64       511        0        0.0\n",
            "XLK_log_hl                      float64       511        0        0.0\n",
            "XLK_close_pos_hl                float64       511        0        0.0\n",
            "XLK_close_pos_hl_centered       float64       511        0        0.0\n",
            "XLK_logret_cc_lag1              float64       511        0        0.0\n",
            "XLK_logret_cc_lag5              float64       511        0        0.0\n",
            "XLK_logret_cc_lag21             float64       511        0        0.0\n",
            "^IXIC_logret_cc                 float64       511        0        0.0\n",
            "^IXIC_logret_oc                 float64       511        0        0.0\n",
            "^IXIC_logret_gap_co             float64       511        0        0.0\n",
            "^IXIC_abs_logret_cc             float64       511        0        0.0\n",
            "^IXIC_abs_logret_oc             float64       511        0        0.0\n",
            "^IXIC_abs_logret_gap_co         float64       511        0        0.0\n",
            "^IXIC_log_hl                    float64       511        0        0.0\n",
            "^IXIC_close_pos_hl              float64       511        0        0.0\n",
            "^IXIC_close_pos_hl_centered     float64       511        0        0.0\n",
            "^IXIC_logret_cc_lag1            float64       511        0        0.0\n",
            "^IXIC_logret_cc_lag5            float64       511        0        0.0\n",
            "^IXIC_logret_cc_lag21           float64       511        0        0.0\n",
            "GOOGL_log_vol                   float64       511        0        0.0\n",
            "GOOGL_log_vol_chg_1d            float64       511        0        0.0\n",
            "GOOGL_log_dollar_vol            float64       511        0        0.0\n",
            "MSFT_log_vol                    float64       511        0        0.0\n",
            "MSFT_log_vol_chg_1d             float64       511        0        0.0\n",
            "MSFT_log_dollar_vol             float64       511        0        0.0\n",
            "NVDA_log_vol                    float64       511        0        0.0\n",
            "NVDA_log_vol_chg_1d             float64       511        0        0.0\n",
            "NVDA_log_dollar_vol             float64       511        0        0.0\n",
            "QQQ_log_vol                     float64       511        0        0.0\n",
            "QQQ_log_vol_chg_1d              float64       511        0        0.0\n",
            "QQQ_log_dollar_vol              float64       511        0        0.0\n",
            "SPY_log_vol                     float64       511        0        0.0\n",
            "SPY_log_vol_chg_1d              float64       511        0        0.0\n",
            "SPY_log_dollar_vol              float64       511        0        0.0\n",
            "XLK_log_vol                     float64       511        0        0.0\n",
            "XLK_log_vol_chg_1d              float64       511        0        0.0\n",
            "XLK_log_dollar_vol              float64       511        0        0.0\n",
            "^IXIC_log_vol                   float64       511        0        0.0\n",
            "^IXIC_log_vol_chg_1d            float64       511        0        0.0\n",
            "^IXIC_log_dollar_vol            float64       511        0        0.0\n",
            "GOOGL_logret_cc_mean_5          float64       511        0        0.0\n",
            "GOOGL_logret_cc_std_5           float64       511        0        0.0\n",
            "GOOGL_logret_cc_mean_21         float64       511        0        0.0\n",
            "GOOGL_logret_cc_std_21          float64       511        0        0.0\n",
            "GOOGL_logret_cc_z_21            float64       511        0        0.0\n",
            "GOOGL_abs_logret_cc_mean_5      float64       511        0        0.0\n",
            "GOOGL_abs_logret_cc_std_5       float64       511        0        0.0\n",
            "GOOGL_abs_logret_cc_mean_21     float64       511        0        0.0\n",
            "GOOGL_abs_logret_cc_std_21      float64       511        0        0.0\n",
            "GOOGL_log_hl_mean_5             float64       511        0        0.0\n",
            "GOOGL_log_hl_std_5              float64       511        0        0.0\n",
            "GOOGL_log_hl_mean_21            float64       511        0        0.0\n",
            "GOOGL_log_hl_std_21             float64       511        0        0.0\n",
            "GOOGL_log_hl_z_21               float64       511        0        0.0\n",
            "MSFT_logret_cc_mean_5           float64       511        0        0.0\n",
            "MSFT_logret_cc_std_5            float64       511        0        0.0\n",
            "MSFT_logret_cc_mean_21          float64       511        0        0.0\n",
            "MSFT_logret_cc_std_21           float64       511        0        0.0\n",
            "MSFT_logret_cc_z_21             float64       511        0        0.0\n",
            "MSFT_abs_logret_cc_mean_5       float64       511        0        0.0\n",
            "MSFT_abs_logret_cc_std_5        float64       511        0        0.0\n",
            "MSFT_abs_logret_cc_mean_21      float64       511        0        0.0\n",
            "MSFT_abs_logret_cc_std_21       float64       511        0        0.0\n",
            "MSFT_log_hl_mean_5              float64       511        0        0.0\n",
            "MSFT_log_hl_std_5               float64       511        0        0.0\n",
            "MSFT_log_hl_mean_21             float64       511        0        0.0\n",
            "MSFT_log_hl_std_21              float64       511        0        0.0\n",
            "MSFT_log_hl_z_21                float64       511        0        0.0\n",
            "NVDA_logret_cc_mean_5           float64       511        0        0.0\n",
            "NVDA_logret_cc_std_5            float64       511        0        0.0\n",
            "NVDA_logret_cc_mean_21          float64       511        0        0.0\n",
            "NVDA_logret_cc_std_21           float64       511        0        0.0\n",
            "NVDA_logret_cc_z_21             float64       511        0        0.0\n",
            "NVDA_abs_logret_cc_mean_5       float64       511        0        0.0\n",
            "NVDA_abs_logret_cc_std_5        float64       511        0        0.0\n",
            "NVDA_abs_logret_cc_mean_21      float64       511        0        0.0\n",
            "NVDA_abs_logret_cc_std_21       float64       511        0        0.0\n",
            "NVDA_log_hl_mean_5              float64       511        0        0.0\n",
            "NVDA_log_hl_std_5               float64       511        0        0.0\n",
            "NVDA_log_hl_mean_21             float64       511        0        0.0\n",
            "NVDA_log_hl_std_21              float64       511        0        0.0\n",
            "NVDA_log_hl_z_21                float64       511        0        0.0\n",
            "QQQ_logret_cc_mean_5            float64       511        0        0.0\n",
            "QQQ_logret_cc_std_5             float64       511        0        0.0\n",
            "QQQ_logret_cc_mean_21           float64       511        0        0.0\n",
            "QQQ_logret_cc_std_21            float64       511        0        0.0\n",
            "QQQ_logret_cc_z_21              float64       511        0        0.0\n",
            "QQQ_abs_logret_cc_mean_5        float64       511        0        0.0\n",
            "QQQ_abs_logret_cc_std_5         float64       511        0        0.0\n",
            "QQQ_abs_logret_cc_mean_21       float64       511        0        0.0\n",
            "QQQ_abs_logret_cc_std_21        float64       511        0        0.0\n",
            "QQQ_log_hl_mean_5               float64       511        0        0.0\n",
            "QQQ_log_hl_std_5                float64       511        0        0.0\n",
            "QQQ_log_hl_mean_21              float64       511        0        0.0\n",
            "QQQ_log_hl_std_21               float64       511        0        0.0\n",
            "QQQ_log_hl_z_21                 float64       511        0        0.0\n",
            "SPY_logret_cc_mean_5            float64       511        0        0.0\n",
            "SPY_logret_cc_std_5             float64       511        0        0.0\n",
            "SPY_logret_cc_mean_21           float64       511        0        0.0\n",
            "SPY_logret_cc_std_21            float64       511        0        0.0\n",
            "SPY_logret_cc_z_21              float64       511        0        0.0\n",
            "SPY_abs_logret_cc_mean_5        float64       511        0        0.0\n",
            "SPY_abs_logret_cc_std_5         float64       511        0        0.0\n",
            "SPY_abs_logret_cc_mean_21       float64       511        0        0.0\n",
            "SPY_abs_logret_cc_std_21        float64       511        0        0.0\n",
            "SPY_log_hl_mean_5               float64       511        0        0.0\n",
            "SPY_log_hl_std_5                float64       511        0        0.0\n",
            "SPY_log_hl_mean_21              float64       511        0        0.0\n",
            "SPY_log_hl_std_21               float64       511        0        0.0\n",
            "SPY_log_hl_z_21                 float64       511        0        0.0\n",
            "XLK_logret_cc_mean_5            float64       511        0        0.0\n",
            "XLK_logret_cc_std_5             float64       511        0        0.0\n",
            "XLK_logret_cc_mean_21           float64       511        0        0.0\n",
            "XLK_logret_cc_std_21            float64       511        0        0.0\n",
            "XLK_logret_cc_z_21              float64       511        0        0.0\n",
            "XLK_abs_logret_cc_mean_5        float64       511        0        0.0\n",
            "XLK_abs_logret_cc_std_5         float64       511        0        0.0\n",
            "XLK_abs_logret_cc_mean_21       float64       511        0        0.0\n",
            "XLK_abs_logret_cc_std_21        float64       511        0        0.0\n",
            "XLK_log_hl_mean_5               float64       511        0        0.0\n",
            "XLK_log_hl_std_5                float64       511        0        0.0\n",
            "XLK_log_hl_mean_21              float64       511        0        0.0\n",
            "XLK_log_hl_std_21               float64       511        0        0.0\n",
            "XLK_log_hl_z_21                 float64       511        0        0.0\n",
            "^IXIC_logret_cc_mean_5          float64       511        0        0.0\n",
            "^IXIC_logret_cc_std_5           float64       511        0        0.0\n",
            "^IXIC_logret_cc_mean_21         float64       511        0        0.0\n",
            "^IXIC_logret_cc_std_21          float64       511        0        0.0\n",
            "^IXIC_logret_cc_z_21            float64       511        0        0.0\n",
            "^IXIC_abs_logret_cc_mean_5      float64       511        0        0.0\n",
            "^IXIC_abs_logret_cc_std_5       float64       511        0        0.0\n",
            "^IXIC_abs_logret_cc_mean_21     float64       511        0        0.0\n",
            "^IXIC_abs_logret_cc_std_21      float64       511        0        0.0\n",
            "^IXIC_log_hl_mean_5             float64       511        0        0.0\n",
            "^IXIC_log_hl_std_5              float64       511        0        0.0\n",
            "^IXIC_log_hl_mean_21            float64       511        0        0.0\n",
            "^IXIC_log_hl_std_21             float64       511        0        0.0\n",
            "^IXIC_log_hl_z_21               float64       511        0        0.0\n",
            "GOOGL_log_vol_mean_5            float64       511        0        0.0\n",
            "GOOGL_log_vol_std_5             float64       511        0        0.0\n",
            "GOOGL_log_vol_mean_21           float64       511        0        0.0\n",
            "GOOGL_log_vol_std_21            float64       511        0        0.0\n",
            "GOOGL_log_vol_z_21              float64       511        0        0.0\n",
            "MSFT_log_vol_mean_5             float64       511        0        0.0\n",
            "MSFT_log_vol_std_5              float64       511        0        0.0\n",
            "MSFT_log_vol_mean_21            float64       511        0        0.0\n",
            "MSFT_log_vol_std_21             float64       511        0        0.0\n",
            "MSFT_log_vol_z_21               float64       511        0        0.0\n",
            "NVDA_log_vol_mean_5             float64       511        0        0.0\n",
            "NVDA_log_vol_std_5              float64       511        0        0.0\n",
            "NVDA_log_vol_mean_21            float64       511        0        0.0\n",
            "NVDA_log_vol_std_21             float64       511        0        0.0\n",
            "NVDA_log_vol_z_21               float64       511        0        0.0\n",
            "QQQ_log_vol_mean_5              float64       511        0        0.0\n",
            "QQQ_log_vol_std_5               float64       511        0        0.0\n",
            "QQQ_log_vol_mean_21             float64       511        0        0.0\n",
            "QQQ_log_vol_std_21              float64       511        0        0.0\n",
            "QQQ_log_vol_z_21                float64       511        0        0.0\n",
            "SPY_log_vol_mean_5              float64       511        0        0.0\n",
            "SPY_log_vol_std_5               float64       511        0        0.0\n",
            "SPY_log_vol_mean_21             float64       511        0        0.0\n",
            "SPY_log_vol_std_21              float64       511        0        0.0\n",
            "SPY_log_vol_z_21                float64       511        0        0.0\n",
            "XLK_log_vol_mean_5              float64       511        0        0.0\n",
            "XLK_log_vol_std_5               float64       511        0        0.0\n",
            "XLK_log_vol_mean_21             float64       511        0        0.0\n",
            "XLK_log_vol_std_21              float64       511        0        0.0\n",
            "XLK_log_vol_z_21                float64       511        0        0.0\n",
            "^IXIC_log_vol_mean_5            float64       511        0        0.0\n",
            "^IXIC_log_vol_std_5             float64       511        0        0.0\n",
            "^IXIC_log_vol_mean_21           float64       511        0        0.0\n",
            "^IXIC_log_vol_std_21            float64       511        0        0.0\n",
            "^IXIC_log_vol_z_21              float64       511        0        0.0\n",
            "GOOGL_corr_SPY_5                float64       511        0        0.0\n",
            "GOOGL_beta_SPY_5                float64       511        0        0.0\n",
            "GOOGL_corr_SPY_21               float64       511        0        0.0\n",
            "GOOGL_beta_SPY_21               float64       511        0        0.0\n",
            "GOOGL_corr_QQQ_5                float64       511        0        0.0\n",
            "GOOGL_beta_QQQ_5                float64       511        0        0.0\n",
            "GOOGL_corr_QQQ_21               float64       511        0        0.0\n",
            "GOOGL_beta_QQQ_21               float64       511        0        0.0\n",
            "GOOGL_corr_^IXIC_5              float64       511        0        0.0\n",
            "GOOGL_beta_^IXIC_5              float64       511        0        0.0\n",
            "GOOGL_corr_^IXIC_21             float64       511        0        0.0\n",
            "GOOGL_beta_^IXIC_21             float64       511        0        0.0\n",
            "GOOGL_corr_XLK_5                float64       511        0        0.0\n",
            "GOOGL_beta_XLK_5                float64       511        0        0.0\n",
            "GOOGL_corr_XLK_21               float64       511        0        0.0\n",
            "GOOGL_beta_XLK_21               float64       511        0        0.0\n",
            "GOOGL_vol_ratio_5_21            float64       511        0        0.0\n",
            "GOOGL_vol_diff_5_21             float64       511        0        0.0\n",
            "GOOGL_vol_regime_score          float64       511        0        0.0\n",
            "GOOGL_price_struct_1            float64       511        0        0.0\n",
            "GOOGL_price_struct_2            float64       511        0        0.0\n",
            "GOOGL_ctx_beta_vol_SPY          float64       511        0        0.0\n",
            "GOOGL_ctx_corr_volratio_SPY     float64       511        0        0.0\n",
            "GOOGL_ctx_beta_vol_QQQ          float64       511        0        0.0\n",
            "GOOGL_ctx_corr_volratio_QQQ     float64       511        0        0.0\n",
            "GOOGL_ctx_beta_vol_^IXIC        float64       511        0        0.0\n",
            "GOOGL_ctx_corr_volratio_^IXIC   float64       511        0        0.0\n",
            "GOOGL_ctx_beta_vol_XLK          float64       511        0        0.0\n",
            "GOOGL_ctx_corr_volratio_XLK     float64       511        0        0.0\n",
            "GOOGL_vix_vol_interact          float64       511        0        0.0\n",
            "GOOGL_fedfunds_trend_interact   float64       511        0        0.0\n",
            "GOOGL_cpi_trend_interact        float64       511        0        0.0\n",
            "VIX_log_level                   float64       511        0        0.0\n",
            "VIX_delta_1d                    float64       511        0        0.0\n",
            "VIX_abs_delta_1d                float64       511        0        0.0\n",
            "VIX_log_hl                      float64       511        0        0.0\n",
            "VIX_range_frac                  float64       511        0        0.0\n",
            "VIX_gap_oc                      float64       511        0        0.0\n",
            "VIX_delta_1d_lag1               float64       511        0        0.0\n",
            "VIX_abs_delta_1d_lag1           float64       511        0        0.0\n",
            "VIX_delta_1d_lag5               float64       511        0        0.0\n",
            "VIX_abs_delta_1d_lag5           float64       511        0        0.0\n",
            "VIX_delta_1d_lag21              float64       511        0        0.0\n",
            "VIX_abs_delta_1d_lag21          float64       511        0        0.0\n",
            "TNX_level                       float64       511        0        0.0\n",
            "TNX_delta_1d                    float64       511        0        0.0\n",
            "TNX_abs_delta_1d                float64       511        0        0.0\n",
            "TNX_delta_5d                    float64       511        0        0.0\n",
            "TNX_log_hl                      float64       511        0        0.0\n",
            "TNX_range                       float64       511        0        0.0\n",
            "TNX_gap_oc                      float64       511        0        0.0\n",
            "TNX_delta_1d_lag1               float64       511        0        0.0\n",
            "TNX_abs_delta_1d_lag1           float64       511        0        0.0\n",
            "TNX_delta_1d_lag5               float64       511        0        0.0\n",
            "TNX_abs_delta_1d_lag5           float64       511        0        0.0\n",
            "TNX_delta_1d_lag21              float64       511        0        0.0\n",
            "TNX_abs_delta_1d_lag21          float64       511        0        0.0\n",
            "eps_surprise_yahoo_lag1         float64       511        0        0.0\n",
            "eps_surprise_yahoo_lag2         float64       511        0        0.0\n",
            "eps_surprise_calc_lag1          float64       511        0        0.0\n",
            "eps_surprise_calc_lag2          float64       511        0        0.0\n",
            "post_earnings_day_1                int8       511        0        0.0\n",
            "post_earnings_day_2                int8       511        0        0.0\n",
            "post_earnings_day_3                int8       511        0        0.0\n",
            "post_earnings_day_4                int8       511        0        0.0\n",
            "post_earnings_day_5                int8       511        0        0.0\n",
            "eps_flag_yahoo_x_vol            float64       511        0        0.0\n",
            "eps_flag_calc_x_vol             float64       511        0        0.0\n",
            "CPI_impulse                     float64       511        0        0.0\n",
            "CPI_prev_release                float64       511        0        0.0\n",
            "CPI_change_prev_release         float64       511        0        0.0\n",
            "FEDFUNDS_impulse                float64       511        0        0.0\n",
            "FEDFUNDS_prev_delta             float64       511        0        0.0\n",
            "FEDFUNDS_release_day_x_mkt_vol  float64       511        0        0.0\n",
            "covid_period                       int8       511        0        0.0\n",
            "pre_covid                          int8       511        0        0.0\n",
            "post_covid                         int8       511        0        0.0\n",
            "crisis_2008                        int8       511        0        0.0\n",
            "pre_crisis_2008                    int8       511        0        0.0\n",
            "post_crisis_2008                   int8       511        0        0.0\n",
            "GOOGL_logret_t1                 float64       511        0        0.0\n",
            "\n",
            "[Top 20 columns by missing]\n",
            "                dtype  non_null  missing  missing_%\n",
            "GOOGL_Close   float64       511        0        0.0\n",
            "GOOGL_High    float64       511        0        0.0\n",
            "GOOGL_Low     float64       511        0        0.0\n",
            "GOOGL_Open    float64       511        0        0.0\n",
            "GOOGL_Volume  float64       511        0        0.0\n",
            "MSFT_Close    float64       511        0        0.0\n",
            "MSFT_High     float64       511        0        0.0\n",
            "MSFT_Low      float64       511        0        0.0\n",
            "MSFT_Open     float64       511        0        0.0\n",
            "MSFT_Volume   float64       511        0        0.0\n",
            "NVDA_Close    float64       511        0        0.0\n",
            "NVDA_High     float64       511        0        0.0\n",
            "NVDA_Low      float64       511        0        0.0\n",
            "NVDA_Open     float64       511        0        0.0\n",
            "NVDA_Volume   float64       511        0        0.0\n",
            "^IXIC_Close   float64       511        0        0.0\n",
            "^IXIC_High    float64       511        0        0.0\n",
            "^IXIC_Low     float64       511        0        0.0\n",
            "^IXIC_Open    float64       511        0        0.0\n",
            "^IXIC_Volume  float64       511        0        0.0\n",
            "\n",
            "[Overall missing]\n",
            "Total missing cells: 0 / 201,845 (0.0000%)\n",
            "\n",
            "[OK] Saved feature list + missing summary to:\n",
            "  - local : /content/my_project/runs/20260118_092035/outputs/feature_lists\n",
            "  - drive : /content/drive/MyDrive/my_project/runs/20260118_092035/outputs/feature_lists\n",
            "[OK] BLOCK 16 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def print_full_info_with_missing(df: pd.DataFrame, title: str = \"DATAFRAME INFO\") -> pd.DataFrame:\n",
        "    \"\"\"Print full column-wise completeness table.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\"[{title}]\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    n_rows, n_cols = df.shape\n",
        "    print(f\"Total rows: {n_rows:,}\")\n",
        "    print(f\"Total cols: {n_cols:,}\")\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        print(f\"Index range: {df.index.min()} -> {df.index.max()}\")\n",
        "\n",
        "    missing = df.isna().sum()\n",
        "    non_null = n_rows - missing\n",
        "    dtypes = df.dtypes.astype(str)\n",
        "\n",
        "    summary = (\n",
        "        pd.DataFrame({\n",
        "            \"dtype\": dtypes,\n",
        "            \"non_null\": non_null,\n",
        "            \"missing\": missing,\n",
        "            \"missing_%\": (missing / max(n_rows, 1) * 100).round(3),\n",
        "        })\n",
        "        .sort_values([\"missing\", \"missing_%\"], ascending=False)\n",
        "    )\n",
        "\n",
        "    with pd.option_context(\n",
        "        \"display.max_rows\", None,\n",
        "        \"display.max_columns\", None,\n",
        "        \"display.width\", 220,\n",
        "        \"display.max_colwidth\", 60\n",
        "    ):\n",
        "        print(\"\\n[Column-wise completeness]\")\n",
        "        print(summary)\n",
        "\n",
        "    print(\"\\n[Top 20 columns by missing]\")\n",
        "    print(summary.head(20))\n",
        "\n",
        "    total_missing = int(missing.sum())\n",
        "    total_cells = int(n_rows * n_cols)\n",
        "    print(\"\\n[Overall missing]\")\n",
        "    print(\n",
        "        f\"Total missing cells: {total_missing:,} / {total_cells:,} \"\n",
        "        f\"({(total_missing / max(total_cells, 1) * 100):.4f}%)\"\n",
        "    )\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "summary = print_full_info_with_missing(full_df, title=\"full_df (after feature blocks + time limit)\")\n",
        "\n",
        "# Save artifacts\n",
        "FEATURE_LIST_DIR_LOCAL = OUTPUTS_DIR / \"feature_lists\"\n",
        "FEATURE_LIST_DIR_DRIVE = DRIVE_PATHS[\"outputs_dir\"] / \"feature_lists\"\n",
        "ensure_dir(FEATURE_LIST_DIR_LOCAL)\n",
        "ensure_dir(FEATURE_LIST_DIR_DRIVE)\n",
        "\n",
        "feature_list = list(full_df.columns)\n",
        "\n",
        "# Feature lists\n",
        "features_txt_local = FEATURE_LIST_DIR_LOCAL / \"feature_list_all_columns.txt\"\n",
        "features_pkl_local = FEATURE_LIST_DIR_LOCAL / \"feature_list_all_columns.pkl\"\n",
        "features_csv_local = FEATURE_LIST_DIR_LOCAL / \"feature_list_all_columns.csv\"\n",
        "\n",
        "features_txt_local.write_text(\"\\n\".join(feature_list), encoding=\"utf-8\")\n",
        "save_pickle(feature_list, features_pkl_local)\n",
        "pd.DataFrame({\"feature\": feature_list}).to_csv(features_csv_local, index=False)\n",
        "\n",
        "copy_file(features_txt_local, FEATURE_LIST_DIR_DRIVE / features_txt_local.name)\n",
        "copy_file(features_pkl_local, FEATURE_LIST_DIR_DRIVE / features_pkl_local.name)\n",
        "copy_file(features_csv_local, FEATURE_LIST_DIR_DRIVE / features_csv_local.name)\n",
        "\n",
        "# Missingness summary\n",
        "missing_csv_local = FEATURE_LIST_DIR_LOCAL / \"missing_summary_all_columns.csv\"\n",
        "summary.reset_index(names=\"feature\").to_csv(missing_csv_local, index=False)\n",
        "copy_file(missing_csv_local, FEATURE_LIST_DIR_DRIVE / missing_csv_local.name)\n",
        "\n",
        "print(\"\\n[OK] Saved feature list + missing summary to:\")\n",
        "print(\"  - local :\", FEATURE_LIST_DIR_LOCAL)\n",
        "print(\"  - drive :\", FEATURE_LIST_DIR_DRIVE)\n",
        "\n",
        "print(\"[OK] BLOCK 16 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ejpsz6gSAXG",
      "metadata": {
        "id": "2ejpsz6gSAXG"
      },
      "source": [
        "---\n",
        "# SECTION 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Data visualization and analysis**\n",
        "\n",
        "**Blocks:** 17-19"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72c9844",
      "metadata": {
        "id": "a72c9844"
      },
      "source": [
        "## BLOCK 17 â€” EDA RETURNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "7a4a02db",
      "metadata": {
        "id": "7a4a02db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64b03a9-1099-44cc-8a76-3df390830627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 7 return columns.\n",
            "[OK] GOOGL: saved hist+box; skew=0.103727, kurt=7.249255\n",
            "[OK] MSFT: saved hist+box; skew=0.333581, kurt=9.976267\n",
            "[OK] NVDA: saved hist+box; skew=-0.140742, kurt=7.938145\n",
            "[OK] QQQ: saved hist+box; skew=0.503170, kurt=15.448890\n",
            "[OK] SPY: saved hist+box; skew=0.771636, kurt=24.452121\n",
            "[OK] XLK: saved hist+box; skew=0.264002, kurt=12.537031\n",
            "[OK] ^IXIC: saved hist+box; skew=0.474512, kurt=15.027781\n",
            "\n",
            "[OK] EDA Returns complete.\n",
            "[INFO] LOCAL plots : /content/my_project/runs/20260118_092035/plots/eda_returns\n",
            "[INFO] DRIVE plots : /content/drive/MyDrive/my_project/runs/20260118_092035/plots/eda_returns\n",
            "[INFO] LOCAL report: /content/my_project/runs/20260118_092035/reports/eda_returns/eda_returns_stats.csv\n",
            "[INFO] DRIVE report: /content/drive/MyDrive/my_project/runs/20260118_092035/reports/eda_returns/eda_returns_stats.csv\n",
            "[OK] BLOCK 17 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check if EDA is enabled\n",
        "if not RUN_PARAMS[\"eda\"][\"enabled\"]:\n",
        "    print(\"[SKIP] BLOCK 17 â€” EDA disabled in RUN_PARAMS.\")\n",
        "else:\n",
        "    # Guardrails\n",
        "    assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
        "    assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "\n",
        "    full_df = full_df.sort_index()\n",
        "\n",
        "    # EDA dirs inside RUN folders\n",
        "    EDA_PLOTS_LOCAL = LOCAL_PATHS[\"plots_dir\"] / \"eda_returns\"\n",
        "    EDA_REPS_LOCAL = LOCAL_PATHS[\"reports_dir\"] / \"eda_returns\"\n",
        "    EDA_PLOTS_DRIVE = DRIVE_PATHS[\"plots_dir\"] / \"eda_returns\"\n",
        "    EDA_REPS_DRIVE = DRIVE_PATHS[\"reports_dir\"] / \"eda_returns\"\n",
        "\n",
        "    for p in [EDA_PLOTS_LOCAL, EDA_REPS_LOCAL, EDA_PLOTS_DRIVE, EDA_REPS_DRIVE]:\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    STATS_FILE_LOCAL = EDA_REPS_LOCAL / \"eda_returns_stats.csv\"\n",
        "    STATS_FILE_DRIVE = EDA_REPS_DRIVE / \"eda_returns_stats.csv\"\n",
        "\n",
        "    # Get bins from RUN_PARAMS\n",
        "    EDA_BINS = int(RUN_PARAMS[\"eda\"][\"returns_bins\"])\n",
        "\n",
        "    def safe_name(s: str) -> str:\n",
        "        \"\"\"Convert ticker name to safe filename.\"\"\"\n",
        "        s = s.replace(\"^\", \"\")\n",
        "        return re.sub(r\"[^A-Za-z0-9\\-_]+\", \"_\", s)\n",
        "\n",
        "    def analyze_returns(series: pd.Series, ticker: str, bins: int = EDA_BINS) -> None:\n",
        "        \"\"\"Analyze returns distribution for a ticker.\"\"\"\n",
        "        data = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "        if data.empty:\n",
        "            print(f\"[WARN] {ticker}: empty after dropna(). Skipping.\")\n",
        "            return\n",
        "\n",
        "        fn = safe_name(ticker)\n",
        "\n",
        "        # Histogram\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(data.values, bins=bins)\n",
        "        plt.title(f\"Histogram of {ticker} Daily Log Returns\")\n",
        "        plt.xlabel(\"Log return\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EDA_PLOTS_LOCAL / f\"{fn}_hist.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Boxplot\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        plt.boxplot(data.values, vert=False)\n",
        "        plt.title(f\"Boxplot of {ticker} Daily Log Returns\")\n",
        "        plt.xlabel(\"Log return\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EDA_PLOTS_LOCAL / f\"{fn}_box.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Stats CSV (append)\n",
        "        skew_val = float(stats.skew(data.values, bias=False))\n",
        "        kurt_val = float(stats.kurtosis(data.values, fisher=False, bias=False))\n",
        "\n",
        "        file_exists = STATS_FILE_LOCAL.exists()\n",
        "        with open(STATS_FILE_LOCAL, mode=\"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if not file_exists:\n",
        "                w.writerow([\"run_id\", \"ticker\", \"col\", \"n\", \"skewness\", \"kurtosis_pearson\"])\n",
        "            w.writerow([RUN_ID, ticker, series.name, int(len(data)), skew_val, kurt_val])\n",
        "\n",
        "        print(f\"[OK] {ticker}: saved hist+box; skew={skew_val:.6f}, kurt={kurt_val:.6f}\")\n",
        "\n",
        "    # AUTO: find all *_logret_cc columns (exclude abs)\n",
        "    ret_cols = sorted([\n",
        "        c for c in full_df.columns\n",
        "        if c.endswith(\"_logret_cc\") and not c.endswith(\"_abs_logret_cc\")\n",
        "    ])\n",
        "    assert len(ret_cols) > 0, \"[ERROR] No *_logret_cc columns found.\"\n",
        "\n",
        "    print(f\"[INFO] Found {len(ret_cols)} return columns.\")\n",
        "\n",
        "    for col in ret_cols:\n",
        "        ticker = col.replace(\"_logret_cc\", \"\")\n",
        "        analyze_returns(full_df[col], ticker=ticker)\n",
        "\n",
        "    # Mirror plots + stats to DRIVE\n",
        "    for img in EDA_PLOTS_LOCAL.glob(\"*.png\"):\n",
        "        copy_file(img, EDA_PLOTS_DRIVE / img.name)\n",
        "\n",
        "    if STATS_FILE_LOCAL.exists():\n",
        "        copy_file(STATS_FILE_LOCAL, STATS_FILE_DRIVE)\n",
        "\n",
        "    print(\"\\n[OK] EDA Returns complete.\")\n",
        "    print(\"[INFO] LOCAL plots :\", EDA_PLOTS_LOCAL)\n",
        "    print(\"[INFO] DRIVE plots :\", EDA_PLOTS_DRIVE)\n",
        "    print(\"[INFO] LOCAL report:\", STATS_FILE_LOCAL)\n",
        "    print(\"[INFO] DRIVE report:\", STATS_FILE_DRIVE)\n",
        "\n",
        "print(\"[OK] BLOCK 17 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c3736c",
      "metadata": {
        "id": "07c3736c"
      },
      "source": [
        "## BLOCK 18 â€” EDA VOLATILITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "878eb90a",
      "metadata": {
        "id": "878eb90a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5325426-e9b1-44e2-e924-ed80cc85df77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] LOCAL EDA plots dir: /content/my_project/runs/20260118_092035/plots/eda_volatility\n",
            "[INFO] DRIVE EDA plots dir: /content/drive/MyDrive/my_project/runs/20260118_092035/plots/eda_volatility\n",
            "[INFO] Found 14 *_logret_cc_std_21 columns.\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/GOOGL_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/GOOGL_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/MSFT_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/MSFT_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/NVDA_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/NVDA_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/QQQ_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/QQQ_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/SPY_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/SPY_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/XLK_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/XLK_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/IXIC_abs_logret_cc_std_21_with_covid_and_2008.png\n",
            "[INFO] Saved (LOCAL): /content/my_project/runs/20260118_092035/plots/eda_volatility/IXIC_logret_cc_std_21_with_covid_and_2008.png\n",
            "\n",
            "[OK] Volatility EDA complete.\n",
            "[OK] BLOCK 18 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check if EDA is enabled\n",
        "if not RUN_PARAMS[\"eda\"][\"enabled\"]:\n",
        "    print(\"[SKIP] BLOCK 18 â€” EDA disabled in RUN_PARAMS.\")\n",
        "else:\n",
        "    # Guardrails\n",
        "    assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
        "    assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "    full_df = full_df.sort_index()\n",
        "\n",
        "    # EDA dirs inside RUN folders\n",
        "    EDA_PLOTS_LOCAL = LOCAL_PATHS[\"plots_dir\"] / \"eda_volatility\"\n",
        "    EDA_PLOTS_DRIVE = DRIVE_PATHS[\"plots_dir\"] / \"eda_volatility\"\n",
        "    EDA_PLOTS_LOCAL.mkdir(parents=True, exist_ok=True)\n",
        "    EDA_PLOTS_DRIVE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _safe_fname(s: str) -> str:\n",
        "        \"\"\"Convert column name to safe filename.\"\"\"\n",
        "        s = s.replace(\"^\", \"\")\n",
        "        return re.sub(r\"[^A-Za-z0-9\\-_]+\", \"_\", s)\n",
        "\n",
        "    def plot_volatility_with_crisis_periods(\n",
        "        df: pd.DataFrame,\n",
        "        vol_col: str,\n",
        "        covid_col: str = \"covid_period\",\n",
        "        crisis_col: str = \"crisis_2008\",\n",
        "    ) -> None:\n",
        "        \"\"\"Plot volatility with COVID and 2008 crisis shading.\"\"\"\n",
        "        missing = [c for c in [vol_col, covid_col, crisis_col] if c not in df.columns]\n",
        "        if missing:\n",
        "            print(f\"[SKIP] {vol_col}: missing columns {missing}\")\n",
        "            return\n",
        "\n",
        "        tmp = df[[vol_col, covid_col, crisis_col]].dropna(subset=[vol_col]).copy()\n",
        "        if tmp.empty:\n",
        "            print(f\"[SKIP] {vol_col}: empty after dropping NaN vol.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        (line_handle,) = plt.plot(tmp.index, tmp[vol_col].astype(\"float64\"), label=vol_col)\n",
        "\n",
        "        def shade_period(mask: pd.Series, label: str) -> Patch:\n",
        "            mask = mask.astype(bool).values\n",
        "            idx = tmp.index\n",
        "\n",
        "            in_seg = False\n",
        "            start = None\n",
        "            for d, flag in zip(idx, mask):\n",
        "                if flag and not in_seg:\n",
        "                    in_seg = True\n",
        "                    start = d\n",
        "                elif (not flag) and in_seg:\n",
        "                    plt.axvspan(start, d, alpha=0.2)\n",
        "                    in_seg = False\n",
        "\n",
        "            if in_seg:\n",
        "                plt.axvspan(start, idx[-1], alpha=0.2)\n",
        "\n",
        "            return Patch(alpha=0.2, label=label)\n",
        "\n",
        "        covid_patch = shade_period(tmp[covid_col] == 1, label=\"COVID period\")\n",
        "        crisis_patch = shade_period(tmp[crisis_col] == 1, label=\"2008 crisis\")\n",
        "\n",
        "        plt.title(f\"{vol_col} with COVID and 2008 crisis shading\")\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Volatility (std)\")\n",
        "        plt.legend(handles=[line_handle, covid_patch, crisis_patch], loc=\"upper right\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_path = EDA_PLOTS_LOCAL / f\"{_safe_fname(vol_col)}_with_covid_and_2008.png\"\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "        plt.close()\n",
        "        print(f\"[INFO] Saved (LOCAL): {save_path}\")\n",
        "\n",
        "        # Mirror to drive\n",
        "        copy_file(save_path, EDA_PLOTS_DRIVE / save_path.name)\n",
        "\n",
        "    # Run for all return-vol columns (*_logret_cc_std_21)\n",
        "    W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
        "    vol_cols = sorted([c for c in full_df.columns if c.endswith(f\"_logret_cc_std_{W_LONG}\")])\n",
        "\n",
        "    print(f\"[INFO] LOCAL EDA plots dir: {EDA_PLOTS_LOCAL}\")\n",
        "    print(f\"[INFO] DRIVE EDA plots dir: {EDA_PLOTS_DRIVE}\")\n",
        "    print(f\"[INFO] Found {len(vol_cols)} *_logret_cc_std_{W_LONG} columns.\")\n",
        "\n",
        "    if len(vol_cols) == 0:\n",
        "        print(f\"[WARN] No *_logret_cc_std_{W_LONG} columns found. Skipping volatility EDA.\")\n",
        "    else:\n",
        "        for col in vol_cols:\n",
        "            plot_volatility_with_crisis_periods(\n",
        "                full_df,\n",
        "                vol_col=col,\n",
        "                covid_col=\"covid_period\",\n",
        "                crisis_col=\"crisis_2008\",\n",
        "            )\n",
        "\n",
        "    print(\"\\n[OK] Volatility EDA complete.\")\n",
        "\n",
        "print(\"[OK] BLOCK 18 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7caba4e4",
      "metadata": {
        "id": "7caba4e4"
      },
      "source": [
        "## BLOCK 19 â€” EDA CATEGORICAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "28044911",
      "metadata": {
        "id": "28044911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718f0e05-83c7-4242-fb7f-beaae87fddd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Categorical/binary columns used for EDA:\n",
            "['is_earnings_day', 'has_eps_surprise_yahoo', 'has_eps_surprise_calc', 'post_earnings_day_1', 'post_earnings_day_2', 'post_earnings_day_3', 'post_earnings_day_4', 'post_earnings_day_5', 'CPI_release_day', 'FEDFUNDS_release_day', 'FEDFUNDS_changed', 'CPI_pct_mom_is_missing', 'CPI_accel_pct_mom_is_missing', 'FEDFUNDS_delta_mom_is_missing', 'is_q1', 'is_q2', 'is_q3', 'is_q4', 'covid_period', 'crisis_2008', 'pre_covid', 'post_covid', 'pre_crisis_2008', 'post_crisis_2008']\n",
            "[INFO] Saved categorical value counts to:\n",
            "  - local: /content/my_project/runs/20260118_092035/reports/eda_categorical/categorical_value_counts.csv\n",
            "  - drive: /content/drive/MyDrive/my_project/runs/20260118_092035/reports/eda_categorical/categorical_value_counts.csv\n",
            "[INFO] Numeric targets used for boxplots: ['GOOGL_logret_cc', 'GOOGL_logret_cc_std_21', 'GOOGL_logret_t1']\n",
            "[INFO] Saved CramÃ©r's V heatmap to:\n",
            "  - local: /content/my_project/runs/20260118_092035/plots/eda_categorical/categorical_cramersV_heatmap.png\n",
            "  - drive: /content/drive/MyDrive/my_project/runs/20260118_092035/plots/eda_categorical/categorical_cramersV_heatmap.png\n",
            "\n",
            "[OK] Categorical/Binary EDA complete.\n",
            "[INFO] LOCAL plots  : /content/my_project/runs/20260118_092035/plots/eda_categorical\n",
            "[INFO] DRIVE plots  : /content/drive/MyDrive/my_project/runs/20260118_092035/plots/eda_categorical\n",
            "[INFO] LOCAL reports: /content/my_project/runs/20260118_092035/reports/eda_categorical\n",
            "[INFO] DRIVE reports: /content/drive/MyDrive/my_project/runs/20260118_092035/reports/eda_categorical\n",
            "[OK] BLOCK 19 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check if EDA is enabled\n",
        "if not RUN_PARAMS[\"eda\"][\"enabled\"]:\n",
        "    print(\"[SKIP] BLOCK 19 â€” EDA disabled in RUN_PARAMS.\")\n",
        "else:\n",
        "    # Guardrails\n",
        "    assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
        "    assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "    full_df = full_df.sort_index()\n",
        "\n",
        "    # EDA dirs inside RUN folders\n",
        "    EDA_PLOTS_LOCAL = LOCAL_PATHS[\"plots_dir\"] / \"eda_categorical\"\n",
        "    EDA_REPS_LOCAL = LOCAL_PATHS[\"reports_dir\"] / \"eda_categorical\"\n",
        "    EDA_PLOTS_DRIVE = DRIVE_PATHS[\"plots_dir\"] / \"eda_categorical\"\n",
        "    EDA_REPS_DRIVE = DRIVE_PATHS[\"reports_dir\"] / \"eda_categorical\"\n",
        "\n",
        "    for p in [EDA_PLOTS_LOCAL, EDA_REPS_LOCAL, EDA_PLOTS_DRIVE, EDA_REPS_DRIVE]:\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _safe_fname(s: str) -> str:\n",
        "        \"\"\"Convert column name to safe filename.\"\"\"\n",
        "        s = s.replace(\"^\", \"\")\n",
        "        return re.sub(r\"[^A-Za-z0-9\\-_]+\", \"_\", s)\n",
        "\n",
        "    # Categorical/binary columns for EDA\n",
        "    categorical_cols = [\n",
        "        \"is_earnings_day\",\n",
        "        \"has_eps_surprise_yahoo\",\n",
        "        \"has_eps_surprise_calc\",\n",
        "        \"post_earnings_day_1\", \"post_earnings_day_2\", \"post_earnings_day_3\",\n",
        "        \"post_earnings_day_4\", \"post_earnings_day_5\",\n",
        "        \"CPI_release_day\",\n",
        "        \"FEDFUNDS_release_day\",\n",
        "        \"FEDFUNDS_changed\",\n",
        "        \"CPI_pct_mom_is_missing\",\n",
        "        \"CPI_accel_pct_mom_is_missing\",\n",
        "        \"FEDFUNDS_delta_mom_is_missing\",\n",
        "        \"is_q1\", \"is_q2\", \"is_q3\", \"is_q4\",\n",
        "        \"covid_period\",\n",
        "        \"crisis_2008\",\n",
        "        \"pre_covid\",\n",
        "        \"post_covid\",\n",
        "        \"pre_crisis_2008\",\n",
        "        \"post_crisis_2008\",\n",
        "    ]\n",
        "    categorical_cols = [c for c in categorical_cols if c in full_df.columns]\n",
        "\n",
        "    print(\"[INFO] Categorical/binary columns used for EDA:\")\n",
        "    print(categorical_cols)\n",
        "\n",
        "    # Save value counts to CSV\n",
        "    cat_counts_file_local = EDA_REPS_LOCAL / \"categorical_value_counts.csv\"\n",
        "    cat_counts_file_drive = EDA_REPS_DRIVE / \"categorical_value_counts.csv\"\n",
        "\n",
        "    with open(cat_counts_file_local, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"run_id\", \"variable\", \"value\", \"count\"])\n",
        "        for col in categorical_cols:\n",
        "            vc = full_df[col].value_counts(dropna=False)\n",
        "            for val, count in vc.items():\n",
        "                writer.writerow([RUN_ID, col, val, int(count)])\n",
        "\n",
        "    copy_file(cat_counts_file_local, cat_counts_file_drive)\n",
        "\n",
        "    print(\"[INFO] Saved categorical value counts to:\")\n",
        "    print(\"  - local:\", cat_counts_file_local)\n",
        "    print(\"  - drive:\", cat_counts_file_drive)\n",
        "\n",
        "    # Bar plots for each categorical column\n",
        "    for col in categorical_cols:\n",
        "        vc = full_df[col].value_counts(dropna=False).sort_index()\n",
        "        if vc.empty:\n",
        "            continue\n",
        "\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar([str(x) for x in vc.index], vc.values)\n",
        "        plt.title(f\"Frequency of {col}\")\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        fname = f\"{_safe_fname(col)}_bar.png\"\n",
        "        out_local = EDA_PLOTS_LOCAL / fname\n",
        "        out_drive = EDA_PLOTS_DRIVE / fname\n",
        "\n",
        "        plt.savefig(out_local, dpi=150)\n",
        "        plt.close()\n",
        "        copy_file(out_local, out_drive)\n",
        "\n",
        "    # Boxplots: numeric targets by categorical\n",
        "    BASE = str(RUN_PARAMS[\"features\"][\"regime_base\"])\n",
        "    W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
        "    TARGET_COL = str(RUN_PARAMS[\"data\"][\"target_col\"])\n",
        "\n",
        "    numeric_targets = [f\"{BASE}_logret_cc\", f\"{BASE}_logret_cc_std_{W_LONG}\"]\n",
        "    if TARGET_COL in full_df.columns:\n",
        "        numeric_targets.append(TARGET_COL)\n",
        "    numeric_targets = [c for c in numeric_targets if c in full_df.columns]\n",
        "\n",
        "    print(\"[INFO] Numeric targets used for boxplots:\", numeric_targets)\n",
        "\n",
        "    for target in numeric_targets:\n",
        "        for col in categorical_cols:\n",
        "            if full_df[col].nunique(dropna=True) < 2:\n",
        "                continue\n",
        "\n",
        "            tmp = full_df[[col, target]].dropna()\n",
        "            if tmp.empty:\n",
        "                continue\n",
        "\n",
        "            groups, labels = [], []\n",
        "            for k in sorted(tmp[col].unique()):\n",
        "                groups.append(tmp.loc[tmp[col] == k, target].astype(\"float64\").values)\n",
        "                labels.append(str(k))\n",
        "\n",
        "            if len(groups) < 2:\n",
        "                continue\n",
        "\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.boxplot(groups, labels=labels, showfliers=False)\n",
        "            plt.title(f\"{target} by {col}\")\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel(target)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            fname = f\"{_safe_fname(target)}_by_{_safe_fname(col)}_box.png\"\n",
        "            out_local = EDA_PLOTS_LOCAL / fname\n",
        "            out_drive = EDA_PLOTS_DRIVE / fname\n",
        "\n",
        "            plt.savefig(out_local, dpi=150)\n",
        "            plt.close()\n",
        "            copy_file(out_local, out_drive)\n",
        "\n",
        "    # CramÃ©r's V heatmap\n",
        "    def cramers_v(x: pd.Series, y: pd.Series) -> float:\n",
        "        \"\"\"Calculate CramÃ©r's V statistic for two categorical variables.\"\"\"\n",
        "        xy = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
        "        if xy.empty:\n",
        "            return np.nan\n",
        "\n",
        "        confusion = pd.crosstab(xy[\"x\"], xy[\"y\"])\n",
        "        if confusion.shape[0] < 2 or confusion.shape[1] < 2:\n",
        "            return 0.0\n",
        "\n",
        "        chi2 = stats.chi2_contingency(confusion, correction=False)[0]\n",
        "        n = confusion.to_numpy().sum()\n",
        "        r, k = confusion.shape\n",
        "        denom = n * (min(r, k) - 1)\n",
        "        return 0.0 if denom <= 0 else float(np.sqrt(chi2 / denom))\n",
        "\n",
        "    n_cat = len(categorical_cols)\n",
        "    if n_cat >= 2:\n",
        "        mat = np.zeros((n_cat, n_cat), dtype=float)\n",
        "        for i, c1 in enumerate(categorical_cols):\n",
        "            for j, c2 in enumerate(categorical_cols):\n",
        "                mat[i, j] = 1.0 if i == j else cramers_v(full_df[c1], full_df[c2])\n",
        "\n",
        "        plt.figure(figsize=(max(10, n_cat * 0.6), max(8, n_cat * 0.6)))\n",
        "        im = plt.imshow(mat, vmin=0.0, vmax=1.0)\n",
        "        plt.title(\"CramÃ©r's V Heatmap â€” Categorical/Binary Variables\")\n",
        "        plt.xticks(range(n_cat), categorical_cols, rotation=90)\n",
        "        plt.yticks(range(n_cat), categorical_cols)\n",
        "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        heatmap_local = EDA_PLOTS_LOCAL / \"categorical_cramersV_heatmap.png\"\n",
        "        heatmap_drive = EDA_PLOTS_DRIVE / \"categorical_cramersV_heatmap.png\"\n",
        "        plt.savefig(heatmap_local, dpi=150)\n",
        "        plt.close()\n",
        "        copy_file(heatmap_local, heatmap_drive)\n",
        "\n",
        "        print(\"[INFO] Saved CramÃ©r's V heatmap to:\")\n",
        "        print(\"  - local:\", heatmap_local)\n",
        "        print(\"  - drive:\", heatmap_drive)\n",
        "    else:\n",
        "        print(\"[INFO] Skipped CramÃ©r's V heatmap: need at least 2 categorical columns.\")\n",
        "\n",
        "    print(\"\\n[OK] Categorical/Binary EDA complete.\")\n",
        "    print(\"[INFO] LOCAL plots  :\", EDA_PLOTS_LOCAL)\n",
        "    print(\"[INFO] DRIVE plots  :\", EDA_PLOTS_DRIVE)\n",
        "    print(\"[INFO] LOCAL reports:\", EDA_REPS_LOCAL)\n",
        "    print(\"[INFO] DRIVE reports:\", EDA_REPS_DRIVE)\n",
        "\n",
        "print(\"[OK] BLOCK 19 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0QuHUrWASAXH",
      "metadata": {
        "id": "0QuHUrWASAXH"
      },
      "source": [
        "---\n",
        "# SECTION 4: Train/Valid/Test Split & NN Features\n",
        "\n",
        "**Data splitting and neural network feature preparation**\n",
        "\n",
        "**Blocks:** 20-22"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9703c068",
      "metadata": {
        "id": "9703c068"
      },
      "source": [
        "## BLOCK 20 â€” SPLIT + WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "b668997c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b668997c",
        "outputId": "12ccedf8-8427-430d-e0ae-652ddaad3439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] model_df range: 2024-01-02 00:00:00 -> 2026-01-14 00:00:00 | rows: 511\n",
            "[INFO] Dropped rows due to missing target: 0\n",
            "[INFO] #Features (ALL): 394\n",
            "[INFO] Data range: 2024-01-02 -> 2026-01-14\n",
            "[INFO] Split sizes: TRAIN=347 (67.91%) | VALID=77 (15.07%) | TEST=87 (17.03%)\n",
            "[INFO] train: <= 2025-05-20 | valid: 2025-05-21 - 2025-09-10 | test: 2025-09-11 - 2026-01-14\n",
            "\n",
            "[INFO] y-weight cap rate (==max_w):\n",
            "  TRAIN: 0.0029 | c=1.0 max_w=4.0\n",
            "  VALID: 0.0130 | c=1.0 max_w=4.0\n",
            "  TEST : 0.0000 | c=1.0 max_w=4.0\n",
            "\n",
            "[INFO] sample_weight diagnostics (per-split mean=1):\n",
            "  TRAIN: time[1.000->2.000] y[1.000->4.000] (median_abs_y=0.0106933) w[0.438->2.440] (mean=1.000)\n",
            "  VALID: time[1.000->2.000] y[1.273->4.000] (median_abs_y=0.00934717) w[0.446->2.523] (mean=1.000)\n",
            "  TEST: time[1.000->2.000] y[1.000->3.478] (median_abs_y=0.00996899) w[0.469->1.827] (mean=1.000)\n",
            "\n",
            "[INFO] Shapes: (347, 394) (77, 394) (87, 394)\n",
            "[OK] feature_cols_t1 set from X_train_t1.columns | #Features= 394\n",
            "\n",
            "[INFO] Saved XGBoost splits to: /content/my_project/data/processed\n",
            "  - X_train/valid/test_xgb.pkl\n",
            "  - y_train/valid/test.pkl\n",
            "  - weights_train/valid/test.pkl\n",
            "[OK] BLOCK 20 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Guardrails\n",
        "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
        "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
        "full_df = full_df.sort_index()\n",
        "\n",
        "# Target column check\n",
        "if TARGET_T1 not in full_df.columns:\n",
        "    raise KeyError(f\"[ERROR] TARGET='{TARGET_T1}' not found in full_df. Run target-definition block first.\")\n",
        "\n",
        "# All features except target\n",
        "feature_cols_t1 = [c for c in full_df.columns if c != TARGET_T1]\n",
        "\n",
        "# Model df (features + target)\n",
        "model_df = full_df.loc[:, feature_cols_t1 + [TARGET_T1]].copy()\n",
        "before = len(model_df)\n",
        "model_df = model_df.dropna(subset=[TARGET_T1]).copy()\n",
        "after = len(model_df)\n",
        "\n",
        "if len(model_df) == 0:\n",
        "    raise ValueError(\"[ERROR] model_df became empty after dropping NaN target rows.\")\n",
        "\n",
        "print(\"[INFO] model_df range:\", model_df.index.min(), \"->\", model_df.index.max(), \"| rows:\", len(model_df))\n",
        "print(\"[INFO] Dropped rows due to missing target:\", before - after)\n",
        "print(\"[INFO] #Features (ALL):\", len(feature_cols_t1))\n",
        "\n",
        "# Time split from RUN_PARAMS (date-based)\n",
        "data_cfg = RUN_PARAMS[\"data\"]\n",
        "\n",
        "train_end_date = data_cfg[\"train_end\"]\n",
        "valid_start_date = data_cfg[\"valid_start\"]\n",
        "valid_end_date = data_cfg[\"valid_end\"]\n",
        "test_start_date = data_cfg[\"test_start\"]\n",
        "test_end_date = data_cfg.get(\"test_end\")\n",
        "\n",
        "print(\"[INFO] Data range:\", model_df.index.min().date(), \"->\", model_df.index.max().date())\n",
        "\n",
        "train_end = pd.Timestamp(train_end_date)\n",
        "valid_start = pd.Timestamp(valid_start_date)\n",
        "valid_end = pd.Timestamp(valid_end_date)\n",
        "test_start = pd.Timestamp(test_start_date)\n",
        "test_end = pd.Timestamp(test_end_date) if test_end_date else model_df.index.max()\n",
        "\n",
        "mask_train = model_df.index <= train_end\n",
        "mask_valid = (model_df.index >= valid_start) & (model_df.index <= valid_end)\n",
        "mask_test = (model_df.index >= test_start) & (model_df.index <= test_end)\n",
        "\n",
        "split_info = f\"train: <= {train_end.date()} | valid: {valid_start.date()} - {valid_end.date()} | test: {test_start.date()} - {test_end.date()}\"\n",
        "\n",
        "train_df = model_df.loc[mask_train].copy()\n",
        "valid_df = model_df.loc[mask_valid].copy()\n",
        "test_df = model_df.loc[mask_test].copy()\n",
        "\n",
        "if len(train_df) == 0:\n",
        "    raise ValueError(\"[ERROR] train_df is empty. Check split configuration and data coverage.\")\n",
        "\n",
        "n_total = len(model_df)\n",
        "pct_train = (len(train_df) / n_total * 100.0) if n_total else 0.0\n",
        "pct_valid = (len(valid_df) / n_total * 100.0) if n_total else 0.0\n",
        "pct_test = (len(test_df) / n_total * 100.0) if n_total else 0.0\n",
        "\n",
        "print(\"[INFO] Split sizes:\",\n",
        "      f\"TRAIN={len(train_df):,} ({pct_train:.2f}%) | \"\n",
        "      f\"VALID={len(valid_df):,} ({pct_valid:.2f}%) | \"\n",
        "      f\"TEST={len(test_df):,} ({pct_test:.2f}%)\")\n",
        "print(f\"[INFO] {split_info}\")\n",
        "\n",
        "# Weights per split: time * |y| (normalize mean=1 per split)\n",
        "_eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
        "c = float(RUN_PARAMS[\"weights\"][\"c\"])\n",
        "max_w = float(RUN_PARAMS[\"weights\"][\"max_w\"])\n",
        "\n",
        "\n",
        "def _build_split_weights(df: pd.DataFrame, target_col: str, split_name: str):\n",
        "    \"\"\"Build sample weights for a split: time * |y| normalized to mean=1.\"\"\"\n",
        "    n = len(df)\n",
        "    if n == 0:\n",
        "        return np.array([], dtype=float), dict(\n",
        "            split=split_name, n=0, time_min=np.nan, time_max=np.nan, time_mean=np.nan,\n",
        "            y_min=np.nan, y_max=np.nan, y_med_abs=np.nan, y_cap_rate=np.nan,\n",
        "            w_min=np.nan, w_max=np.nan, w_mean=np.nan\n",
        "        )\n",
        "\n",
        "    w_time = np.linspace(1.0, 2.0, n, dtype=float)\n",
        "\n",
        "    y_vals = df[target_col].astype(float).to_numpy()\n",
        "    abs_y = np.abs(y_vals)\n",
        "    med_abs = float(np.median(abs_y)) if n else 0.0\n",
        "\n",
        "    ratio = abs_y / (med_abs + _eps)\n",
        "    w_y = 1.0 + c * np.sqrt(ratio)\n",
        "    w_y = np.clip(w_y, 1.0, max_w)\n",
        "    cap_rate = float(np.mean(w_y >= max_w))\n",
        "\n",
        "    w = w_time * w_y\n",
        "    w = w / (w.mean() + _eps)\n",
        "\n",
        "    stats_dict = dict(\n",
        "        split=split_name, n=n,\n",
        "        time_min=float(w_time.min()), time_max=float(w_time.max()), time_mean=float(w_time.mean()),\n",
        "        y_min=float(w_y.min()), y_max=float(w_y.max()), y_med_abs=float(med_abs), y_cap_rate=float(cap_rate),\n",
        "        w_min=float(w.min()), w_max=float(w.max()), w_mean=float(w.mean())\n",
        "    )\n",
        "    return w, stats_dict\n",
        "\n",
        "\n",
        "w_train, st_train = _build_split_weights(train_df, TARGET_T1, \"TRAIN\")\n",
        "w_valid, st_valid = _build_split_weights(valid_df, TARGET_T1, \"VALID\")\n",
        "w_test, st_test = _build_split_weights(test_df, TARGET_T1, \"TEST\")\n",
        "\n",
        "print(\"\\n[INFO] y-weight cap rate (==max_w):\")\n",
        "print(f\"  TRAIN: {st_train['y_cap_rate']:.4f} | c={c} max_w={max_w}\")\n",
        "print(f\"  VALID: {st_valid['y_cap_rate']:.4f} | c={c} max_w={max_w}\" if len(valid_df) else \"  VALID: empty\")\n",
        "print(f\"  TEST : {st_test['y_cap_rate']:.4f} | c={c} max_w={max_w}\" if len(test_df) else \"  TEST : empty\")\n",
        "\n",
        "print(\"\\n[INFO] sample_weight diagnostics (per-split mean=1):\")\n",
        "for st in [st_train, st_valid, st_test]:\n",
        "    if st[\"n\"] == 0:\n",
        "        print(f\"  {st['split']}: empty\")\n",
        "        continue\n",
        "    print(\n",
        "        f\"  {st['split']}: \"\n",
        "        f\"time[{st['time_min']:.3f}->{st['time_max']:.3f}] \"\n",
        "        f\"y[{st['y_min']:.3f}->{st['y_max']:.3f}] (median_abs_y={st['y_med_abs']:.6g}) \"\n",
        "        f\"w[{st['w_min']:.3f}->{st['w_max']:.3f}] (mean={st['w_mean']:.3f})\"\n",
        "    )\n",
        "\n",
        "train_df[\"sample_weight\"] = w_train\n",
        "valid_df[\"sample_weight\"] = w_valid\n",
        "test_df[\"sample_weight\"] = w_test\n",
        "\n",
        "# Final X/y matrices (source-of-truth = X_train_t1.columns)\n",
        "X_train_t1 = train_df.loc[:, feature_cols_t1].copy()\n",
        "y_train_t1 = train_df.loc[:, TARGET_T1].copy()\n",
        "\n",
        "X_valid_t1 = valid_df.loc[:, feature_cols_t1].copy()\n",
        "y_valid_t1 = valid_df.loc[:, TARGET_T1].copy()\n",
        "\n",
        "X_test_t1 = test_df.loc[:, feature_cols_t1].copy()\n",
        "y_test_t1 = test_df.loc[:, TARGET_T1].copy()\n",
        "\n",
        "feature_cols_t1 = list(X_train_t1.columns)\n",
        "\n",
        "assert len(X_train_t1) == len(w_train), \"[ERROR] w_train length mismatch\"\n",
        "assert len(X_valid_t1) == len(w_valid), \"[ERROR] w_valid length mismatch\"\n",
        "assert len(X_test_t1) == len(w_test), \"[ERROR] w_test length mismatch\"\n",
        "\n",
        "print(\"\\n[INFO] Shapes:\", X_train_t1.shape, X_valid_t1.shape, X_test_t1.shape)\n",
        "print(\"[OK] feature_cols_t1 set from X_train_t1.columns | #Features=\", len(feature_cols_t1))\n",
        "\n",
        "# Save XGBoost splits (project data/processed local + drive)\n",
        "LOCAL_PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
        "DRIVE_PROC_DATA_DIR = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "# Features (X) for XGBoost\n",
        "X_train_t1.to_pickle(LOCAL_PROC_DATA_DIR / \"X_train_xgb.pkl\")\n",
        "X_valid_t1.to_pickle(LOCAL_PROC_DATA_DIR / \"X_valid_xgb.pkl\")\n",
        "X_test_t1.to_pickle(LOCAL_PROC_DATA_DIR / \"X_test_xgb.pkl\")\n",
        "\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_train_xgb.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_xgb.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_valid_xgb.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_xgb.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_test_xgb.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_xgb.pkl\")\n",
        "\n",
        "# Target (y) - shared across all models\n",
        "save_pickle(y_train_t1, LOCAL_PROC_DATA_DIR / \"y_train.pkl\")\n",
        "save_pickle(y_valid_t1, LOCAL_PROC_DATA_DIR / \"y_valid.pkl\")\n",
        "save_pickle(y_test_t1, LOCAL_PROC_DATA_DIR / \"y_test.pkl\")\n",
        "\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"y_train.pkl\", DRIVE_PROC_DATA_DIR / \"y_train.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"y_valid.pkl\", DRIVE_PROC_DATA_DIR / \"y_valid.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"y_test.pkl\", DRIVE_PROC_DATA_DIR / \"y_test.pkl\")\n",
        "\n",
        "# Sample weights - shared across all models\n",
        "save_pickle(w_train, LOCAL_PROC_DATA_DIR / \"weights_train.pkl\")\n",
        "save_pickle(w_valid, LOCAL_PROC_DATA_DIR / \"weights_valid.pkl\")\n",
        "save_pickle(w_test, LOCAL_PROC_DATA_DIR / \"weights_test.pkl\")\n",
        "\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"weights_train.pkl\", DRIVE_PROC_DATA_DIR / \"weights_train.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"weights_valid.pkl\", DRIVE_PROC_DATA_DIR / \"weights_valid.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"weights_test.pkl\", DRIVE_PROC_DATA_DIR / \"weights_test.pkl\")\n",
        "\n",
        "print(\"\\n[INFO] Saved XGBoost splits to:\", LOCAL_PROC_DATA_DIR)\n",
        "print(\"  - X_train/valid/test_xgb.pkl\")\n",
        "print(\"  - y_train/valid/test.pkl\")\n",
        "print(\"  - weights_train/valid/test.pkl\")\n",
        "\n",
        "print(\"[OK] BLOCK 20 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb7b238",
      "metadata": {
        "id": "8bb7b238"
      },
      "source": [
        "## BLOCK 21 â€” NN FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "68e8face",
      "metadata": {
        "id": "68e8face",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc86883-8137-4a13-bcc1-021e6a311a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Dropped 8 constant columns\n",
            "[INFO] Features after cleaning: 386\n",
            "[INFO] Computing Mutual Information (this may take a minute)...\n",
            "[OK] NN feature lists saved:\n",
            "  - /content/my_project/runs/20260118_092035/feature_selection/neural_feature_cols_40_bygroup.pkl | n= 40\n",
            "  - /content/my_project/runs/20260118_092035/feature_selection/neural_feature_cols_80_bygroup.pkl | n= 80\n",
            "[INFO] Top-10 MI features (TRAIN): ['SPY_log_vol_std_21', 'XLK_logret_cc_mean_5', 'XLK_High', 'NVDA_log_hl_std_21', 'GOOGL_beta_^IXIC_21', 'GOOGL_log_hl', 'GOOGL_beta_QQQ_21', '^IXIC_log_vol_std_21', 'GOOGL_logret_cc_std_21', 'MSFT_abs_logret_cc']\n",
            "[INFO] Groups covered in NEURAL40: ['CPI', 'FEDFUNDS', 'GOOGL', 'MSFT', 'NVDA', 'QQQ', 'SPY', 'TNX', 'VIX', 'XLK', '^GDAXI', '^IXIC', 'eps']\n",
            "[INFO] Groups covered in NEURAL80: ['CPI', 'FEDFUNDS', 'GOOGL', 'MSFT', 'NVDA', 'QQQ', 'SPY', 'TNX', 'VIX', 'XLK', '^GDAXI', '^IXIC', 'eps', 'is']\n",
            "[OK] BLOCK 21 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preconditions\n",
        "assert \"X_train_t1\" in globals(), \"[ERROR] X_train_t1 missing. Run BLOCK 20 first.\"\n",
        "assert \"y_train_t1\" in globals(), \"[ERROR] y_train_t1 missing. Run BLOCK 20 first.\"\n",
        "\n",
        "FS_OUT = ensure_dir(Path(FS_DIR))\n",
        "\n",
        "# Params from RUN_PARAMS\n",
        "NN_CFG = RUN_PARAMS[\"nn_feature_select\"]\n",
        "N40 = int(NN_CFG[\"n40\"])\n",
        "N80 = int(NN_CFG[\"n80\"])\n",
        "per_group_40 = int(NN_CFG[\"per_group_40\"])\n",
        "per_group_80 = int(NN_CFG[\"per_group_80\"])\n",
        "corr_thr = float(NN_CFG[\"corr_thr\"])\n",
        "mi_neighbors = int(NN_CFG[\"mi_n_neighbors\"])\n",
        "mi_random_state = int(NN_CFG[\"mi_random_state\"])\n",
        "\n",
        "\n",
        "# Helpers\n",
        "def _dedup_preserve_order(seq):\n",
        "    \"\"\"Remove duplicates while preserving order.\"\"\"\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in seq:\n",
        "        if isinstance(x, str) and x and (x not in seen):\n",
        "            out.append(x)\n",
        "            seen.add(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _feature_group(col: str) -> str:\n",
        "    \"\"\"Extract feature group from column name.\"\"\"\n",
        "    if not isinstance(col, str) or not col:\n",
        "        return \"OTHER\"\n",
        "    if col.startswith(\"^\"):\n",
        "        return col.split(\"_\", 1)[0]\n",
        "    return col.split(\"_\", 1)[0]\n",
        "\n",
        "\n",
        "def _safe_spearman_corr(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Spearman correlation on numeric DataFrame.\"\"\"\n",
        "    return df.corr(method=\"spearman\")\n",
        "\n",
        "\n",
        "def _select_with_decorrelation(ranked_cols, X_train_num, k, corr_threshold):\n",
        "    \"\"\"Select top-k features with de-correlation.\"\"\"\n",
        "    ranked_cols = [c for c in ranked_cols if c in X_train_num.columns]\n",
        "    if k <= 0:\n",
        "        return []\n",
        "\n",
        "    selected = []\n",
        "    if len(ranked_cols) == 0:\n",
        "        return selected\n",
        "\n",
        "    # Precompute correlation only once for speed\n",
        "    sub = X_train_num.loc[:, ranked_cols]\n",
        "    corr = _safe_spearman_corr(sub).abs()\n",
        "\n",
        "    for c in ranked_cols:\n",
        "        if len(selected) == 0:\n",
        "            selected.append(c)\n",
        "            if len(selected) >= k:\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        too_close = False\n",
        "        for s in selected:\n",
        "            val = corr.at[c, s]\n",
        "            if pd.notna(val) and float(val) >= corr_threshold:\n",
        "                too_close = True\n",
        "                break\n",
        "        if not too_close:\n",
        "            selected.append(c)\n",
        "            if len(selected) >= k:\n",
        "                break\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "# Numeric matrix (TRAIN only) + cleaning\n",
        "X = X_train_t1.copy()\n",
        "y = y_train_t1.astype(float).to_numpy()\n",
        "\n",
        "# Keep numeric dtypes only\n",
        "X = X.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "# Drop constant columns\n",
        "nunique = X.nunique(dropna=False)\n",
        "const_cols = nunique[nunique <= 1].index.tolist()\n",
        "if const_cols:\n",
        "    X = X.drop(columns=const_cols)\n",
        "    print(f\"[INFO] Dropped {len(const_cols)} constant columns\")\n",
        "\n",
        "# Replace inf -> nan, then drop cols that have any non-finite\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "bad_cols = [c for c in X.columns if not np.isfinite(X[c].to_numpy(dtype=float)).all()]\n",
        "if bad_cols:\n",
        "    X = X.drop(columns=bad_cols)\n",
        "    print(f\"[INFO] Dropped {len(bad_cols)} columns with non-finite values\")\n",
        "\n",
        "if X.shape[1] == 0:\n",
        "    raise ValueError(\"[ERROR] No usable numeric features left after cleaning on TRAIN.\")\n",
        "\n",
        "print(f\"[INFO] Features after cleaning: {X.shape[1]}\")\n",
        "\n",
        "# Mutual Information ranking (TRAIN only)\n",
        "print(\"[INFO] Computing Mutual Information (this may take a minute)...\")\n",
        "mi = mutual_info_regression(\n",
        "    X.to_numpy(dtype=float),\n",
        "    y,\n",
        "    n_neighbors=mi_neighbors,\n",
        "    random_state=mi_random_state,\n",
        ")\n",
        "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Group-aware shortlist\n",
        "groups = {}\n",
        "for col in mi_series.index:\n",
        "    g = _feature_group(col)\n",
        "    groups.setdefault(g, []).append(col)\n",
        "\n",
        "\n",
        "def _build_group_seed(per_group):\n",
        "    \"\"\"Build seed list with per_group features from each group.\"\"\"\n",
        "    seed = []\n",
        "    for g, cols in groups.items():\n",
        "        take = cols[:per_group]\n",
        "        seed.extend(take)\n",
        "    return _dedup_preserve_order(seed)\n",
        "\n",
        "\n",
        "seed40 = _build_group_seed(per_group_40)\n",
        "seed80 = _build_group_seed(per_group_80)\n",
        "\n",
        "# Global ranked list\n",
        "ranked_all = mi_series.index.tolist()\n",
        "\n",
        "# Final pick with de-correlation\n",
        "rank40 = _dedup_preserve_order(seed40 + ranked_all)\n",
        "rank80 = _dedup_preserve_order(seed80 + ranked_all)\n",
        "\n",
        "neural_40 = _select_with_decorrelation(rank40, X, N40, corr_thr)\n",
        "neural_80 = _select_with_decorrelation(rank80, X, N80, corr_thr)\n",
        "\n",
        "if len(neural_40) == 0 or len(neural_80) == 0:\n",
        "    raise ValueError(\"[ERROR] NN feature selection produced empty sets.\")\n",
        "\n",
        "# Ensure 40 âŠ† 80 if possible\n",
        "if not set(neural_40).issubset(set(neural_80)):\n",
        "    base = _dedup_preserve_order(neural_40 + neural_80 + ranked_all)\n",
        "    neural_80 = _select_with_decorrelation(base, X, N80, corr_thr)\n",
        "\n",
        "# Save\n",
        "p40 = FS_OUT / \"neural_feature_cols_40_bygroup.pkl\"\n",
        "p80 = FS_OUT / \"neural_feature_cols_80_bygroup.pkl\"\n",
        "save_pickle(neural_40, p40)\n",
        "save_pickle(neural_80, p80)\n",
        "\n",
        "# Also save to DRIVE\n",
        "FS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"fs_dir\"]))\n",
        "copy_file(p40, FS_OUT_DRIVE / p40.name)\n",
        "copy_file(p80, FS_OUT_DRIVE / p80.name)\n",
        "\n",
        "print(\"[OK] NN feature lists saved:\")\n",
        "print(\"  -\", p40, \"| n=\", len(neural_40))\n",
        "print(\"  -\", p80, \"| n=\", len(neural_80))\n",
        "\n",
        "# Diagnostics\n",
        "print(\"[INFO] Top-10 MI features (TRAIN):\", mi_series.head(10).index.tolist())\n",
        "print(\"[INFO] Groups covered in NEURAL40:\", sorted({_feature_group(c) for c in neural_40}))\n",
        "print(\"[INFO] Groups covered in NEURAL80:\", sorted({_feature_group(c) for c in neural_80}))\n",
        "\n",
        "print(\"[OK] BLOCK 21 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3101cc5f",
      "metadata": {
        "id": "3101cc5f"
      },
      "source": [
        "## BLOCK 22 â€” NEURAL FEATURE PREP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "34278f05",
      "metadata": {
        "id": "34278f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3dc75c2-572c-40ce-bdb5-facbb50480d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [LOAD] neural_feature_cols_40_bygroup.pkl <- RUN_ID (LOCAL)\n",
            "  [LOAD] neural_feature_cols_80_bygroup.pkl <- RUN_ID (LOCAL)\n",
            "\n",
            "[OK] Neural feature matrices created:\n",
            "  - 40: TRAIN=(347, 40) | VALID=(77, 40) | TEST=(87, 40)\n",
            "  - 80: TRAIN=(347, 80) | VALID=(77, 80) | TEST=(87, 80)\n",
            "\n",
            "[OK] Saved Neural splits to: /content/my_project/data/processed\n",
            "  - X_train/valid/test_neural_40.pkl\n",
            "  - X_train/valid/test_neural_80.pkl\n",
            "[OK] BLOCK 22 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preconditions\n",
        "assert \"X_train_t1\" in globals(), \"[ERROR] X_train_t1 missing. Run BLOCK 20 first.\"\n",
        "assert \"X_valid_t1\" in globals(), \"[ERROR] X_valid_t1 missing. Run BLOCK 20 first.\"\n",
        "assert \"X_test_t1\" in globals(), \"[ERROR] X_test_t1 missing. Run BLOCK 20 first.\"\n",
        "\n",
        "FS_OUT = ensure_dir(Path(FS_DIR))\n",
        "FS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"fs_dir\"]))\n",
        "\n",
        "# Load neural feature lists from BLOCK 21 (with fallback)\n",
        "FS_DIR_LOCAL = Path(LOCAL_PATHS[\"fs_dir\"])\n",
        "FS_DIR_DRIVE = Path(DRIVE_PATHS[\"fs_dir\"])\n",
        "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "neural_cols_40 = load_with_fallback(\n",
        "    \"neural_feature_cols_40_bygroup.pkl\",\n",
        "    FS_DIR_LOCAL, FALLBACK_PROC_LOCAL, FS_DIR_DRIVE, FALLBACK_PROC_DRIVE\n",
        ")\n",
        "neural_cols_80 = load_with_fallback(\n",
        "    \"neural_feature_cols_80_bygroup.pkl\",\n",
        "    FS_DIR_LOCAL, FALLBACK_PROC_LOCAL, FS_DIR_DRIVE, FALLBACK_PROC_DRIVE\n",
        ")\n",
        "\n",
        "# Clean and dedupe\n",
        "neural_cols_40 = list(dict.fromkeys([c for c in neural_cols_40 if isinstance(c, str) and c.strip()]))\n",
        "neural_cols_80 = list(dict.fromkeys([c for c in neural_cols_80 if isinstance(c, str) and c.strip()]))\n",
        "\n",
        "# Filter to columns that exist in X_train_t1\n",
        "train_cols = set(X_train_t1.columns)\n",
        "neural_cols_40 = [c for c in neural_cols_40 if c in train_cols]\n",
        "neural_cols_80 = [c for c in neural_cols_80 if c in train_cols]\n",
        "\n",
        "if len(neural_cols_40) == 0:\n",
        "    raise ValueError(\"[ERROR] neural_cols_40 became empty after filtering to X_train_t1.columns.\")\n",
        "if len(neural_cols_80) == 0:\n",
        "    raise ValueError(\"[ERROR] neural_cols_80 became empty after filtering to X_train_t1.columns.\")\n",
        "\n",
        "# Verify columns exist in valid/test splits\n",
        "valid_cols = set(X_valid_t1.columns)\n",
        "test_cols = set(X_test_t1.columns)\n",
        "\n",
        "missing_valid_40 = sorted(set(neural_cols_40) - valid_cols)\n",
        "missing_test_40 = sorted(set(neural_cols_40) - test_cols)\n",
        "missing_valid_80 = sorted(set(neural_cols_80) - valid_cols)\n",
        "missing_test_80 = sorted(set(neural_cols_80) - test_cols)\n",
        "\n",
        "\n",
        "def _report_missing(tag, miss):\n",
        "    \"\"\"Report missing columns.\"\"\"\n",
        "    if miss:\n",
        "        print(f\"[ERROR] Missing in {tag}: {len(miss)} columns (showing up to 20): {miss[:20]}\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "err = False\n",
        "err |= _report_missing(\"X_valid_t1 (neural_40)\", missing_valid_40)\n",
        "err |= _report_missing(\"X_test_t1  (neural_40)\", missing_test_40)\n",
        "err |= _report_missing(\"X_valid_t1 (neural_80)\", missing_valid_80)\n",
        "err |= _report_missing(\"X_test_t1  (neural_80)\", missing_test_80)\n",
        "\n",
        "if err:\n",
        "    raise KeyError(\"[ERROR] Inconsistent columns across splits for neural feature sets.\")\n",
        "\n",
        "# Build neural feature matrices\n",
        "X_train_neural_40 = X_train_t1.loc[:, neural_cols_40].copy()\n",
        "X_valid_neural_40 = X_valid_t1.loc[:, neural_cols_40].copy()\n",
        "X_test_neural_40 = X_test_t1.loc[:, neural_cols_40].copy()\n",
        "\n",
        "X_train_neural_80 = X_train_t1.loc[:, neural_cols_80].copy()\n",
        "X_valid_neural_80 = X_valid_t1.loc[:, neural_cols_80].copy()\n",
        "X_test_neural_80 = X_test_t1.loc[:, neural_cols_80].copy()\n",
        "\n",
        "print(\"\\n[OK] Neural feature matrices created:\")\n",
        "print(f\"  - 40: TRAIN={X_train_neural_40.shape} | VALID={X_valid_neural_40.shape} | TEST={X_test_neural_40.shape}\")\n",
        "print(f\"  - 80: TRAIN={X_train_neural_80.shape} | VALID={X_valid_neural_80.shape} | TEST={X_test_neural_80.shape}\")\n",
        "\n",
        "# Save resolved neural feature lists to FS_DIR\n",
        "p40_res = FS_OUT / \"neural_feature_cols_40_bygroup_resolved.pkl\"\n",
        "p80_res = FS_OUT / \"neural_feature_cols_80_bygroup_resolved.pkl\"\n",
        "save_pickle(neural_cols_40, p40_res)\n",
        "save_pickle(neural_cols_80, p80_res)\n",
        "\n",
        "copy_file(p40_res, FS_OUT_DRIVE / p40_res.name)\n",
        "copy_file(p80_res, FS_OUT_DRIVE / p80_res.name)\n",
        "\n",
        "# Save Neural 40 splits to data/processed\n",
        "LOCAL_PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
        "DRIVE_PROC_DATA_DIR = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "X_train_neural_40.to_pickle(LOCAL_PROC_DATA_DIR / \"X_train_neural_40.pkl\")\n",
        "X_valid_neural_40.to_pickle(LOCAL_PROC_DATA_DIR / \"X_valid_neural_40.pkl\")\n",
        "X_test_neural_40.to_pickle(LOCAL_PROC_DATA_DIR / \"X_test_neural_40.pkl\")\n",
        "\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_train_neural_40.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_neural_40.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_valid_neural_40.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_neural_40.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_test_neural_40.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_neural_40.pkl\")\n",
        "\n",
        "# Save Neural 80 splits to data/processed\n",
        "X_train_neural_80.to_pickle(LOCAL_PROC_DATA_DIR / \"X_train_neural_80.pkl\")\n",
        "X_valid_neural_80.to_pickle(LOCAL_PROC_DATA_DIR / \"X_valid_neural_80.pkl\")\n",
        "X_test_neural_80.to_pickle(LOCAL_PROC_DATA_DIR / \"X_test_neural_80.pkl\")\n",
        "\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_train_neural_80.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_neural_80.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_valid_neural_80.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_neural_80.pkl\")\n",
        "copy_file(LOCAL_PROC_DATA_DIR / \"X_test_neural_80.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_neural_80.pkl\")\n",
        "\n",
        "print(\"\\n[OK] Saved Neural splits to:\", LOCAL_PROC_DATA_DIR)\n",
        "print(\"  - X_train/valid/test_neural_40.pkl\")\n",
        "print(\"  - X_train/valid/test_neural_80.pkl\")\n",
        "\n",
        "print(\"[OK] BLOCK 22 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dLWrjqUoSAXH",
      "metadata": {
        "id": "dLWrjqUoSAXH"
      },
      "source": [
        "---\n",
        "# SECTION 5: Feature Selection\n",
        "\n",
        "**XGBoost feature selection with mutual information**\n",
        "\n",
        "**Block:** 23"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a42ed7a",
      "metadata": {
        "id": "8a42ed7a"
      },
      "source": [
        "## BLOCK 23 â€” XGB FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "8c6d97bd",
      "metadata": {
        "id": "8c6d97bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4c40ab-680f-452e-f949-3c2fa28c6b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading splits with fallback...\n",
            "  [LOAD] X_train_xgb.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_xgb.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_xgb.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_valid.pkl <- data/processed (LOCAL)\n",
            "[INFO] Loaded: X_train=(347, 394) | X_valid=(77, 394) | X_test=(87, 394)\n",
            "[INFO] XGB Feature Selection output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/feature_selection\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/feature_selection\n",
            "[INFO] Found missingness flags: 3\n",
            "[INFO] Dropping constant TRAIN columns: 6\n",
            "[INFO] Spearman filter: start=385 | kept=194 | thresh=0.85\n",
            "[INFO] Training XGBoost for feature selection...\n",
            "[INFO] XGB train done. best_iteration=20 | best_score(valid_rmse)=0.02192389685676885\n",
            "[INFO] Computing permutation importance (repeats=15)...\n",
            "[INFO] Candidates by GAIN: cum<=0.85 with min 10 => 194\n",
            "[INFO] Selected after strict filters: 0\n",
            "[WARN] Too few features; relaxing to perm_mean > 0.\n",
            "[INFO] Selected after relaxed filter: 0\n",
            "[WARN] Still too few; final fallback to GAIN candidates excluding strongly-negative.\n",
            "[INFO] Selected after final fallback: 0\n",
            "[WARN] Final fallback: taking top 10 features by XGB GAIN.\n",
            "[INFO] Selected after GAIN-only fallback: 10\n",
            "[INFO] Missingness flags added: 0\n",
            "[INFO] FINAL selected features: 10\n",
            "\n",
            "[OK] Saved XGB Feature Selection artifacts:\n",
            "  - selected_features_xgb.txt\n",
            "  - selected_features_xgb.pkl\n",
            "  - selected_features_xgb.csv\n",
            "  - feature_importance_gain.csv\n",
            "  - feature_importance_permutation_valid.csv\n",
            "[OK] Saved to /content/my_project/data/processed:\n",
            "  - X_train_xgb_selected.pkl ((347, 10))\n",
            "  - X_valid_xgb_selected.pkl ((77, 10))\n",
            "  - X_test_xgb_selected.pkl ((87, 10))\n",
            "[OK] BLOCK 23 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Config from RUN_PARAMS\n",
        "XGB_FS_CFG = RUN_PARAMS[\"xgb_fs\"]\n",
        "MISSING_SUFFIX = \"_is_missing\"\n",
        "\n",
        "# Input/Output directories\n",
        "PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
        "FS_OUT_LOCAL = ensure_dir(Path(FS_DIR))\n",
        "FS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"fs_dir\"]))\n",
        "\n",
        "# -------------------------\n",
        "# Load splits with fallback (RUN_ID -> data/processed -> DRIVE)\n",
        "# -------------------------\n",
        "RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
        "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
        "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "print(\"[INFO] Loading splits with fallback...\")\n",
        "\n",
        "X_train_t1 = load_with_fallback(\"X_train_xgb.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "X_valid_t1 = load_with_fallback(\"X_valid_xgb.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "X_test_t1 = load_with_fallback(\"X_test_xgb.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "print(f\"[INFO] Loaded: X_train={X_train_t1.shape} | X_valid={X_valid_t1.shape} | X_test={X_test_t1.shape}\")\n",
        "\n",
        "print(\"[INFO] XGB Feature Selection output dirs:\")\n",
        "print(\"  - LOCAL:\", FS_OUT_LOCAL)\n",
        "print(\"  - DRIVE:\", FS_OUT_DRIVE)\n",
        "\n",
        "\n",
        "# Metric functions defined in Cell 5 (using w_rmse instead of weighted_rmse)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 1) Identify missingness flags (for dependency closure)\n",
        "# -------------------------\n",
        "all_train_cols = list(X_train_t1.columns)\n",
        "missing_flags = [c for c in all_train_cols if c.endswith(MISSING_SUFFIX)]\n",
        "missing_flag_set = set(missing_flags)\n",
        "\n",
        "base_to_missing = {}\n",
        "for flag in missing_flags:\n",
        "    base = flag[:-len(MISSING_SUFFIX)]\n",
        "    if base in missing_flag_set:\n",
        "        continue\n",
        "    if base in all_train_cols:\n",
        "        base_to_missing[base] = flag\n",
        "\n",
        "print(f\"[INFO] Found missingness flags: {len(missing_flags)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 2) Build matrices for FS (exclude flags)\n",
        "# -------------------------\n",
        "nonflag_cols = [c for c in all_train_cols if c not in missing_flag_set and c != TARGET_T1 and c != \"sample_weight\"]\n",
        "\n",
        "Xtr = X_train_t1.loc[:, nonflag_cols].copy()\n",
        "Xva = X_valid_t1.loc[:, nonflag_cols].copy()\n",
        "\n",
        "ytr = y_train_t1.astype(float).copy()\n",
        "yva = y_valid_t1.astype(float).copy()\n",
        "\n",
        "wtr = _to_np(w_train)\n",
        "wva = _to_np(w_valid)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Drop constant TRAIN columns\n",
        "# -------------------------\n",
        "nunique = Xtr.nunique(dropna=False)\n",
        "const_cols = nunique[nunique <= 1].index.tolist()\n",
        "if const_cols:\n",
        "    print(f\"[INFO] Dropping constant TRAIN columns: {len(const_cols)}\")\n",
        "    Xtr = Xtr.drop(columns=const_cols)\n",
        "    Xva = Xva.drop(columns=const_cols, errors=\"ignore\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Spearman filter\n",
        "# -------------------------\n",
        "SPEARMAN_THRESH = float(XGB_FS_CFG[\"spearman_thresh\"])\n",
        "\n",
        "corr_ff = Xtr.corr(method=\"spearman\").abs().fillna(0.0)\n",
        "cols = list(corr_ff.columns)\n",
        "\n",
        "feat_to_y = Xtr.apply(lambda s: s.corr(ytr, method=\"spearman\"))\n",
        "feat_to_y_abs = feat_to_y.abs().fillna(0.0)\n",
        "\n",
        "pairs = []\n",
        "for i in range(len(cols)):\n",
        "    for j in range(i + 1, len(cols)):\n",
        "        cval = float(corr_ff.iat[i, j])\n",
        "        if cval > SPEARMAN_THRESH:\n",
        "            pairs.append((cols[i], cols[j], cval))\n",
        "pairs.sort(key=lambda t: t[2], reverse=True)\n",
        "\n",
        "active = set(cols)\n",
        "\n",
        "for a, b, cval in pairs:\n",
        "    if (a in active) and (b in active):\n",
        "        ay = float(feat_to_y_abs.get(a, 0.0))\n",
        "        by = float(feat_to_y_abs.get(b, 0.0))\n",
        "        drop = a if ay < by else b\n",
        "        active.remove(drop)\n",
        "\n",
        "kept_cols = [c for c in cols if c in active]\n",
        "print(f\"[INFO] Spearman filter: start={len(cols)} | kept={len(kept_cols)} | thresh={SPEARMAN_THRESH}\")\n",
        "\n",
        "X_train_fs = Xtr.loc[:, kept_cols].copy()\n",
        "X_valid_fs = Xva.loc[:, kept_cols].copy()\n",
        "\n",
        "# -------------------------\n",
        "# 5) XGBoost FS model (GAIN)\n",
        "# -------------------------\n",
        "xgb_params = dict(\n",
        "    n_estimators=int(XGB_FS_CFG[\"n_estimators\"]),\n",
        "    learning_rate=float(XGB_FS_CFG[\"learning_rate\"]),\n",
        "    max_depth=int(XGB_FS_CFG[\"max_depth\"]),\n",
        "    min_child_weight=float(XGB_FS_CFG[\"min_child_weight\"]),\n",
        "    gamma=float(XGB_FS_CFG[\"gamma\"]),\n",
        "    subsample=float(XGB_FS_CFG[\"subsample\"]),\n",
        "    colsample_bytree=float(XGB_FS_CFG[\"colsample_bytree\"]),\n",
        "    reg_alpha=float(XGB_FS_CFG[\"reg_alpha\"]),\n",
        "    reg_lambda=float(XGB_FS_CFG[\"reg_lambda\"]),\n",
        "    max_delta_step=float(XGB_FS_CFG[\"max_delta_step\"]),\n",
        "    objective=\"reg:squarederror\",\n",
        "    eval_metric=\"rmse\",\n",
        "    tree_method=\"hist\",\n",
        "    random_state=int(XGB_FS_CFG[\"random_state\"]),\n",
        "    n_jobs=-1,\n",
        "    early_stopping_rounds=int(XGB_FS_CFG[\"early_stopping_rounds\"]),\n",
        ")\n",
        "\n",
        "print(\"[INFO] Training XGBoost for feature selection...\")\n",
        "model_fs = xgb.XGBRegressor(**xgb_params)\n",
        "model_fs.fit(\n",
        "    X_train_fs, ytr.loc[X_train_fs.index],\n",
        "    sample_weight=wtr,\n",
        "    eval_set=[(X_valid_fs, yva.loc[X_valid_fs.index])],\n",
        "    sample_weight_eval_set=[wva],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "best_iter = getattr(model_fs, \"best_iteration\", None)\n",
        "best_score = getattr(model_fs, \"best_score\", None)\n",
        "print(f\"[INFO] XGB train done. best_iteration={best_iter} | best_score(valid_rmse)={best_score}\")\n",
        "\n",
        "booster = model_fs.get_booster()\n",
        "score_gain = booster.get_score(importance_type=\"gain\")\n",
        "\n",
        "imp_gain = pd.DataFrame({\"feature\": kept_cols})\n",
        "imp_gain[\"gain\"] = imp_gain[\"feature\"].map(score_gain).fillna(0.0).astype(float)\n",
        "imp_gain = imp_gain.sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "gain_sum = float(imp_gain[\"gain\"].sum())\n",
        "imp_gain[\"gain_frac\"] = imp_gain[\"gain\"] / (gain_sum + EPS)\n",
        "imp_gain[\"cum_gain\"] = imp_gain[\"gain_frac\"].cumsum()\n",
        "\n",
        "# -------------------------\n",
        "# 6) Permutation importance on VALID\n",
        "# -------------------------\n",
        "def neg_weighted_rmse_scorer(estimator, X, y):\n",
        "    p = estimator.predict(X)\n",
        "    return -w_rmse(y, p, wva)\n",
        "\n",
        "\n",
        "PERM_REPEATS = int(XGB_FS_CFG[\"perm_repeats\"])\n",
        "\n",
        "print(f\"[INFO] Computing permutation importance (repeats={PERM_REPEATS})...\")\n",
        "perm = permutation_importance(\n",
        "    model_fs,\n",
        "    X_valid_fs,\n",
        "    yva.loc[X_valid_fs.index],\n",
        "    scoring=neg_weighted_rmse_scorer,\n",
        "    n_repeats=PERM_REPEATS,\n",
        "    random_state=int(XGB_FS_CFG[\"random_state\"]),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_df = pd.DataFrame({\n",
        "    \"feature\": X_valid_fs.columns,\n",
        "    \"perm_importance_mean\": perm.importances_mean,\n",
        "    \"perm_importance_std\": perm.importances_std\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "perm_df = perm_df.merge(\n",
        "    imp_gain.loc[:, [\"feature\", \"gain\", \"gain_frac\", \"cum_gain\"]],\n",
        "    on=\"feature\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 7) Selection policy + fallback ladder\n",
        "# -------------------------\n",
        "GAIN_CUM_THRESH = float(XGB_FS_CFG[\"gain_cum_thresh\"])\n",
        "MIN_FEATURES = int(XGB_FS_CFG[\"min_features\"])\n",
        "NEG_SIGMA = float(XGB_FS_CFG[\"neg_sigma\"])\n",
        "POS_SIGMA = float(XGB_FS_CFG[\"pos_sigma\"])\n",
        "MIN_GAIN = float(XGB_FS_CFG[\"min_gain\"])\n",
        "\n",
        "perm_df[\"perm_strongly_negative\"] = (\n",
        "    (perm_df[\"perm_importance_mean\"] < 0) &\n",
        "    (np.abs(perm_df[\"perm_importance_mean\"]) > (NEG_SIGMA * (perm_df[\"perm_importance_std\"] + EPS)))\n",
        ")\n",
        "\n",
        "perm_df[\"perm_confident_positive\"] = (\n",
        "    (perm_df[\"perm_importance_mean\"] > 0) &\n",
        "    (perm_df[\"perm_importance_mean\"] > (POS_SIGMA * (perm_df[\"perm_importance_std\"] + EPS)))\n",
        ")\n",
        "\n",
        "strong_neg_set = set(perm_df.loc[perm_df[\"perm_strongly_negative\"], \"feature\"].tolist())\n",
        "\n",
        "gain_candidates = imp_gain.loc[\n",
        "    (imp_gain[\"cum_gain\"] <= GAIN_CUM_THRESH) | (imp_gain.index < MIN_FEATURES),\n",
        "    \"feature\"\n",
        "].tolist()\n",
        "print(f\"[INFO] Candidates by GAIN: cum<={GAIN_CUM_THRESH} with min {MIN_FEATURES} => {len(gain_candidates)}\")\n",
        "\n",
        "perm_map = perm_df.set_index(\"feature\")[[\"perm_confident_positive\", \"perm_importance_mean\", \"gain\"]]\n",
        "\n",
        "selected = []\n",
        "for f in gain_candidates:\n",
        "    if f in strong_neg_set:\n",
        "        continue\n",
        "    g = float(perm_map.loc[f, \"gain\"]) if f in perm_map.index else 0.0\n",
        "    if g < MIN_GAIN:\n",
        "        continue\n",
        "    is_pos = bool(perm_map.loc[f, \"perm_confident_positive\"]) if f in perm_map.index else False\n",
        "    if not is_pos:\n",
        "        continue\n",
        "    selected.append(f)\n",
        "\n",
        "print(f\"[INFO] Selected after strict filters: {len(selected)}\")\n",
        "\n",
        "# Fallback 1: relax to perm_mean > 0\n",
        "if len(selected) < MIN_FEATURES:\n",
        "    print(\"[WARN] Too few features; relaxing to perm_mean > 0.\")\n",
        "    selected = []\n",
        "    for f in gain_candidates:\n",
        "        if f in strong_neg_set:\n",
        "            continue\n",
        "        g = float(perm_map.loc[f, \"gain\"]) if f in perm_map.index else 0.0\n",
        "        if g < MIN_GAIN:\n",
        "            continue\n",
        "        pm = float(perm_map.loc[f, \"perm_importance_mean\"]) if f in perm_map.index else -1.0\n",
        "        if pm <= 0:\n",
        "            continue\n",
        "        selected.append(f)\n",
        "    print(f\"[INFO] Selected after relaxed filter: {len(selected)}\")\n",
        "\n",
        "# Fallback 2: gain candidates excluding strong-neg\n",
        "if len(selected) < MIN_FEATURES:\n",
        "    print(\"[WARN] Still too few; final fallback to GAIN candidates excluding strongly-negative.\")\n",
        "    selected = []\n",
        "    for f in gain_candidates:\n",
        "        if f in strong_neg_set:\n",
        "            continue\n",
        "        g = float(perm_map.loc[f, \"gain\"]) if f in perm_map.index else 0.0\n",
        "        if g < MIN_GAIN:\n",
        "            continue\n",
        "        selected.append(f)\n",
        "    print(f\"[INFO] Selected after final fallback: {len(selected)}\")\n",
        "\n",
        "# Fallback 3: GUARANTEE min_features - take top by XGB GAIN\n",
        "if len(selected) < MIN_FEATURES:\n",
        "    print(f\"[WARN] Final fallback: taking top {MIN_FEATURES} features by XGB GAIN.\")\n",
        "    # Sort gain_candidates by XGB gain (highest first)\n",
        "    top_by_gain = imp_gain.head(MIN_FEATURES)[\"feature\"].tolist()\n",
        "    selected = top_by_gain\n",
        "    print(f\"[INFO] Selected after GAIN-only fallback: {len(selected)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 8) Dependency closure: add X_is_missing if exists\n",
        "# -------------------------\n",
        "selected_set = set(selected)\n",
        "flags_added = []\n",
        "\n",
        "for base, flag in base_to_missing.items():\n",
        "    if (base in selected_set) and (flag in X_train_t1.columns) and (flag not in selected_set):\n",
        "        selected.append(flag)\n",
        "        selected_set.add(flag)\n",
        "        flags_added.append(flag)\n",
        "\n",
        "print(f\"[INFO] Missingness flags added: {len(flags_added)}\")\n",
        "\n",
        "selected_final = list(dict.fromkeys(selected))\n",
        "print(f\"[INFO] FINAL selected features: {len(selected_final)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 9) Save outputs (LOCAL + DRIVE)\n",
        "# -------------------------\n",
        "# Selected features\n",
        "sel_txt = FS_OUT_LOCAL / \"selected_features_xgb.txt\"\n",
        "sel_pkl = FS_OUT_LOCAL / \"selected_features_xgb.pkl\"\n",
        "sel_csv = FS_OUT_LOCAL / \"selected_features_xgb.csv\"\n",
        "\n",
        "sel_txt.write_text(\"\\n\".join(selected_final), encoding=\"utf-8\")\n",
        "save_pickle(selected_final, sel_pkl)\n",
        "pd.DataFrame({\"feature\": selected_final}).to_csv(sel_csv, index=False)\n",
        "\n",
        "copy_file(sel_txt, FS_OUT_DRIVE / sel_txt.name)\n",
        "copy_file(sel_pkl, FS_OUT_DRIVE / sel_pkl.name)\n",
        "copy_file(sel_csv, FS_OUT_DRIVE / sel_csv.name)\n",
        "\n",
        "# Importance tables\n",
        "imp_gain_csv = FS_OUT_LOCAL / \"feature_importance_gain.csv\"\n",
        "perm_csv = FS_OUT_LOCAL / \"feature_importance_permutation_valid.csv\"\n",
        "\n",
        "imp_gain.to_csv(imp_gain_csv, index=False)\n",
        "perm_df.sort_values(\"perm_importance_mean\", ascending=False).to_csv(perm_csv, index=False)\n",
        "\n",
        "copy_file(imp_gain_csv, FS_OUT_DRIVE / imp_gain_csv.name)\n",
        "copy_file(perm_csv, FS_OUT_DRIVE / perm_csv.name)\n",
        "\n",
        "# Create and save filtered matrices (xgb_selected) to data/processed/\n",
        "DRIVE_PROC_DATA_DIR = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "X_train_xgb_selected = X_train_t1.loc[:, selected_final].copy()\n",
        "X_valid_xgb_selected = X_valid_t1.loc[:, selected_final].copy()\n",
        "X_test_xgb_selected = X_test_t1.loc[:, selected_final].copy()\n",
        "\n",
        "X_train_xgb_selected.to_pickle(PROC_DATA_DIR / \"X_train_xgb_selected.pkl\")\n",
        "X_valid_xgb_selected.to_pickle(PROC_DATA_DIR / \"X_valid_xgb_selected.pkl\")\n",
        "X_test_xgb_selected.to_pickle(PROC_DATA_DIR / \"X_test_xgb_selected.pkl\")\n",
        "\n",
        "copy_file(PROC_DATA_DIR / \"X_train_xgb_selected.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_xgb_selected.pkl\")\n",
        "copy_file(PROC_DATA_DIR / \"X_valid_xgb_selected.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_xgb_selected.pkl\")\n",
        "copy_file(PROC_DATA_DIR / \"X_test_xgb_selected.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_xgb_selected.pkl\")\n",
        "\n",
        "print(\"\\n[OK] Saved XGB Feature Selection artifacts:\")\n",
        "print(\"  -\", sel_txt.name)\n",
        "print(\"  -\", sel_pkl.name)\n",
        "print(\"  -\", sel_csv.name)\n",
        "print(\"  -\", imp_gain_csv.name)\n",
        "print(\"  -\", perm_csv.name)\n",
        "print(f\"[OK] Saved to {PROC_DATA_DIR}:\")\n",
        "print(f\"  - X_train_xgb_selected.pkl ({X_train_xgb_selected.shape})\")\n",
        "print(f\"  - X_valid_xgb_selected.pkl ({X_valid_xgb_selected.shape})\")\n",
        "print(f\"  - X_test_xgb_selected.pkl ({X_test_xgb_selected.shape})\")\n",
        "\n",
        "print(\"[OK] BLOCK 23 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V3TLpp-ISAXI",
      "metadata": {
        "id": "V3TLpp-ISAXI"
      },
      "source": [
        "---\n",
        "# SECTION 6: XGBoost Hyperparameter Optimization\n",
        "\n",
        "**3-stage HPO: Broad â†’ Refine â†’ Low-LR**\n",
        "\n",
        "**Block:** 24"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c884fe67",
      "metadata": {
        "id": "c884fe67"
      },
      "source": [
        "## BLOCK 24 â€” XGB HYPERPARAMETER OPTIMIZATION (HPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "09ac2c55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "09ac2c55",
        "outputId": "a3f6c679-ce85-4982-a4a8-fe15bdf866f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [LOAD] selected_features_xgb.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded 10 selected features\n",
            "[INFO] Data loading with fallback enabled\n",
            "[INFO] HPO output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/model_selection\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/model_selection\n",
            "[INFO] Loading splits with fallback logic...\n",
            "  [LOAD] X_train_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_valid.pkl <- data/processed (LOCAL)\n",
            "[INFO] Loaded: X_train_sel=(347, 10) | X_valid_sel=(77, 10)\n",
            "[INFO] Shapes (selected): (347, 10) (77, 10)\n",
            "[INFO] Aligned VALID with lookback=7: skipped first 6 rows\n",
            "[INFO] VALID shape after alignment: (71, 10)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] VALID_ES shape: (31, 10) | VALID_SCORE shape: (40, 10)\n",
            "\n",
            "[INFO] HPO start | VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Stage1 trials=120 | Stage2 trials=60 | Stage2 LOW-LR=30\n",
            "[INFO] n_estimators=1500 | early_stop=40\n",
            "[INFO] S1 0000 | sc_wrmse=0.024931 | best=0.024931 | best_iter=21\n",
            "[INFO] S1 0009 | sc_wrmse=0.024948 | best=0.024905 | best_iter=21\n",
            "[INFO] S1 0019 | sc_wrmse=0.024965 | best=0.024895 | best_iter=22\n",
            "[INFO] S1 0029 | sc_wrmse=0.024971 | best=0.024798 | best_iter=19\n",
            "[INFO] S1 0039 | sc_wrmse=0.024942 | best=0.024798 | best_iter=21\n",
            "[INFO] S1 0049 | sc_wrmse=0.024871 | best=0.024775 | best_iter=21\n",
            "[INFO] S1 0059 | sc_wrmse=0.024945 | best=0.024775 | best_iter=22\n",
            "[INFO] S1 0069 | sc_wrmse=0.024961 | best=0.024775 | best_iter=21\n",
            "[INFO] S1 0079 | sc_wrmse=0.024974 | best=0.024775 | best_iter=22\n",
            "[INFO] S1 0089 | sc_wrmse=0.024969 | best=0.024781 | best_iter=58\n",
            "[INFO] S1 0099 | sc_wrmse=0.024965 | best=0.024713 | best_iter=21\n",
            "[INFO] S1 0109 | sc_wrmse=0.024940 | best=0.024713 | best_iter=21\n",
            "[INFO] S1 0119 | sc_wrmse=0.024977 | best=0.024713 | best_iter=2\n",
            "[INFO] S2 0000 | sc_wrmse=0.024662 | best=0.024662 | best_iter=20\n",
            "[INFO] S2 0019 | sc_wrmse=0.024779 | best=0.024652 | best_iter=21\n",
            "[INFO] S2 0039 | sc_wrmse=0.024741 | best=0.024652 | best_iter=5\n",
            "[INFO] S2 0059 | sc_wrmse=0.024663 | best=0.024652 | best_iter=20\n",
            "[INFO] S2L 0000 | sc_wrmse=0.024840 | best=0.024652 | best_iter=21\n",
            "[INFO] S2L 0019 | sc_wrmse=0.024835 | best=0.024652 | best_iter=21\n",
            "\n",
            "[INFO] Top 10 trials by VALID_SCORE wRMSE:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           stage  trial  valid_sc_wrmse  valid_sc_wmae  valid_sc_diracc  \\\n",
              "0  STAGE2_REFINE     10        0.024652       0.015426             0.65   \n",
              "1  STAGE2_REFINE      0        0.024662       0.015436             0.65   \n",
              "2  STAGE2_REFINE     59        0.024663       0.015436             0.65   \n",
              "3  STAGE2_REFINE     38        0.024663       0.015437             0.65   \n",
              "4  STAGE2_REFINE     31        0.024678       0.015450             0.65   \n",
              "5  STAGE2_REFINE     58        0.024682       0.015454             0.65   \n",
              "6  STAGE2_REFINE     37        0.024682       0.015454             0.65   \n",
              "7  STAGE2_REFINE     11        0.024686       0.015458             0.65   \n",
              "8  STAGE2_REFINE     26        0.024692       0.015463             0.65   \n",
              "9  STAGE2_REFINE     21        0.024696       0.015467             0.65   \n",
              "\n",
              "   best_iteration  best_score_rmse_eval_es  valid_es_wrmse_explicit  \\\n",
              "0              20                 0.016706                 0.016706   \n",
              "1              20                 0.016708                 0.016708   \n",
              "2              20                 0.016708                 0.016708   \n",
              "3              20                 0.016709                 0.016709   \n",
              "4               5                 0.016712                 0.016712   \n",
              "5              20                 0.016713                 0.016713   \n",
              "6              20                 0.016713                 0.016713   \n",
              "7               5                 0.016714                 0.016714   \n",
              "8              20                 0.016716                 0.016716   \n",
              "9              19                 0.016717                 0.016717   \n",
              "\n",
              "   elapsed_sec  max_depth  learning_rate  min_child_weight  subsample  \\\n",
              "0     0.029409          5       0.150000         25.162532   0.613330   \n",
              "1     0.029163          5       0.136988          9.776936   0.581053   \n",
              "2     0.027498          5       0.131810         18.182452   0.615668   \n",
              "3     0.029042          5       0.135731         22.658868   0.612473   \n",
              "4     0.023681          4       0.150000         24.684807   0.537844   \n",
              "5     0.028062          4       0.150000         10.971645   0.625433   \n",
              "6     0.028253          5       0.119743         15.416853   0.616164   \n",
              "7     0.024910          5       0.150000         11.012592   0.549376   \n",
              "8     0.030083          5       0.126259          8.906601   0.621300   \n",
              "9     0.028085          5       0.139619         17.228054   0.643756   \n",
              "\n",
              "   colsample_bytree     gamma     reg_alpha  reg_lambda  max_delta_step  \n",
              "0          0.657014  2.049081  1.748344e-04    0.006743        0.706817  \n",
              "1          0.690221  1.775688  8.155800e-05    0.016269        0.672960  \n",
              "2          0.588979  1.637688  4.938851e-06    0.020091        0.857587  \n",
              "3          0.556268  1.348584  5.314513e-07    0.025990        0.833280  \n",
              "4          0.661754  1.666847  5.579534e-07    0.020432        0.900477  \n",
              "5          0.679968  1.881768  7.499861e-07    0.015671        1.049515  \n",
              "6          0.635439  1.915929  9.935912e-06    0.009025        0.969461  \n",
              "7          0.582835  1.670690  6.289413e-06    0.011476        0.744665  \n",
              "8          0.656529  1.096136  1.312054e-06    0.046843        0.886311  \n",
              "9          0.532423  1.559824  1.873891e-04    0.044638        0.871253  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2306a90a-71f7-4d90-b921-7497dec46d6e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stage</th>\n",
              "      <th>trial</th>\n",
              "      <th>valid_sc_wrmse</th>\n",
              "      <th>valid_sc_wmae</th>\n",
              "      <th>valid_sc_diracc</th>\n",
              "      <th>best_iteration</th>\n",
              "      <th>best_score_rmse_eval_es</th>\n",
              "      <th>valid_es_wrmse_explicit</th>\n",
              "      <th>elapsed_sec</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>min_child_weight</th>\n",
              "      <th>subsample</th>\n",
              "      <th>colsample_bytree</th>\n",
              "      <th>gamma</th>\n",
              "      <th>reg_alpha</th>\n",
              "      <th>reg_lambda</th>\n",
              "      <th>max_delta_step</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>10</td>\n",
              "      <td>0.024652</td>\n",
              "      <td>0.015426</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016706</td>\n",
              "      <td>0.016706</td>\n",
              "      <td>0.029409</td>\n",
              "      <td>5</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>25.162532</td>\n",
              "      <td>0.613330</td>\n",
              "      <td>0.657014</td>\n",
              "      <td>2.049081</td>\n",
              "      <td>1.748344e-04</td>\n",
              "      <td>0.006743</td>\n",
              "      <td>0.706817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>0</td>\n",
              "      <td>0.024662</td>\n",
              "      <td>0.015436</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016708</td>\n",
              "      <td>0.016708</td>\n",
              "      <td>0.029163</td>\n",
              "      <td>5</td>\n",
              "      <td>0.136988</td>\n",
              "      <td>9.776936</td>\n",
              "      <td>0.581053</td>\n",
              "      <td>0.690221</td>\n",
              "      <td>1.775688</td>\n",
              "      <td>8.155800e-05</td>\n",
              "      <td>0.016269</td>\n",
              "      <td>0.672960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>59</td>\n",
              "      <td>0.024663</td>\n",
              "      <td>0.015436</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016708</td>\n",
              "      <td>0.016708</td>\n",
              "      <td>0.027498</td>\n",
              "      <td>5</td>\n",
              "      <td>0.131810</td>\n",
              "      <td>18.182452</td>\n",
              "      <td>0.615668</td>\n",
              "      <td>0.588979</td>\n",
              "      <td>1.637688</td>\n",
              "      <td>4.938851e-06</td>\n",
              "      <td>0.020091</td>\n",
              "      <td>0.857587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>38</td>\n",
              "      <td>0.024663</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016709</td>\n",
              "      <td>0.016709</td>\n",
              "      <td>0.029042</td>\n",
              "      <td>5</td>\n",
              "      <td>0.135731</td>\n",
              "      <td>22.658868</td>\n",
              "      <td>0.612473</td>\n",
              "      <td>0.556268</td>\n",
              "      <td>1.348584</td>\n",
              "      <td>5.314513e-07</td>\n",
              "      <td>0.025990</td>\n",
              "      <td>0.833280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>31</td>\n",
              "      <td>0.024678</td>\n",
              "      <td>0.015450</td>\n",
              "      <td>0.65</td>\n",
              "      <td>5</td>\n",
              "      <td>0.016712</td>\n",
              "      <td>0.016712</td>\n",
              "      <td>0.023681</td>\n",
              "      <td>4</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>24.684807</td>\n",
              "      <td>0.537844</td>\n",
              "      <td>0.661754</td>\n",
              "      <td>1.666847</td>\n",
              "      <td>5.579534e-07</td>\n",
              "      <td>0.020432</td>\n",
              "      <td>0.900477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>58</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.015454</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016713</td>\n",
              "      <td>0.016713</td>\n",
              "      <td>0.028062</td>\n",
              "      <td>4</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>10.971645</td>\n",
              "      <td>0.625433</td>\n",
              "      <td>0.679968</td>\n",
              "      <td>1.881768</td>\n",
              "      <td>7.499861e-07</td>\n",
              "      <td>0.015671</td>\n",
              "      <td>1.049515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>37</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.015454</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016713</td>\n",
              "      <td>0.016713</td>\n",
              "      <td>0.028253</td>\n",
              "      <td>5</td>\n",
              "      <td>0.119743</td>\n",
              "      <td>15.416853</td>\n",
              "      <td>0.616164</td>\n",
              "      <td>0.635439</td>\n",
              "      <td>1.915929</td>\n",
              "      <td>9.935912e-06</td>\n",
              "      <td>0.009025</td>\n",
              "      <td>0.969461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>11</td>\n",
              "      <td>0.024686</td>\n",
              "      <td>0.015458</td>\n",
              "      <td>0.65</td>\n",
              "      <td>5</td>\n",
              "      <td>0.016714</td>\n",
              "      <td>0.016714</td>\n",
              "      <td>0.024910</td>\n",
              "      <td>5</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>11.012592</td>\n",
              "      <td>0.549376</td>\n",
              "      <td>0.582835</td>\n",
              "      <td>1.670690</td>\n",
              "      <td>6.289413e-06</td>\n",
              "      <td>0.011476</td>\n",
              "      <td>0.744665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>26</td>\n",
              "      <td>0.024692</td>\n",
              "      <td>0.015463</td>\n",
              "      <td>0.65</td>\n",
              "      <td>20</td>\n",
              "      <td>0.016716</td>\n",
              "      <td>0.016716</td>\n",
              "      <td>0.030083</td>\n",
              "      <td>5</td>\n",
              "      <td>0.126259</td>\n",
              "      <td>8.906601</td>\n",
              "      <td>0.621300</td>\n",
              "      <td>0.656529</td>\n",
              "      <td>1.096136</td>\n",
              "      <td>1.312054e-06</td>\n",
              "      <td>0.046843</td>\n",
              "      <td>0.886311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>STAGE2_REFINE</td>\n",
              "      <td>21</td>\n",
              "      <td>0.024696</td>\n",
              "      <td>0.015467</td>\n",
              "      <td>0.65</td>\n",
              "      <td>19</td>\n",
              "      <td>0.016717</td>\n",
              "      <td>0.016717</td>\n",
              "      <td>0.028085</td>\n",
              "      <td>5</td>\n",
              "      <td>0.139619</td>\n",
              "      <td>17.228054</td>\n",
              "      <td>0.643756</td>\n",
              "      <td>0.532423</td>\n",
              "      <td>1.559824</td>\n",
              "      <td>1.873891e-04</td>\n",
              "      <td>0.044638</td>\n",
              "      <td>0.871253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2306a90a-71f7-4d90-b921-7497dec46d6e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2306a90a-71f7-4d90-b921-7497dec46d6e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2306a90a-71f7-4d90-b921-7497dec46d6e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"[OK] BLOCK 24 complete\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"stage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"STAGE2_REFINE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trial\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 59,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_sc_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4759855101819782e-05,\n        \"min\": 0.02465207104058234,\n        \"max\": 0.02469640147622813,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.024692463134802837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_sc_wmae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.3521688085120415e-05,\n        \"min\": 0.01542615916983491,\n        \"max\": 0.015466773813819177,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.015463181091207902\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_sc_diracc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1702778228589004e-16,\n        \"min\": 0.65,\n        \"max\": 0.65,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.65\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_iteration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 5,\n        \"max\": 20,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_score_rmse_eval_es\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5951336040100967e-06,\n        \"min\": 0.01670588174060792,\n        \"max\": 0.01671667412374373,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.01671568980986268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_es_wrmse_explicit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.595240079992069e-06,\n        \"min\": 0.016705881410127195,\n        \"max\": 0.016716674310766927,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.016715690124147798\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"elapsed_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0020264981803315976,\n        \"min\": 0.023681163787841797,\n        \"max\": 0.030083179473876953,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.030083179473876953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_depth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010974561439514795,\n        \"min\": 0.11974333346817766,\n        \"max\": 0.15,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_child_weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.2176273401113455,\n        \"min\": 8.906601242313926,\n        \"max\": 25.162531642492063,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8.906601242313926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subsample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03434043198938799,\n        \"min\": 0.5378435696535413,\n        \"max\": 0.6437556342167994,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6213002985413741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"colsample_bytree\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05489053403071527,\n        \"min\": 0.5324233249683995,\n        \"max\": 0.6902209952359977,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6565288253462819\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gamma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2798938203599182,\n        \"min\": 1.096135651174638,\n        \"max\": 2.0490812139631616,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.096135651174638\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reg_alpha\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.497223475409306e-05,\n        \"min\": 5.31451337689063e-07,\n        \"max\": 0.00018738911742019986,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.3120535745716797e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reg_lambda\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013887180202025336,\n        \"min\": 0.00674259509665434,\n        \"max\": 0.04684297554116256,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.04684297554116256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_delta_step\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11626918229338654,\n        \"min\": 0.6729596316106273,\n        \"max\": 1.0495151221822316,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.8863112729317355\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] BEST summary:\n",
            "valid_mode              VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2...\n",
            "n_features                                                             10\n",
            "n_trials_total                                                        210\n",
            "best_valid_sc_wrmse                                              0.024652\n",
            "best_valid_sc_wmae                                               0.015426\n",
            "best_valid_sc_diracc                                                 0.65\n",
            "best_iteration                                                         20\n",
            "dtype: object\n",
            "\n",
            "[INFO] BEST params:\n",
            "{'max_depth': 5, 'learning_rate': 0.15, 'min_child_weight': 25.162531642492063, 'subsample': 0.6133296092171685, 'colsample_bytree': 0.6570143998957291, 'gamma': 2.0490812139631616, 'reg_alpha': 0.00017483444490510667, 'reg_lambda': 0.00674259509665434, 'max_delta_step': 0.7068169911285699}\n",
            "  - best_params_xgb_reg_t1.pkl (persistent)\n",
            "\n",
            "[OK] Saved HPO outputs:\n",
            "  - best_model_xgb_reg_t1.json\n",
            "  - best_params_xgb_reg_t1.txt\n",
            "  - best_params_xgb_reg_t1.pkl\n",
            "[OK] BLOCK 24 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Config\n",
        "HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
        "\n",
        "# Load selected features from BLOCK 23 (with fallback)\n",
        "FS_DIR_LOCAL = Path(LOCAL_PATHS[\"fs_dir\"])\n",
        "FS_DIR_DRIVE = Path(DRIVE_PATHS[\"fs_dir\"])\n",
        "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "selected_features = load_with_fallback(\n",
        "    \"selected_features_xgb.pkl\",\n",
        "    FS_DIR_LOCAL,           # 1. runs/RUN_ID/feature_selection (LOCAL)\n",
        "    FALLBACK_PROC_LOCAL,    # 2. data/processed (LOCAL)\n",
        "    FS_DIR_DRIVE,           # 3. runs/RUN_ID/feature_selection (DRIVE)\n",
        "    FALLBACK_PROC_DRIVE     # 4. data/processed (DRIVE)\n",
        ")\n",
        "print(f\"[INFO] Loaded {len(selected_features)} selected features\")\n",
        "# Directories\n",
        "\n",
        "# --- Fallback Logic: Try RUN_ID first, then data/processed ---\n",
        "def get_data_path(filename, run_local, fallback_local, run_drive=None, fallback_drive=None):\n",
        "    \"\"\"Return first existing path from: RUN_LOCAL -> FALLBACK_LOCAL -> RUN_DRIVE -> FALLBACK_DRIVE.\"\"\"\n",
        "    paths = [\n",
        "        (Path(run_local) / filename, \"RUN_ID (LOCAL)\"),\n",
        "        (Path(fallback_local) / filename, \"data/processed (LOCAL)\"),\n",
        "    ]\n",
        "    if run_drive:\n",
        "        paths.append((Path(run_drive) / filename, \"RUN_ID (DRIVE)\"))\n",
        "    if fallback_drive:\n",
        "        paths.append((Path(fallback_drive) / filename, \"data/processed (DRIVE)\"))\n",
        "\n",
        "    for path, src in paths:\n",
        "        if path.exists():\n",
        "            print(f\"  [LOAD] {filename} <- {src}\")\n",
        "            return path\n",
        "\n",
        "    raise FileNotFoundError(f\"{filename} not found in any location\")\n",
        "\n",
        "RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
        "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
        "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "print(\"[INFO] Data loading with fallback enabled\")\n",
        "\n",
        "MS_OUT_LOCAL = ensure_dir(Path(MS_DIR))\n",
        "MS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"ms_dir\"]))\n",
        "\n",
        "print(\"[INFO] HPO output dirs:\")\n",
        "print(\"  - LOCAL:\", MS_OUT_LOCAL)\n",
        "print(\"  - DRIVE:\", MS_OUT_DRIVE)\n",
        "\n",
        "# -------------------------\n",
        "# Load splits from data/processed/ (TRAIN + VALID only)\n",
        "# -------------------------\n",
        "print(\"[INFO] Loading splits with fallback logic...\")\n",
        "\n",
        "X_train_sel = pd.read_pickle(get_data_path(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
        "X_valid_sel = pd.read_pickle(get_data_path(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
        "\n",
        "y_train_t1 = load_pickle(get_data_path(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
        "y_valid_t1 = load_pickle(get_data_path(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
        "\n",
        "w_train = load_pickle(get_data_path(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
        "w_valid = load_pickle(get_data_path(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
        "\n",
        "print(f\"[INFO] Loaded: X_train_sel={X_train_sel.shape} | X_valid_sel={X_valid_sel.shape}\")\n",
        "\n",
        "y_train = y_train_t1.astype(float).copy()\n",
        "y_valid = y_valid_t1.astype(float).copy()\n",
        "\n",
        "w_train_arr = np.asarray(w_train, dtype=float)\n",
        "w_valid_arr = np.asarray(w_valid, dtype=float)\n",
        "\n",
        "print(\"[INFO] Shapes (selected):\", X_train_sel.shape, X_valid_sel.shape)\n",
        "\n",
        "# -------------------------\n",
        "# Align VALID data with Neural Networks (skip first lookback-1 rows)\n",
        "# -------------------------\n",
        "LOOKBACK = int(HPO_CFG[\"lookback\"])\n",
        "SKIP_ROWS = LOOKBACK - 1\n",
        "\n",
        "X_valid_sel = X_valid_sel.iloc[SKIP_ROWS:]\n",
        "y_valid = y_valid.iloc[SKIP_ROWS:]\n",
        "w_valid_arr = w_valid_arr[SKIP_ROWS:]\n",
        "\n",
        "print(f\"[INFO] Aligned VALID with lookback={LOOKBACK}: skipped first {SKIP_ROWS} rows\")\n",
        "print(f\"[INFO] VALID shape after alignment: {X_valid_sel.shape}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# VALID split for ES vs SCORE (date-based)\n",
        "# -------------------------\n",
        "VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
        "VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
        "VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
        "VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
        "\n",
        "\n",
        "def split_valid_for_es_and_score(Xv: pd.DataFrame, yv: pd.Series, wv: np.ndarray,\n",
        "                                  valid_es_start: str, valid_es_end: str,\n",
        "                                  valid_score_start: str, valid_score_end: str):\n",
        "    \"\"\"Split validation into ES (early stopping) and SCORE (model selection) sets.\"\"\"\n",
        "    if not isinstance(Xv.index, pd.DatetimeIndex):\n",
        "        return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "    es_start = pd.Timestamp(valid_es_start)\n",
        "    es_end = pd.Timestamp(valid_es_end)\n",
        "    sc_start = pd.Timestamp(valid_score_start)\n",
        "    sc_end = pd.Timestamp(valid_score_end)\n",
        "\n",
        "    mask_es = (Xv.index >= es_start) & (Xv.index <= es_end)\n",
        "    mask_sc = (Xv.index >= sc_start) & (Xv.index <= sc_end)\n",
        "\n",
        "    X_es = Xv.loc[mask_es]\n",
        "    y_es = yv.loc[mask_es]\n",
        "    X_sc = Xv.loc[mask_sc]\n",
        "    y_sc = yv.loc[mask_sc]\n",
        "\n",
        "    wv_s = pd.Series(wv, index=Xv.index)\n",
        "    w_es = wv_s.loc[mask_es].to_numpy(dtype=float)\n",
        "    w_sc = wv_s.loc[mask_sc].to_numpy(dtype=float)\n",
        "\n",
        "    mode_str = f\"VALID_ES={valid_es_start}:{valid_es_end} / VALID_SCORE={valid_score_start}:{valid_score_end}\"\n",
        "\n",
        "    if len(X_es) > 0 and len(X_sc) > 0:\n",
        "        return (X_es, y_es, w_es), (X_sc, y_sc, w_sc), mode_str\n",
        "\n",
        "    return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "\n",
        "(X_valid_es, y_valid_es, w_valid_es), (X_valid_sc, y_valid_sc, w_valid_sc), valid_mode = split_valid_for_es_and_score(\n",
        "    X_valid_sel, y_valid, w_valid_arr,\n",
        "    VALID_ES_START, VALID_ES_END, VALID_SCORE_START, VALID_SCORE_END\n",
        ")\n",
        "\n",
        "print(\"[INFO] VALID mode:\", valid_mode)\n",
        "print(\"[INFO] VALID_ES shape:\", X_valid_es.shape, \"| VALID_SCORE shape:\", X_valid_sc.shape)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Weighted metrics\n",
        "# -------------------------\n",
        "# Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Search configuration from RUN_PARAMS\n",
        "# -------------------------\n",
        "RANDOM_SEED = int(HPO_CFG[\"random_state\"])\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "N_ESTIMATORS = int(HPO_CFG[\"n_estimators\"])\n",
        "EARLY_STOP = int(HPO_CFG[\"early_stopping_rounds\"])\n",
        "\n",
        "N_TRIALS_STAGE1 = int(HPO_CFG[\"n_trials_stage1\"])\n",
        "N_TRIALS_STAGE2 = int(HPO_CFG[\"n_trials_stage2\"])\n",
        "N_TRIALS_STAGE2_LOWLR = int(HPO_CFG[\"n_trials_stage2_lowlr\"])\n",
        "\n",
        "PRINT_EVERY_STAGE1 = int(HPO_CFG[\"print_every_stage1\"])\n",
        "PRINT_EVERY_STAGE2 = int(HPO_CFG[\"print_every_stage2\"])\n",
        "\n",
        "TIE_TOL = float(HPO_CFG[\"tie_tol\"])\n",
        "\n",
        "BASE_MODEL_CFG = dict(\n",
        "    n_estimators=N_ESTIMATORS,\n",
        "    objective=HPO_CFG[\"objective\"],\n",
        "    eval_metric=HPO_CFG[\"eval_metric\"],\n",
        "    tree_method=HPO_CFG[\"tree_method\"],\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1,\n",
        "    verbosity=0,\n",
        "    early_stopping_rounds=EARLY_STOP,\n",
        ")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Parameter sampling (from RUN_PARAMS[\"hpo\"][\"sampling\"])\n",
        "# -------------------------\n",
        "SAMP_CFG = HPO_CFG[\"sampling\"]\n",
        "BROAD_CFG = SAMP_CFG[\"broad\"]\n",
        "REFINE_CFG = SAMP_CFG[\"refine\"]\n",
        "LOWLR_CFG = SAMP_CFG[\"refine_low_lr\"]\n",
        "\n",
        "\n",
        "def _clip(v, lo, hi) -> float:\n",
        "    return float(min(max(float(v), lo), hi))\n",
        "\n",
        "\n",
        "def _log_uniform(rng, lo_exp, hi_exp) -> float:\n",
        "    return float(10 ** rng.uniform(lo_exp, hi_exp))\n",
        "\n",
        "\n",
        "def sample_broad(rng, broad_cfg):\n",
        "    \"\"\"Broad sampling biased toward lower learning_rate for stability.\"\"\"\n",
        "    lr_high_prob = broad_cfg[\"lr_high_prob\"]\n",
        "    if rng.random() < (1 - lr_high_prob):\n",
        "        lr_lo, lr_hi = broad_cfg[\"lr_low\"]\n",
        "        lr = float(np.exp(rng.uniform(np.log(lr_lo), np.log(lr_hi))))\n",
        "    else:\n",
        "        lr_lo, lr_hi = broad_cfg[\"lr_high\"]\n",
        "        lr = float(rng.uniform(lr_lo, lr_hi))\n",
        "\n",
        "    md_lo, md_hi = broad_cfg[\"max_depth\"]\n",
        "    mcw_lo, mcw_hi = broad_cfg[\"min_child_weight_log\"]\n",
        "    ss_lo, ss_hi = broad_cfg[\"subsample\"]\n",
        "    cs_lo, cs_hi = broad_cfg[\"colsample_bytree\"]\n",
        "    gm_lo, gm_hi = broad_cfg[\"gamma\"]\n",
        "    ra_lo, ra_hi = broad_cfg[\"reg_alpha_exp\"]\n",
        "    rl_lo, rl_hi = broad_cfg[\"reg_lambda_exp\"]\n",
        "    mds_lo, mds_hi = broad_cfg[\"max_delta_step\"]\n",
        "\n",
        "    return {\n",
        "        \"max_depth\": int(rng.integers(md_lo, md_hi)),\n",
        "        \"learning_rate\": lr,\n",
        "        \"min_child_weight\": float(np.exp(rng.uniform(np.log(mcw_lo), np.log(mcw_hi)))),\n",
        "        \"subsample\": float(rng.uniform(ss_lo, ss_hi)),\n",
        "        \"colsample_bytree\": float(rng.uniform(cs_lo, cs_hi)),\n",
        "        \"gamma\": float(rng.uniform(gm_lo, gm_hi)),\n",
        "        \"reg_alpha\": _log_uniform(rng, ra_lo, ra_hi),\n",
        "        \"reg_lambda\": _log_uniform(rng, rl_lo, rl_hi),\n",
        "        \"max_delta_step\": float(rng.uniform(mds_lo, mds_hi)),\n",
        "    }\n",
        "\n",
        "\n",
        "def _log_jitter(rng, v, sigma=0.6, lo_exp=-12, hi_exp=2):\n",
        "    v = float(max(v, 1e-12))\n",
        "    logv = np.log10(v) + rng.normal(0.0, sigma)\n",
        "    logv = float(np.clip(logv, lo_exp, hi_exp))\n",
        "    return float(10 ** logv)\n",
        "\n",
        "\n",
        "def sample_refine(rng, best, refine_cfg):\n",
        "    \"\"\"Refine around best parameters.\"\"\"\n",
        "    md_delta = refine_cfg[\"max_depth_delta\"]\n",
        "    md_clip = refine_cfg[\"max_depth_clip\"]\n",
        "    lr_sigma = refine_cfg[\"lr_sigma\"]\n",
        "    lr_clip = refine_cfg[\"lr_clip\"]\n",
        "    mcw_sigma = refine_cfg[\"min_child_weight_sigma\"]\n",
        "    mcw_clip = refine_cfg[\"min_child_weight_clip\"]\n",
        "    ss_sigma = refine_cfg[\"subsample_sigma\"]\n",
        "    ss_clip = refine_cfg[\"subsample_clip\"]\n",
        "    cs_sigma = refine_cfg[\"colsample_sigma\"]\n",
        "    cs_clip = refine_cfg[\"colsample_clip\"]\n",
        "    gm_sigma = refine_cfg[\"gamma_sigma\"]\n",
        "    gm_clip = refine_cfg[\"gamma_clip\"]\n",
        "    ra_sigma = refine_cfg[\"reg_alpha_sigma\"]\n",
        "    ra_clip = refine_cfg[\"reg_alpha_exp_clip\"]\n",
        "    rl_sigma = refine_cfg[\"reg_lambda_sigma\"]\n",
        "    rl_clip = refine_cfg[\"reg_lambda_exp_clip\"]\n",
        "    mds_sigma = refine_cfg[\"max_delta_step_sigma\"]\n",
        "    mds_clip = refine_cfg[\"max_delta_step_clip\"]\n",
        "\n",
        "    return {\n",
        "        \"max_depth\": int(np.clip(int(best[\"max_depth\"] + rng.integers(md_delta[0], md_delta[1])), md_clip[0], md_clip[1])),\n",
        "        \"learning_rate\": _clip(best[\"learning_rate\"] * float(np.exp(rng.normal(0.0, lr_sigma))), lr_clip[0], lr_clip[1]),\n",
        "        \"min_child_weight\": _clip(best[\"min_child_weight\"] * float(np.exp(rng.normal(0.0, mcw_sigma))), mcw_clip[0], mcw_clip[1]),\n",
        "        \"subsample\": _clip(best[\"subsample\"] + rng.normal(0.0, ss_sigma), ss_clip[0], ss_clip[1]),\n",
        "        \"colsample_bytree\": _clip(best[\"colsample_bytree\"] + rng.normal(0.0, cs_sigma), cs_clip[0], cs_clip[1]),\n",
        "        \"gamma\": _clip(best[\"gamma\"] + rng.normal(0.0, gm_sigma), gm_clip[0], gm_clip[1]),\n",
        "        \"reg_alpha\": _log_jitter(rng, best[\"reg_alpha\"], sigma=ra_sigma, lo_exp=ra_clip[0], hi_exp=ra_clip[1]),\n",
        "        \"reg_lambda\": _log_jitter(rng, best[\"reg_lambda\"], sigma=rl_sigma, lo_exp=rl_clip[0], hi_exp=rl_clip[1]),\n",
        "        \"max_delta_step\": _clip(best[\"max_delta_step\"] + rng.normal(0.0, mds_sigma), mds_clip[0], mds_clip[1]),\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_refine_low_lr(rng, best, refine_cfg, lowlr_cfg):\n",
        "    \"\"\"Refine with lower learning rate for stability.\"\"\"\n",
        "    lr_shift = lowlr_cfg[\"lr_shift\"]\n",
        "    lr_clip = lowlr_cfg[\"lr_clip\"]\n",
        "    lr_sigma = refine_cfg[\"lr_sigma\"]\n",
        "    low_lr = _clip(best[\"learning_rate\"] * float(np.exp(rng.normal(lr_shift, lr_sigma))), lr_clip[0], lr_clip[1])\n",
        "    params = sample_refine(rng, best, refine_cfg)\n",
        "    params[\"learning_rate\"] = low_lr\n",
        "    return params\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Single trial runner\n",
        "# -------------------------\n",
        "def run_trial(trial_id: int, stage: str, params: dict):\n",
        "    \"\"\"Run a single HPO trial.\"\"\"\n",
        "    model = xgb.XGBRegressor(**BASE_MODEL_CFG, **params)\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.fit(\n",
        "        X_train_sel, y_train,\n",
        "        sample_weight=w_train_arr,\n",
        "        eval_set=[(X_valid_es, y_valid_es)],\n",
        "        sample_weight_eval_set=[w_valid_es],\n",
        "        verbose=False\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    best_iter = getattr(model, \"best_iteration\", None)\n",
        "    best_score = getattr(model, \"best_score\", None)\n",
        "\n",
        "    pred_es = model.predict(X_valid_es)\n",
        "    es_wrmse = w_rmse(y_valid_es, pred_es, w_valid_es)\n",
        "\n",
        "    pred_sc = model.predict(X_valid_sc)\n",
        "    sc_wrmse = w_rmse(y_valid_sc, pred_sc, w_valid_sc)\n",
        "\n",
        "    row = {\n",
        "        \"stage\": stage,\n",
        "        \"trial\": int(trial_id),\n",
        "        \"valid_sc_wrmse\": sc_wrmse,\n",
        "        \"valid_sc_wmae\": w_mae(y_valid_sc, pred_sc, w_valid_sc),\n",
        "        \"valid_sc_diracc\": dir_acc(y_valid_sc, pred_sc),\n",
        "        \"best_iteration\": None if best_iter is None else int(best_iter),\n",
        "        \"best_score_rmse_eval_es\": None if best_score is None else float(best_score),\n",
        "        \"valid_es_wrmse_explicit\": float(es_wrmse),\n",
        "        \"elapsed_sec\": float(elapsed),\n",
        "        **params,\n",
        "    }\n",
        "    return model, row\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# HPO loop (Stage 1 + Stage 2 + Stage 2 LOW-LR)\n",
        "# -------------------------\n",
        "results = []\n",
        "best_model = None\n",
        "best_row = None\n",
        "best_valid = np.inf\n",
        "\n",
        "best_params_keys = [\n",
        "    \"max_depth\", \"learning_rate\", \"min_child_weight\", \"subsample\", \"colsample_bytree\",\n",
        "    \"gamma\", \"reg_alpha\", \"reg_lambda\", \"max_delta_step\"\n",
        "]\n",
        "\n",
        "\n",
        "def is_better(row, best_row, tie_tol):\n",
        "    \"\"\"Check if row is better than best_row (lower wRMSE, tie-break by higher iteration).\"\"\"\n",
        "    if best_row is None:\n",
        "        return True\n",
        "\n",
        "    a = float(row[\"valid_sc_wrmse\"])\n",
        "    b = float(best_row[\"valid_sc_wrmse\"])\n",
        "\n",
        "    if a < (b - tie_tol):\n",
        "        return True\n",
        "    if abs(a - b) <= tie_tol:\n",
        "        ai = -1 if row[\"best_iteration\"] is None else int(row[\"best_iteration\"])\n",
        "        bi = -1 if best_row[\"best_iteration\"] is None else int(best_row[\"best_iteration\"])\n",
        "        return ai > bi\n",
        "    return False\n",
        "\n",
        "\n",
        "print(f\"\\n[INFO] HPO start | VALID mode: {valid_mode}\")\n",
        "print(f\"[INFO] Stage1 trials={N_TRIALS_STAGE1} | Stage2 trials={N_TRIALS_STAGE2} | Stage2 LOW-LR={N_TRIALS_STAGE2_LOWLR}\")\n",
        "print(f\"[INFO] n_estimators={N_ESTIMATORS} | early_stop={EARLY_STOP}\")\n",
        "\n",
        "# Stage 1 (broad)\n",
        "for i in range(N_TRIALS_STAGE1):\n",
        "    params = sample_broad(rng, BROAD_CFG)\n",
        "    model_i, row_i = run_trial(trial_id=i, stage=\"STAGE1_BROAD\", params=params)\n",
        "    results.append(row_i)\n",
        "\n",
        "    if is_better(row_i, best_row, TIE_TOL):\n",
        "        best_model = model_i\n",
        "        best_row = row_i\n",
        "        best_valid = float(best_row[\"valid_sc_wrmse\"])\n",
        "\n",
        "    if (i + 1) % PRINT_EVERY_STAGE1 == 0 or i == 0:\n",
        "        print(f\"[INFO] S1 {i:04d} | sc_wrmse={row_i['valid_sc_wrmse']:.6f} | best={best_valid:.6f} | best_iter={row_i['best_iteration']}\")\n",
        "\n",
        "best_params_stage1 = {k: best_row[k] for k in best_params_keys}\n",
        "\n",
        "# Stage 2 (refine around best)\n",
        "for j in range(N_TRIALS_STAGE2):\n",
        "    params = sample_refine(rng, best_params_stage1, REFINE_CFG)\n",
        "    model_j, row_j = run_trial(trial_id=j, stage=\"STAGE2_REFINE\", params=params)\n",
        "    results.append(row_j)\n",
        "\n",
        "    if is_better(row_j, best_row, TIE_TOL):\n",
        "        best_model = model_j\n",
        "        best_row = row_j\n",
        "        best_valid = float(best_row[\"valid_sc_wrmse\"])\n",
        "\n",
        "    if (j + 1) % PRINT_EVERY_STAGE2 == 0 or j == 0:\n",
        "        print(f\"[INFO] S2 {j:04d} | sc_wrmse={row_j['valid_sc_wrmse']:.6f} | best={best_valid:.6f} | best_iter={row_j['best_iteration']}\")\n",
        "\n",
        "# Stage 2B (LOW-LR refine branch)\n",
        "for k in range(N_TRIALS_STAGE2_LOWLR):\n",
        "    params = sample_refine_low_lr(rng, best_params_stage1, REFINE_CFG, LOWLR_CFG)\n",
        "    model_k, row_k = run_trial(trial_id=k, stage=\"STAGE2_LOWLR\", params=params)\n",
        "    results.append(row_k)\n",
        "\n",
        "    if is_better(row_k, best_row, TIE_TOL):\n",
        "        best_model = model_k\n",
        "        best_row = row_k\n",
        "        best_valid = float(best_row[\"valid_sc_wrmse\"])\n",
        "\n",
        "    if (k + 1) % PRINT_EVERY_STAGE2 == 0 or k == 0:\n",
        "        print(f\"[INFO] S2L {k:04d} | sc_wrmse={row_k['valid_sc_wrmse']:.6f} | best={best_valid:.6f} | best_iter={row_k['best_iteration']}\")\n",
        "\n",
        "res_df = pd.DataFrame(results).sort_values(\"valid_sc_wrmse\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n[INFO] Top 10 trials by VALID_SCORE wRMSE:\")\n",
        "display(res_df.head(10))\n",
        "\n",
        "print(\"\\n[INFO] BEST summary:\")\n",
        "best_summary = {\n",
        "    \"valid_mode\": valid_mode,\n",
        "    \"n_features\": int(len(selected_features)),\n",
        "    \"n_trials_total\": int(len(res_df)),\n",
        "    \"best_valid_sc_wrmse\": float(best_row[\"valid_sc_wrmse\"]),\n",
        "    \"best_valid_sc_wmae\": float(best_row[\"valid_sc_wmae\"]),\n",
        "    \"best_valid_sc_diracc\": float(best_row[\"valid_sc_diracc\"]),\n",
        "    \"best_iteration\": best_row[\"best_iteration\"],\n",
        "}\n",
        "print(pd.Series(best_summary))\n",
        "\n",
        "best_params = {k: best_row[k] for k in best_params_keys}\n",
        "best_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n",
        "for k in best_params_keys:\n",
        "    if k != \"max_depth\":\n",
        "        best_params[k] = float(best_params[k])\n",
        "\n",
        "print(\"\\n[INFO] BEST params:\")\n",
        "print(best_params)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Save outputs (LOCAL + DRIVE)\n",
        "# -------------------------\n",
        "BEST_MODEL_PATH = MS_OUT_LOCAL / \"best_model_xgb_reg_t1.json\"\n",
        "BEST_PARAMS_TXT = MS_OUT_LOCAL / \"best_params_xgb_reg_t1.txt\"\n",
        "BEST_PARAMS_PKL = MS_OUT_LOCAL / \"best_params_xgb_reg_t1.pkl\"\n",
        "\n",
        "# Save model\n",
        "best_model.get_booster().save_model(str(BEST_MODEL_PATH))\n",
        "copy_file(BEST_MODEL_PATH, MS_OUT_DRIVE / BEST_MODEL_PATH.name)\n",
        "\n",
        "# Save params as text\n",
        "lines = [\n",
        "    f\"valid_mode={valid_mode}\",\n",
        "    f\"best_valid_sc_wrmse={best_row['valid_sc_wrmse']}\",\n",
        "    f\"best_valid_sc_wmae={best_row['valid_sc_wmae']}\",\n",
        "    f\"best_valid_sc_diracc={best_row['valid_sc_diracc']}\",\n",
        "    f\"best_iteration={best_row['best_iteration']}\",\n",
        "    f\"n_estimators={N_ESTIMATORS}\",\n",
        "    f\"early_stop={EARLY_STOP}\",\n",
        "    f\"trials_stage1={N_TRIALS_STAGE1}\",\n",
        "    f\"trials_stage2={N_TRIALS_STAGE2}\",\n",
        "    f\"trials_stage2_lowlr={N_TRIALS_STAGE2_LOWLR}\",\n",
        "    f\"random_seed={RANDOM_SEED}\",\n",
        "]\n",
        "for k in best_params_keys:\n",
        "    lines.append(f\"{k}={best_params[k]}\")\n",
        "\n",
        "BEST_PARAMS_TXT.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "copy_file(BEST_PARAMS_TXT, MS_OUT_DRIVE / BEST_PARAMS_TXT.name)\n",
        "\n",
        "# Save params as pickle\n",
        "save_pickle(best_params, BEST_PARAMS_PKL)\n",
        "copy_file(BEST_PARAMS_PKL, MS_OUT_DRIVE / BEST_PARAMS_PKL.name)\n",
        "\n",
        "# Save ALSO to persistent location (for future runs without HPO)\n",
        "save_pickle(best_params, DATA_DIRS_LOCAL[\"processed\"] / \"best_params_xgb_reg_t1.pkl\")\n",
        "copy_file(DATA_DIRS_LOCAL[\"processed\"] / \"best_params_xgb_reg_t1.pkl\",\n",
        "          DATA_DIRS_DRIVE[\"processed\"] / \"best_params_xgb_reg_t1.pkl\")\n",
        "print(\"  - best_params_xgb_reg_t1.pkl (persistent)\")\n",
        "\n",
        "print(\"\\n[OK] Saved HPO outputs:\")\n",
        "print(\"  -\", BEST_MODEL_PATH.name)\n",
        "print(\"  -\", BEST_PARAMS_TXT.name)\n",
        "print(\"  -\", BEST_PARAMS_PKL.name)\n",
        "\n",
        "print(\"[OK] BLOCK 24 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DPSKIKtJSAXJ",
      "metadata": {
        "id": "DPSKIKtJSAXJ"
      },
      "source": [
        "---\n",
        "# SECTION 7: XGBoost Final Model\n",
        "\n",
        "**Train and evaluate best XGBoost model**\n",
        "\n",
        "**Block:** 25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd3a08b4",
      "metadata": {
        "id": "dd3a08b4"
      },
      "source": [
        "## BLOCK 25 â€” FINAL MODEL TRAIN + BASELINE COMPARISON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "2ed32fcb",
      "metadata": {
        "id": "2ed32fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19449c98-8f43-4bd1-cb4e-eb8938918786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Final Model output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/models\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/models\n",
            "\n",
            "[INFO] Loading data (RUN_ID -> data/processed fallback):\n",
            "  [LOAD] X_train_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_test.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_test.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] best_params_xgb_reg_t1.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: X_train_sel=(347, 10) | X_valid_sel=(77, 10) | X_test_sel=(87, 10)\n",
            "[INFO] Using xgb_selected features (10 features)\n",
            "[INFO] Aligned VALID+TEST with lookback=7: skipped first 6 rows\n",
            "[INFO] VALID shapes after alignment: X=(71, 10)\n",
            "[INFO] TEST shapes after alignment: X=(81, 10)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] VALID_ES: (31, 10) | VALID_SCORE: (40, 10)\n",
            "\n",
            "[INFO] Computing BASELINES...\n",
            "  [1] BASELINE_ZERO (predict 0):\n",
            "      VALID_SCORE: wRMSE=0.025004 | DirAcc=0.3500\n",
            "      TEST: wRMSE=0.019984 | DirAcc=0.4444\n",
            "  [2] BASELINE_NAIVE (last-value forecast):\n",
            "      VALID_SCORE: wRMSE=0.031440 | DirAcc=0.4250\n",
            "      TEST: wRMSE=0.025313 | DirAcc=0.5062\n",
            "\n",
            "[INFO] BASELINE Summary:\n",
            "  - BASELINE_ZERO   | VALID_SCORE  | wRMSE=0.025004 | DirAcc=0.3500\n",
            "  - BASELINE_ZERO   | TEST         | wRMSE=0.019984 | DirAcc=0.4444\n",
            "  - BASELINE_NAIVE  | VALID_SCORE  | wRMSE=0.031440 | DirAcc=0.4250\n",
            "  - BASELINE_NAIVE  | TEST         | wRMSE=0.025313 | DirAcc=0.5062\n",
            "\n",
            "[INFO] Training FINAL MODEL...\n",
            "[INFO] Training complete. best_iteration=20 | best_score(ES)=0.01670588174060792\n",
            "\n",
            "[INFO] Evaluating FINAL MODEL...\n",
            "[INFO] FINAL MODEL results:\n",
            "  - VALID_SCORE: wRMSE=0.024652 | DirAcc=0.6500\n",
            "  - TEST: wRMSE=0.019788 | DirAcc=0.5556\n",
            "\n",
            "[INFO] BASELINE vs FINAL MODEL comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            model        split   n     wRMSE      wMAE    DirAcc  \\\n",
              "0   BASELINE_ZERO  VALID_SCORE  40  0.025004  0.015738  0.350000   \n",
              "1   BASELINE_ZERO         TEST  81  0.019984  0.015729  0.444444   \n",
              "2  BASELINE_NAIVE  VALID_SCORE  40  0.031440  0.021985  0.425000   \n",
              "3  BASELINE_NAIVE         TEST  81  0.025313  0.020068  0.506173   \n",
              "4       FINAL_XGB  VALID_SCORE  40  0.024652  0.015426  0.650000   \n",
              "5       FINAL_XGB         TEST  81  0.019788  0.015597  0.555556   \n",
              "\n",
              "   wRMSE_vs_zero  wRMSE_vs_naive  \n",
              "0       0.000000        0.006437  \n",
              "1       0.000000        0.005329  \n",
              "2      -0.006437        0.000000  \n",
              "3      -0.005329        0.000000  \n",
              "4       0.000351        0.006788  \n",
              "5       0.000195        0.005525  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d5dc9c5-8516-4979-a307-6a538ba5e5fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>split</th>\n",
              "      <th>n</th>\n",
              "      <th>wRMSE</th>\n",
              "      <th>wMAE</th>\n",
              "      <th>DirAcc</th>\n",
              "      <th>wRMSE_vs_zero</th>\n",
              "      <th>wRMSE_vs_naive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BASELINE_ZERO</td>\n",
              "      <td>VALID_SCORE</td>\n",
              "      <td>40</td>\n",
              "      <td>0.025004</td>\n",
              "      <td>0.015738</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BASELINE_ZERO</td>\n",
              "      <td>TEST</td>\n",
              "      <td>81</td>\n",
              "      <td>0.019984</td>\n",
              "      <td>0.015729</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BASELINE_NAIVE</td>\n",
              "      <td>VALID_SCORE</td>\n",
              "      <td>40</td>\n",
              "      <td>0.031440</td>\n",
              "      <td>0.021985</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>-0.006437</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BASELINE_NAIVE</td>\n",
              "      <td>TEST</td>\n",
              "      <td>81</td>\n",
              "      <td>0.025313</td>\n",
              "      <td>0.020068</td>\n",
              "      <td>0.506173</td>\n",
              "      <td>-0.005329</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FINAL_XGB</td>\n",
              "      <td>VALID_SCORE</td>\n",
              "      <td>40</td>\n",
              "      <td>0.024652</td>\n",
              "      <td>0.015426</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.006788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>FINAL_XGB</td>\n",
              "      <td>TEST</td>\n",
              "      <td>81</td>\n",
              "      <td>0.019788</td>\n",
              "      <td>0.015597</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.005525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d5dc9c5-8516-4979-a307-6a538ba5e5fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d5dc9c5-8516-4979-a307-6a538ba5e5fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d5dc9c5-8516-4979-a307-6a538ba5e5fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_35ead514-d817-4963-bf56-6d0d27b79d70\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_35ead514-d817-4963-bf56-6d0d27b79d70 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BASELINE_ZERO\",\n          \"BASELINE_NAIVE\",\n          \"FINAL_XGB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TEST\",\n          \"VALID_SCORE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22,\n        \"min\": 40,\n        \"max\": 81,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          81,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wRMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0042798445169841175,\n        \"min\": 0.019788430803363736,\n        \"max\": 0.03144003541115706,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.025003501413126943,\n          0.019983748548673245\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wMAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0028580246506850356,\n        \"min\": 0.01542615916983491,\n        \"max\": 0.021985245302051933,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.01573836841257764,\n          0.015729483115759623\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DirAcc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1058739002620268,\n        \"min\": 0.35,\n        \"max\": 0.65,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.35,\n          0.4444444444444444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wRMSE_vs_zero\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003130944140919263,\n        \"min\": -0.006436533998030115,\n        \"max\": 0.0003514303725446026,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.006436533998030115,\n          0.00019531774530950896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wRMSE_vs_naive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0031559435070606053,\n        \"min\": 0.0,\n        \"max\": 0.006787964370574717,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0053292966284727294,\n          0.005524614373782238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] Saved FINAL MODEL artifacts:\n",
            "  - final_model_xgb.json\n",
            "  - final_metrics.csv\n",
            "  - predictions/xgb/ predictions_valid.csv\n",
            "  - predictions/xgb/ predictions_test.csv\n",
            "[INFO] Tomorrow prediction: 0.000854 (0.0854%)\n",
            "[OK] Saved: predictions/xgb/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "[INFO] Computing SHAP values...\n",
            "[INFO] Top 10 features by SHAP importance:\n",
            "            feature  mean_abs_shap\n",
            "          GOOGL_Low            0.0\n",
            "         MSFT_Close            0.0\n",
            "        MSFT_Volume            0.0\n",
            "         SPY_Volume            0.0\n",
            "          QQQ_Close            0.0\n",
            "         ^VIX_Close            0.0\n",
            "          ^TNX_Open            0.0\n",
            "         XLK_Volume            0.0\n",
            "        ^GDAXI_High            0.0\n",
            "EU_break_close_flag            0.0\n",
            "[OK] Saved SHAP analysis:\n",
            "  - shap_values_test.csv\n",
            "  - shap_feature_importance.csv\n",
            "  - shap_summary_bar.png\n",
            "  - shap_summary_beeswarm.png\n",
            "[OK] BLOCK 25 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Config\n",
        "HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
        "\n",
        "# Directories\n",
        "# Fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE\n",
        "RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])  # runs/RUN_ID/processed (LOCAL)\n",
        "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]  # data/processed (LOCAL)\n",
        "RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])  # runs/RUN_ID/processed (DRIVE)\n",
        "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]  # data/processed (DRIVE)\n",
        "\n",
        "RUN_MS_LOCAL = Path(LOCAL_PATHS[\"ms_dir\"])  # runs/RUN_ID/model_selection (LOCAL)\n",
        "RUN_MS_DRIVE = Path(DRIVE_PATHS[\"ms_dir\"])  # runs/RUN_ID/model_selection (DRIVE)\n",
        "MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n",
        "MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n",
        "\n",
        "print(\"[INFO] Final Model output dirs:\")\n",
        "print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n",
        "print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n",
        "\n",
        "# -------------------------\n",
        "# 1. Load data from snapshot (data/processed/)\n",
        "# -------------------------\n",
        "print(\"\\n[INFO] Loading data (RUN_ID -> data/processed fallback):\")\n",
        "\n",
        "X_train_sel = load_with_fallback(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "X_valid_sel = load_with_fallback(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "X_test_sel = load_with_fallback(\"X_test_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "best_params = load_with_fallback(\"best_params_xgb_reg_t1.pkl\", RUN_MS_LOCAL, FALLBACK_PROC_LOCAL, RUN_MS_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "print(f\"[INFO] Loaded: X_train_sel={X_train_sel.shape} | X_valid_sel={X_valid_sel.shape} | X_test_sel={X_test_sel.shape}\")\n",
        "\n",
        "y_train = y_train_t1.astype(float).copy()\n",
        "y_valid = y_valid_t1.astype(float).copy()\n",
        "y_test = y_test_t1.astype(float).copy()\n",
        "\n",
        "w_train_arr = np.asarray(w_train, dtype=float)\n",
        "w_valid_arr = np.asarray(w_valid, dtype=float)\n",
        "w_test_arr = np.asarray(w_test, dtype=float)\n",
        "# -------------------------\n",
        "# Apply SHAP feature selection if configured\n",
        "# -------------------------\n",
        "SHAP_CFG = RUN_PARAMS.get(\"shap\", {})\n",
        "SHAP_TOP_N = SHAP_CFG.get(\"top_n_features\", None)\n",
        "\n",
        "if SHAP_TOP_N:\n",
        "    shap_features_file = f\"shap_top_{SHAP_TOP_N}_features.pkl\"\n",
        "    shap_path_local = FALLBACK_PROC_LOCAL / shap_features_file\n",
        "    shap_path_drive = FALLBACK_PROC_DRIVE / shap_features_file\n",
        "\n",
        "    shap_features = None\n",
        "    for path in [shap_path_local, shap_path_drive]:\n",
        "        if path.exists():\n",
        "            shap_features = load_pickle(path)\n",
        "            print(f\"[INFO] Loaded SHAP top {SHAP_TOP_N} features from {path}\")\n",
        "            break\n",
        "\n",
        "    if shap_features:\n",
        "        # Filter to SHAP-selected features\n",
        "        X_train_sel = X_train_sel[shap_features]\n",
        "        X_valid_sel = X_valid_sel[shap_features]\n",
        "        X_test_sel = X_test_sel[shap_features]\n",
        "        print(f\"[INFO] Using SHAP top {SHAP_TOP_N} features: {shap_features}\")\n",
        "    else:\n",
        "        print(f\"[WARN] {shap_features_file} not found. Run XGBoost with SHAP first.\")\n",
        "        print(f\"[INFO] Using original xgb_selected features\")\n",
        "else:\n",
        "    print(f\"[INFO] Using xgb_selected features ({X_train_sel.shape[1]} features)\")\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Align VALID + TEST data with Neural Networks (skip first lookback-1 rows)\n",
        "# This ensures metrics are comparable across XGBoost and Neural models\n",
        "# -------------------------\n",
        "LOOKBACK = int(HPO_CFG[\"lookback\"])\n",
        "SKIP_ROWS = LOOKBACK - 1\n",
        "\n",
        "# Align VALID\n",
        "X_valid_sel = X_valid_sel.iloc[SKIP_ROWS:]\n",
        "y_valid = y_valid.iloc[SKIP_ROWS:]\n",
        "w_valid_arr = w_valid_arr[SKIP_ROWS:]\n",
        "\n",
        "# Align TEST\n",
        "X_test_sel = X_test_sel.iloc[SKIP_ROWS:]\n",
        "y_test = y_test.iloc[SKIP_ROWS:]\n",
        "w_test_arr = w_test_arr[SKIP_ROWS:]\n",
        "\n",
        "print(f\"[INFO] Aligned VALID+TEST with lookback={LOOKBACK}: skipped first {SKIP_ROWS} rows\")\n",
        "print(f\"[INFO] VALID shapes after alignment: X={X_valid_sel.shape}\")\n",
        "print(f\"[INFO] TEST shapes after alignment: X={X_test_sel.shape}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2. Split VALID into ES and SCORE (date-based)\n",
        "# -------------------------\n",
        "VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
        "VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
        "VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
        "VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
        "\n",
        "\n",
        "def split_valid_es_score(Xv, yv, wv):\n",
        "    \"\"\"Split validation into ES (early stopping) and SCORE (model selection) sets.\"\"\"\n",
        "    if not isinstance(Xv.index, pd.DatetimeIndex):\n",
        "        return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "    wv_s = pd.Series(wv, index=Xv.index)\n",
        "\n",
        "    es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n",
        "    sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n",
        "\n",
        "    m_es = (Xv.index >= es_start) & (Xv.index <= es_end)\n",
        "    m_sc = (Xv.index >= sc_start) & (Xv.index <= sc_end)\n",
        "    mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n",
        "\n",
        "    if m_es.sum() > 0 and m_sc.sum() > 0:\n",
        "        return (Xv.loc[m_es], yv.loc[m_es], wv_s.loc[m_es].to_numpy(float)), \\\n",
        "               (Xv.loc[m_sc], yv.loc[m_sc], wv_s.loc[m_sc].to_numpy(float)), \\\n",
        "               mode_str\n",
        "\n",
        "    return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "\n",
        "(X_es, y_es, w_es), (X_sc, y_sc, w_sc), valid_mode = split_valid_es_score(\n",
        "    X_valid_sel, y_valid, w_valid_arr\n",
        ")\n",
        "\n",
        "print(f\"[INFO] VALID mode: {valid_mode}\")\n",
        "print(f\"[INFO] VALID_ES: {X_es.shape} | VALID_SCORE: {X_sc.shape}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 3. Metrics functions\n",
        "# -------------------------\n",
        "# Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 4. BASELINES (Zero + Naive)\n",
        "# -------------------------\n",
        "print(\"\\n[INFO] Computing BASELINES...\")\n",
        "\n",
        "baseline_results = []\n",
        "\n",
        "# ----- BASELINE ZERO (predict 0) -----\n",
        "print(\"  [1] BASELINE_ZERO (predict 0):\")\n",
        "\n",
        "# Zero baseline on VALID_SCORE\n",
        "pred_zero_sc = np.zeros(len(y_sc), dtype=float)\n",
        "baseline_results.append({\n",
        "    \"model\": \"BASELINE_ZERO\",\n",
        "    \"split\": \"VALID_SCORE\",\n",
        "    \"n\": int(len(y_sc)),\n",
        "    \"wRMSE\": w_rmse(y_sc, pred_zero_sc, w_sc),\n",
        "    \"wMAE\": w_mae(y_sc, pred_zero_sc, w_sc),\n",
        "    \"DirAcc\": dir_acc(y_sc, pred_zero_sc),\n",
        "})\n",
        "\n",
        "# Zero baseline on TEST\n",
        "pred_zero_test = np.zeros(len(y_test), dtype=float)\n",
        "baseline_results.append({\n",
        "    \"model\": \"BASELINE_ZERO\",\n",
        "    \"split\": \"TEST\",\n",
        "    \"n\": int(len(y_test)),\n",
        "    \"wRMSE\": w_rmse(y_test, pred_zero_test, w_test_arr),\n",
        "    \"wMAE\": w_mae(y_test, pred_zero_test, w_test_arr),\n",
        "    \"DirAcc\": dir_acc(y_test, pred_zero_test),\n",
        "})\n",
        "\n",
        "for r in baseline_results[-2:]:\n",
        "    print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "# ----- BASELINE NAIVE (last-value forecast) -----\n",
        "# Predict: y[t] = y[t-1] (tomorrow's return = today's return)\n",
        "print(\"  [2] BASELINE_NAIVE (last-value forecast):\")\n",
        "\n",
        "# For naive forecast, we need the previous day's actual value\n",
        "# y_sc and y_test are already aligned - we need to load full y to get lag\n",
        "\n",
        "# Try to load full y for naive baseline\n",
        "try:\n",
        "    y_full = load_with_fallback(\"y_full.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    # Get indices for valid_score and test\n",
        "    y_full_arr = np.asarray(y_full, dtype=float)\n",
        "\n",
        "    # Naive: predict previous value\n",
        "    # For valid_score period, we need the value from day before\n",
        "    # Assuming y_sc starts where y_valid ends (after ES period)\n",
        "    # We'll use shifted y values\n",
        "\n",
        "    # Simple approach: shift y by 1 within each split\n",
        "    pred_naive_sc = np.roll(y_sc, 1)\n",
        "    pred_naive_sc[0] = 0  # First value has no previous, use 0\n",
        "\n",
        "    pred_naive_test = np.roll(y_test, 1)\n",
        "    pred_naive_test[0] = 0  # First value has no previous, use 0\n",
        "\n",
        "    naive_available = True\n",
        "except:\n",
        "    # If y_full not available, use simple shift within splits\n",
        "    pred_naive_sc = np.roll(np.asarray(y_sc), 1)\n",
        "    pred_naive_sc[0] = 0\n",
        "\n",
        "    pred_naive_test = np.roll(np.asarray(y_test), 1)\n",
        "    pred_naive_test[0] = 0\n",
        "\n",
        "    naive_available = True\n",
        "\n",
        "if naive_available:\n",
        "    # Naive baseline on VALID_SCORE\n",
        "    baseline_results.append({\n",
        "        \"model\": \"BASELINE_NAIVE\",\n",
        "        \"split\": \"VALID_SCORE\",\n",
        "        \"n\": int(len(y_sc)),\n",
        "        \"wRMSE\": w_rmse(y_sc, pred_naive_sc, w_sc),\n",
        "        \"wMAE\": w_mae(y_sc, pred_naive_sc, w_sc),\n",
        "        \"DirAcc\": dir_acc(y_sc, pred_naive_sc),\n",
        "    })\n",
        "\n",
        "    # Naive baseline on TEST\n",
        "    baseline_results.append({\n",
        "        \"model\": \"BASELINE_NAIVE\",\n",
        "        \"split\": \"TEST\",\n",
        "        \"n\": int(len(y_test)),\n",
        "        \"wRMSE\": w_rmse(y_test, pred_naive_test, w_test_arr),\n",
        "        \"wMAE\": w_mae(y_test, pred_naive_test, w_test_arr),\n",
        "        \"DirAcc\": dir_acc(y_test, pred_naive_test),\n",
        "    })\n",
        "\n",
        "    for r in baseline_results[-2:]:\n",
        "        print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "print(\"\\n[INFO] BASELINE Summary:\")\n",
        "for r in baseline_results:\n",
        "    print(f\"  - {r['model']:15} | {r['split']:12} | wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 5. Train FINAL MODEL (early stop on VALID_ES)\n",
        "# -------------------------\n",
        "print(\"\\n[INFO] Training FINAL MODEL...\")\n",
        "\n",
        "N_ESTIMATORS = int(HPO_CFG[\"n_estimators\"])\n",
        "EARLY_STOP = int(HPO_CFG[\"early_stopping_rounds\"])\n",
        "RANDOM_SEED = int(HPO_CFG[\"random_state\"])\n",
        "\n",
        "# Ensure correct types in best_params\n",
        "best_params = dict(best_params)\n",
        "best_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n",
        "for k in [\"learning_rate\", \"min_child_weight\", \"subsample\", \"colsample_bytree\",\n",
        "          \"gamma\", \"reg_alpha\", \"reg_lambda\", \"max_delta_step\"]:\n",
        "    if k in best_params:\n",
        "        best_params[k] = float(best_params[k])\n",
        "\n",
        "# XGBoost settings from config\n",
        "OBJECTIVE = HPO_CFG[\"objective\"]\n",
        "EVAL_METRIC = HPO_CFG[\"eval_metric\"]\n",
        "TREE_METHOD = HPO_CFG[\"tree_method\"]\n",
        "\n",
        "model = xgb.XGBRegressor(\n",
        "    n_estimators=N_ESTIMATORS,\n",
        "    objective=OBJECTIVE,\n",
        "    eval_metric=EVAL_METRIC,\n",
        "    tree_method=TREE_METHOD,\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1,\n",
        "    verbosity=0,\n",
        "    early_stopping_rounds=EARLY_STOP,\n",
        "    **best_params\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train_sel, y_train,\n",
        "    sample_weight=w_train_arr,\n",
        "    eval_set=[(X_es, y_es)],\n",
        "    sample_weight_eval_set=[w_es],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "best_iter = getattr(model, \"best_iteration\", None)\n",
        "best_score = getattr(model, \"best_score\", None)\n",
        "print(f\"[INFO] Training complete. best_iteration={best_iter} | best_score(ES)={best_score}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 6. Evaluate FINAL MODEL on VALID_SCORE + TEST\n",
        "# -------------------------\n",
        "print(\"\\n[INFO] Evaluating FINAL MODEL...\")\n",
        "\n",
        "model_results = []\n",
        "\n",
        "# Model on VALID_SCORE\n",
        "pred_model_sc = model.predict(X_sc)\n",
        "model_results.append({\n",
        "    \"model\": \"FINAL_XGB\",\n",
        "    \"split\": \"VALID_SCORE\",\n",
        "    \"n\": int(len(y_sc)),\n",
        "    \"wRMSE\": w_rmse(y_sc, pred_model_sc, w_sc),\n",
        "    \"wMAE\": w_mae(y_sc, pred_model_sc, w_sc),\n",
        "    \"DirAcc\": dir_acc(y_sc, pred_model_sc),\n",
        "})\n",
        "\n",
        "# Model on TEST\n",
        "pred_model_test = model.predict(X_test_sel)\n",
        "model_results.append({\n",
        "    \"model\": \"FINAL_XGB\",\n",
        "    \"split\": \"TEST\",\n",
        "    \"n\": int(len(y_test)),\n",
        "    \"wRMSE\": w_rmse(y_test, pred_model_test, w_test_arr),\n",
        "    \"wMAE\": w_mae(y_test, pred_model_test, w_test_arr),\n",
        "    \"DirAcc\": dir_acc(y_test, pred_model_test),\n",
        "})\n",
        "\n",
        "print(\"[INFO] FINAL MODEL results:\")\n",
        "for r in model_results:\n",
        "    print(f\"  - {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 7. Comparison: BASELINE vs MODEL\n",
        "# -------------------------\n",
        "all_results = baseline_results + model_results\n",
        "metrics_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Add improvement column (compared to BASELINE_ZERO)\n",
        "baseline_zero_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_ZERO\"}\n",
        "metrics_df[\"wRMSE_vs_zero\"] = metrics_df.apply(\n",
        "    lambda row: baseline_zero_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n",
        ")\n",
        "\n",
        "# Add improvement vs BASELINE_NAIVE\n",
        "baseline_naive_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_NAIVE\"}\n",
        "metrics_df[\"wRMSE_vs_naive\"] = metrics_df.apply(\n",
        "    lambda row: baseline_naive_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n[INFO] BASELINE vs FINAL MODEL comparison:\")\n",
        "display(metrics_df)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 8. Build predictions DataFrames\n",
        "# -------------------------\n",
        "preds_valid_score_df = pd.DataFrame({\n",
        "    \"date\": X_sc.index,\n",
        "    \"actual\": y_sc.values,\n",
        "    \"baseline_zero\": pred_zero_sc,\n",
        "    \"baseline_naive\": pred_naive_sc,\n",
        "    \"predicted\": pred_model_sc,\n",
        "    \"sample_weight\": w_sc,\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "preds_test_df = pd.DataFrame({\n",
        "    \"date\": X_test_sel.index,\n",
        "    \"actual\": y_test.values,\n",
        "    \"baseline_zero\": pred_zero_test,\n",
        "    \"baseline_naive\": pred_naive_test,\n",
        "    \"predicted\": pred_model_test,\n",
        "    \"sample_weight\": w_test_arr,\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 9. Save artifacts (LOCAL + DRIVE)\n",
        "# -------------------------\n",
        "# Model\n",
        "# Save model as JSON\n",
        "model_path_local = MODELS_OUT_LOCAL / \"final_model_xgb.json\"\n",
        "model.get_booster().save_model(str(model_path_local))\n",
        "copy_file(model_path_local, MODELS_OUT_DRIVE / model_path_local.name)\n",
        "\n",
        "# Also save as pickle for easier loading\n",
        "model_pkl_local = MODELS_OUT_LOCAL / \"final_model_xgb.pkl\"\n",
        "save_pickle(model, model_pkl_local)\n",
        "copy_file(model_pkl_local, MODELS_OUT_DRIVE / model_pkl_local.name)\n",
        "\n",
        "# Metrics\n",
        "metrics_path_local = MODELS_OUT_LOCAL / \"final_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_path_local, index=False)\n",
        "copy_file(metrics_path_local, MODELS_OUT_DRIVE / metrics_path_local.name)\n",
        "\n",
        "# Baseline results JSON (for CLI summary)\n",
        "OUTPUTS_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"outputs_dir\"]))\n",
        "OUTPUTS_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"outputs_dir\"]))\n",
        "\n",
        "for baseline_row in baseline_results:\n",
        "    if baseline_row[\"split\"] == \"TEST\":\n",
        "        baseline_json = {\n",
        "            \"model\": baseline_row[\"model\"],\n",
        "            \"test_wrmse\": baseline_row[\"wRMSE\"],\n",
        "            \"test_wmae\": baseline_row.get(\"wMAE\"),\n",
        "            \"test_diracc\": baseline_row[\"DirAcc\"],\n",
        "        }\n",
        "        baseline_name = baseline_row[\"model\"].lower()\n",
        "        save_json(baseline_json, OUTPUTS_LOCAL / f\"{baseline_name}_results.json\")\n",
        "        save_json(baseline_json, OUTPUTS_DRIVE / f\"{baseline_name}_results.json\")\n",
        "\n",
        "# Predictions -> predictions/xgb/\n",
        "PRED_XGB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / \"xgb\")\n",
        "PRED_XGB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / \"xgb\")\n",
        "\n",
        "preds_valid_path_local = PRED_XGB_LOCAL / \"predictions_valid.csv\"\n",
        "preds_valid_score_df.to_csv(preds_valid_path_local, index=False)\n",
        "copy_file(preds_valid_path_local, PRED_XGB_DRIVE / preds_valid_path_local.name)\n",
        "\n",
        "preds_test_path_local = PRED_XGB_LOCAL / \"predictions_test.csv\"\n",
        "preds_test_df.to_csv(preds_test_path_local, index=False)\n",
        "copy_file(preds_test_path_local, PRED_XGB_DRIVE / preds_test_path_local.name)\n",
        "\n",
        "print(\"\\n[OK] Saved FINAL MODEL artifacts:\")\n",
        "print(\"  -\", model_path_local.name)\n",
        "print(\"  -\", metrics_path_local.name)\n",
        "print(\"  - predictions/xgb/\", preds_valid_path_local.name)\n",
        "print(\"  - predictions/xgb/\", preds_test_path_local.name)\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Tomorrow Prediction + Plot\n",
        "# -------------------------\n",
        "PLOT_CFG = RUN_PARAMS[\"plot\"]\n",
        "N_PLOT = int(PLOT_CFG[\"n_plot\"])\n",
        "FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n",
        "DPI = int(PLOT_CFG[\"dpi\"])\n",
        "\n",
        "# Predict tomorrow\n",
        "last_date = X_test_sel.index[-1]\n",
        "X_last = X_test_sel.iloc[[-1]]\n",
        "pred_tomorrow = float(model.predict(X_last)[0])\n",
        "pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n",
        "\n",
        "# Save tomorrow prediction\n",
        "pred_tomorrow_df = pd.DataFrame([{\n",
        "    \"feature_set\": \"xgb_selected\",\n",
        "    \"last_data_date\": last_date,\n",
        "    \"predicted_for\": \"next_trading_day\",\n",
        "    \"pred_logret\": pred_tomorrow,\n",
        "    \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
        "}])\n",
        "pred_tomorrow_df.to_csv(PRED_XGB_LOCAL / \"tomorrow.csv\", index=False)\n",
        "copy_file(PRED_XGB_LOCAL / \"tomorrow.csv\", PRED_XGB_DRIVE / \"tomorrow.csv\")\n",
        "\n",
        "# Create backtest dataframe\n",
        "hist_df = pd.DataFrame({\n",
        "    \"date\": X_test_sel.index,\n",
        "    \"actual\": y_test.values,\n",
        "    \"y_pred\": pred_model_test,\n",
        "}).set_index(\"date\")\n",
        "\n",
        "hist_tail = hist_df.tail(N_PLOT).copy()\n",
        "hist_tail.to_csv(PRED_XGB_LOCAL / \"backtest.csv\")\n",
        "copy_file(PRED_XGB_LOCAL / \"backtest.csv\", PRED_XGB_DRIVE / \"backtest.csv\")\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
        "ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n",
        "ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted (y_pred)\")\n",
        "ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n",
        "ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
        "ax.set_title(f\"XGBoost Predictions â€” xgb_selected â€” last {len(hist_tail)} days + tomorrow\")\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Log Return\")\n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(PRED_XGB_LOCAL / \"plot.png\", dpi=DPI)\n",
        "copy_file(PRED_XGB_LOCAL / \"plot.png\", PRED_XGB_DRIVE / \"plot.png\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n",
        "print(f\"[OK] Saved: predictions/xgb/ (tomorrow.csv, backtest.csv, plot.png)\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# SHAP Analysis (config-based)\n",
        "# -------------------------\n",
        "SHAP_CFG = RUN_PARAMS.get(\"shap\", {})\n",
        "SHAP_ENABLED = bool(SHAP_CFG.get(\"enabled\", True))\n",
        "\n",
        "if SHAP_ENABLED:\n",
        "    print(\"\\n[INFO] Computing SHAP values...\")\n",
        "\n",
        "    try:\n",
        "        import shap\n",
        "\n",
        "        # Config\n",
        "        SHAP_MAX_DISPLAY = int(SHAP_CFG.get(\"max_display\", 20))\n",
        "        SHAP_FIGSIZE = tuple(SHAP_CFG.get(\"figsize\", [10, 8]))\n",
        "        SHAP_BAR = bool(SHAP_CFG.get(\"plot_type_bar\", True))\n",
        "        SHAP_BEESWARM = bool(SHAP_CFG.get(\"plot_type_beeswarm\", True))\n",
        "        SHAP_SAVE_VALUES = bool(SHAP_CFG.get(\"save_values\", True))\n",
        "\n",
        "        # Create explainer (TreeExplainer is fast for XGBoost)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "\n",
        "        # Compute SHAP values on TEST set\n",
        "        shap_values = explainer.shap_values(X_test_sel)\n",
        "\n",
        "        # Save SHAP values as DataFrame\n",
        "        if SHAP_SAVE_VALUES:\n",
        "            shap_df = pd.DataFrame(shap_values, columns=X_test_sel.columns, index=X_test_sel.index)\n",
        "            shap_df.to_csv(PRED_XGB_LOCAL / \"shap_values_test.csv\")\n",
        "            copy_file(PRED_XGB_LOCAL / \"shap_values_test.csv\", PRED_XGB_DRIVE / \"shap_values_test.csv\")\n",
        "\n",
        "        # Feature importance (mean |SHAP|)\n",
        "        shap_importance = pd.DataFrame({\n",
        "            \"feature\": X_test_sel.columns,\n",
        "            \"mean_abs_shap\": np.abs(shap_values).mean(axis=0)\n",
        "        }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
        "        shap_importance.to_csv(PRED_XGB_LOCAL / \"shap_feature_importance.csv\", index=False)\n",
        "        copy_file(PRED_XGB_LOCAL / \"shap_feature_importance.csv\", PRED_XGB_DRIVE / \"shap_feature_importance.csv\")\n",
        "        # Save top N features for future runs\n",
        "        SHAP_TOP_N_SAVE = SHAP_CFG.get(\"top_n_features\", None)\n",
        "        if SHAP_TOP_N_SAVE:\n",
        "            top_features = shap_importance.head(SHAP_TOP_N_SAVE)[\"feature\"].tolist()\n",
        "            shap_save_file = f\"shap_top_{SHAP_TOP_N_SAVE}_features.pkl\"\n",
        "            save_pickle(top_features, FALLBACK_PROC_LOCAL / shap_save_file)\n",
        "            copy_file(FALLBACK_PROC_LOCAL / shap_save_file, FALLBACK_PROC_DRIVE / shap_save_file)\n",
        "            print(f\"[INFO] Saved {shap_save_file}: {top_features}\")\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"[INFO] Top 10 features by SHAP importance:\")\n",
        "        print(shap_importance.head(10).to_string(index=False))\n",
        "\n",
        "        # SHAP Summary Plot (bar)\n",
        "        if SHAP_BAR:\n",
        "            fig_bar, ax_bar = plt.subplots(figsize=SHAP_FIGSIZE)\n",
        "            shap.summary_plot(shap_values, X_test_sel, plot_type=\"bar\", show=False, max_display=SHAP_MAX_DISPLAY)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(PRED_XGB_LOCAL / \"shap_summary_bar.png\", dpi=DPI, bbox_inches=\"tight\")\n",
        "            copy_file(PRED_XGB_LOCAL / \"shap_summary_bar.png\", PRED_XGB_DRIVE / \"shap_summary_bar.png\")\n",
        "            plt.close()\n",
        "\n",
        "        # SHAP Summary Plot (beeswarm)\n",
        "        if SHAP_BEESWARM:\n",
        "            fig_bee, ax_bee = plt.subplots(figsize=SHAP_FIGSIZE)\n",
        "            shap.summary_plot(shap_values, X_test_sel, show=False, max_display=SHAP_MAX_DISPLAY)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(PRED_XGB_LOCAL / \"shap_summary_beeswarm.png\", dpi=DPI, bbox_inches=\"tight\")\n",
        "            copy_file(PRED_XGB_LOCAL / \"shap_summary_beeswarm.png\", PRED_XGB_DRIVE / \"shap_summary_beeswarm.png\")\n",
        "            plt.close()\n",
        "\n",
        "        print(\"[OK] Saved SHAP analysis:\")\n",
        "        if SHAP_SAVE_VALUES:\n",
        "            print(\"  - shap_values_test.csv\")\n",
        "        print(\"  - shap_feature_importance.csv\")\n",
        "        if SHAP_BAR:\n",
        "            print(\"  - shap_summary_bar.png\")\n",
        "        if SHAP_BEESWARM:\n",
        "            print(\"  - shap_summary_beeswarm.png\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"[WARN] shap not installed. Run: pip install shap\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] SHAP analysis failed: {e}\")\n",
        "else:\n",
        "    print(\"[INFO] SHAP analysis disabled in config.\")\n",
        "\n",
        "print(\"[OK] BLOCK 25 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F2TdKbE8SAXJ",
      "metadata": {
        "id": "F2TdKbE8SAXJ"
      },
      "source": [
        "---\n",
        "# SECTION 7B: LightGBM Model\n",
        "\n",
        "**Train and evaluate LightGBM model (similar to XGBoost)**\n",
        "\n",
        "**Blocks:** 25B"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Voc5kb2SAXJ",
      "metadata": {
        "id": "0Voc5kb2SAXJ"
      },
      "source": [
        "## BLOCK 25B â€” LIGHTGBM FINAL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "6wIe6nMiSAXJ",
      "metadata": {
        "id": "6wIe6nMiSAXJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29c680ce-f5b8-4d22-9b2a-9975209699fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] LightGBM available\n",
            "[INFO] LightGBM Final Model output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/models\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/models\n",
            "\n",
            "[INFO] Loading data (RUN_ID -> data/processed fallback):\n",
            "  [LOAD] X_train_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_xgb_selected.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_test.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_test.pkl <- data/processed (LOCAL)\n",
            "[INFO] Using default LightGBM params from config\n",
            "[INFO] Loaded: X_train_sel=(347, 10) | X_valid_sel=(77, 10) | X_test_sel=(87, 10)\n",
            "[INFO] Aligned VALID+TEST with lookback=7: skipped first 6 rows\n",
            "[INFO] VALID shapes after alignment: X=(71, 10)\n",
            "[INFO] TEST shapes after alignment: X=(81, 10)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] VALID_ES: (31, 10) | VALID_SCORE: (40, 10)\n",
            "\n",
            "[INFO] Computing BASELINES...\n",
            "  [1] BASELINE_ZERO (predict 0):\n",
            "      VALID_SCORE: wRMSE=0.025004 | DirAcc=0.3500\n",
            "      TEST: wRMSE=0.019984 | DirAcc=0.4444\n",
            "  [2] BASELINE_NAIVE (last-value forecast):\n",
            "      VALID_SCORE: wRMSE=0.031440 | DirAcc=0.4250\n",
            "      TEST: wRMSE=0.025313 | DirAcc=0.5062\n",
            "\n",
            "[INFO] BASELINE Summary:\n",
            "  - BASELINE_ZERO   | VALID_SCORE  | wRMSE=0.025004 | DirAcc=0.3500\n",
            "  - BASELINE_ZERO   | TEST         | wRMSE=0.019984 | DirAcc=0.4444\n",
            "  - BASELINE_NAIVE  | VALID_SCORE  | wRMSE=0.031440 | DirAcc=0.4250\n",
            "  - BASELINE_NAIVE  | TEST         | wRMSE=0.025313 | DirAcc=0.5062\n",
            "\n",
            "[INFO] Training FINAL LightGBM MODEL...\n",
            "[INFO] Training complete. best_iteration=2\n",
            "\n",
            "[INFO] Evaluating FINAL MODEL...\n",
            "[INFO] FINAL MODEL results:\n",
            "  - VALID_SCORE: wRMSE=0.024944 | DirAcc=0.6500\n",
            "  - TEST: wRMSE=0.019934 | DirAcc=0.5556\n",
            "\n",
            "[INFO] BASELINE vs FINAL MODEL comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            model        split   n     wRMSE      wMAE    DirAcc  \\\n",
              "0   BASELINE_ZERO  VALID_SCORE  40  0.025004  0.015738  0.350000   \n",
              "1   BASELINE_ZERO         TEST  81  0.019984  0.015729  0.444444   \n",
              "2  BASELINE_NAIVE  VALID_SCORE  40  0.031440  0.021985  0.425000   \n",
              "3  BASELINE_NAIVE         TEST  81  0.025313  0.020068  0.506173   \n",
              "4       FINAL_LGB  VALID_SCORE  40  0.024944  0.015696  0.650000   \n",
              "5       FINAL_LGB         TEST  81  0.019934  0.015688  0.555556   \n",
              "\n",
              "   wRMSE_vs_zero  wRMSE_vs_naive  \n",
              "0       0.000000        0.006437  \n",
              "1       0.000000        0.005329  \n",
              "2      -0.006437        0.000000  \n",
              "3      -0.005329        0.000000  \n",
              "4       0.000060        0.006497  \n",
              "5       0.000049        0.005379  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e43d3db2-d807-42da-8e39-366d4d57e509\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>split</th>\n",
              "      <th>n</th>\n",
              "      <th>wRMSE</th>\n",
              "      <th>wMAE</th>\n",
              "      <th>DirAcc</th>\n",
              "      <th>wRMSE_vs_zero</th>\n",
              "      <th>wRMSE_vs_naive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BASELINE_ZERO</td>\n",
              "      <td>VALID_SCORE</td>\n",
              "      <td>40</td>\n",
              "      <td>0.025004</td>\n",
              "      <td>0.015738</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BASELINE_ZERO</td>\n",
              "      <td>TEST</td>\n",
              "      <td>81</td>\n",
              "      <td>0.019984</td>\n",
              "      <td>0.015729</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BASELINE_NAIVE</td>\n",
              "      <td>VALID_SCORE</td>\n",
              "      <td>40</td>\n",
              "      <td>0.031440</td>\n",
              "      <td>0.021985</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>-0.006437</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BASELINE_NAIVE</td>\n",
              "      <td>TEST</td>\n",
              "      <td>81</td>\n",
              "      <td>0.025313</td>\n",
              "      <td>0.020068</td>\n",
              "      <td>0.506173</td>\n",
              "      <td>-0.005329</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FINAL_LGB</td>\n",
              "      <td>VALID_SCORE</td>\n",
              "      <td>40</td>\n",
              "      <td>0.024944</td>\n",
              "      <td>0.015696</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.006497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>FINAL_LGB</td>\n",
              "      <td>TEST</td>\n",
              "      <td>81</td>\n",
              "      <td>0.019934</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.005379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e43d3db2-d807-42da-8e39-366d4d57e509')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e43d3db2-d807-42da-8e39-366d4d57e509 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e43d3db2-d807-42da-8e39-366d4d57e509');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_95f20329-1f9d-4351-9991-bdc2a3e44687\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_95f20329-1f9d-4351-9991-bdc2a3e44687 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BASELINE_ZERO\",\n          \"BASELINE_NAIVE\",\n          \"FINAL_LGB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TEST\",\n          \"VALID_SCORE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22,\n        \"min\": 40,\n        \"max\": 81,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          81,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wRMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004254249394768061,\n        \"min\": 0.019934300215366338,\n        \"max\": 0.03144003541115706,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.025003501413126943,\n          0.019983748548673245\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wMAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0028102644920881795,\n        \"min\": 0.01568816076333821,\n        \"max\": 0.021985245302051933,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.01573836841257764,\n          0.015729483115759623\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DirAcc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1058739002620268,\n        \"min\": 0.35,\n        \"max\": 0.65,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.35,\n          0.4444444444444444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wRMSE_vs_zero\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0030721708239232672,\n        \"min\": -0.006436533998030115,\n        \"max\": 5.998958535379656e-05,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.006436533998030115,\n          4.944833330690693e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wRMSE_vs_naive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0030924368003659272,\n        \"min\": 0.0,\n        \"max\": 0.006496523583383911,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0053292966284727294,\n          0.005378744961779636\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] Saved FINAL LightGBM MODEL artifacts:\n",
            "  - final_model_lgb.json\n",
            "  - final_model_lgb.pkl\n",
            "  - final_metrics_lgb.csv\n",
            "  - predictions/lgb/ predictions_valid.csv\n",
            "  - predictions/lgb/ predictions_test.csv\n",
            "[INFO] Tomorrow prediction: 0.000118 (0.0118%)\n",
            "[OK] Saved: predictions/lgb/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "[OK] BLOCK 25B complete.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LIGHTGBM FINAL MODEL (identical structure to XGBoost)\n",
        "# ============================================================\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "    print(\"[OK] LightGBM available\")\n",
        "except ImportError:\n",
        "    LGB_AVAILABLE = False\n",
        "    print(\"[SKIP] LightGBM not installed\")\n",
        "\n",
        "if LGB_AVAILABLE:\n",
        "    # Config (use lgb_hpo if exists, fallback to hpo)\n",
        "    HPO_CFG = RUN_PARAMS.get(\"lgb_hpo\", RUN_PARAMS[\"hpo\"])\n",
        "\n",
        "    # Directories\n",
        "    RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
        "    FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "    RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
        "    FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "\n",
        "    RUN_MS_LOCAL = Path(LOCAL_PATHS[\"ms_dir\"])\n",
        "    RUN_MS_DRIVE = Path(DRIVE_PATHS[\"ms_dir\"])\n",
        "    MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n",
        "    MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n",
        "\n",
        "    print(\"[INFO] LightGBM Final Model output dirs:\")\n",
        "    print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n",
        "    print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n",
        "\n",
        "    # -------------------------\n",
        "    # 1. Load data from snapshot\n",
        "    # -------------------------\n",
        "    print(\"\\n[INFO] Loading data (RUN_ID -> data/processed fallback):\")\n",
        "\n",
        "    X_train_sel = load_with_fallback(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "    X_valid_sel = load_with_fallback(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "    X_test_sel = load_with_fallback(\"X_test_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "    y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    # Load best_params\n",
        "    try:\n",
        "        best_params = load_with_fallback(\"best_params_lgb_reg_t1.pkl\", RUN_MS_LOCAL, FALLBACK_PROC_LOCAL, RUN_MS_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "        print(\"[INFO] Loaded best_params_lgb from file\")\n",
        "    except FileNotFoundError:\n",
        "        LGB_FS_CFG = RUN_PARAMS.get(\"lgb_fs\", RUN_PARAMS[\"xgb_fs\"])\n",
        "        best_params = {\n",
        "            \"max_depth\": int(LGB_FS_CFG.get(\"max_depth\", 3)),\n",
        "            \"num_leaves\": int(LGB_FS_CFG.get(\"num_leaves\", 31)),\n",
        "            \"learning_rate\": float(LGB_FS_CFG.get(\"learning_rate\", 0.05)),\n",
        "            \"min_child_samples\": int(LGB_FS_CFG.get(\"min_child_samples\", 20)),\n",
        "            \"subsample\": float(LGB_FS_CFG.get(\"subsample\", 0.7)),\n",
        "            \"colsample_bytree\": float(LGB_FS_CFG.get(\"colsample_bytree\", 0.7)),\n",
        "            \"reg_alpha\": float(LGB_FS_CFG.get(\"reg_alpha\", 1e-4)),\n",
        "            \"reg_lambda\": float(LGB_FS_CFG.get(\"reg_lambda\", 5.0)),\n",
        "        }\n",
        "        print(\"[INFO] Using default LightGBM params from config\")\n",
        "\n",
        "    print(f\"[INFO] Loaded: X_train_sel={X_train_sel.shape} | X_valid_sel={X_valid_sel.shape} | X_test_sel={X_test_sel.shape}\")\n",
        "\n",
        "    y_train = y_train_t1.astype(float).copy()\n",
        "    y_valid = y_valid_t1.astype(float).copy()\n",
        "    y_test = y_test_t1.astype(float).copy()\n",
        "\n",
        "    w_train_arr = np.asarray(w_train, dtype=float)\n",
        "    w_valid_arr = np.asarray(w_valid, dtype=float)\n",
        "    w_test_arr = np.asarray(w_test, dtype=float)\n",
        "\n",
        "    # Align VALID + TEST with Neural Networks\n",
        "    LOOKBACK = int(HPO_CFG[\"lookback\"])\n",
        "    SKIP_ROWS = LOOKBACK - 1\n",
        "\n",
        "    X_valid_sel = X_valid_sel.iloc[SKIP_ROWS:]\n",
        "    y_valid = y_valid.iloc[SKIP_ROWS:]\n",
        "    w_valid_arr = w_valid_arr[SKIP_ROWS:]\n",
        "\n",
        "    X_test_sel = X_test_sel.iloc[SKIP_ROWS:]\n",
        "    y_test = y_test.iloc[SKIP_ROWS:]\n",
        "    w_test_arr = w_test_arr[SKIP_ROWS:]\n",
        "\n",
        "    print(f\"[INFO] Aligned VALID+TEST with lookback={LOOKBACK}: skipped first {SKIP_ROWS} rows\")\n",
        "    print(f\"[INFO] VALID shapes after alignment: X={X_valid_sel.shape}\")\n",
        "    print(f\"[INFO] TEST shapes after alignment: X={X_test_sel.shape}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 2. Split VALID into ES and SCORE\n",
        "    # -------------------------\n",
        "    VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
        "    VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
        "    VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
        "    VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
        "\n",
        "    def split_valid_es_score_lgb(Xv, yv, wv):\n",
        "        if not isinstance(Xv.index, pd.DatetimeIndex):\n",
        "            return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
        "        wv_s = pd.Series(wv, index=Xv.index)\n",
        "\n",
        "        es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n",
        "        sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n",
        "\n",
        "        m_es = (Xv.index >= es_start) & (Xv.index <= es_end)\n",
        "        m_sc = (Xv.index >= sc_start) & (Xv.index <= sc_end)\n",
        "        mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n",
        "        if m_es.sum() > 0 and m_sc.sum() > 0:\n",
        "            return (Xv.loc[m_es], yv.loc[m_es], wv_s.loc[m_es].to_numpy(float)), \\\n",
        "                   (Xv.loc[m_sc], yv.loc[m_sc], wv_s.loc[m_sc].to_numpy(float)), \\\n",
        "                   mode_str\n",
        "        return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "    (X_es, y_es, w_es), (X_sc, y_sc, w_sc), valid_mode = split_valid_es_score_lgb(\n",
        "        X_valid_sel, y_valid, w_valid_arr\n",
        "    )\n",
        "\n",
        "    print(f\"[INFO] VALID mode: {valid_mode}\")\n",
        "    print(f\"[INFO] VALID_ES: {X_es.shape} | VALID_SCORE: {X_sc.shape}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 3. BASELINES\n",
        "    # -------------------------\n",
        "    print(\"\\n[INFO] Computing BASELINES...\")\n",
        "\n",
        "    baseline_results = []\n",
        "\n",
        "    print(\"  [1] BASELINE_ZERO (predict 0):\")\n",
        "    pred_zero_sc = np.zeros(len(y_sc), dtype=float)\n",
        "    baseline_results.append({\n",
        "        \"model\": \"BASELINE_ZERO\", \"split\": \"VALID_SCORE\", \"n\": int(len(y_sc)),\n",
        "        \"wRMSE\": w_rmse(y_sc, pred_zero_sc, w_sc),\n",
        "        \"wMAE\": w_mae(y_sc, pred_zero_sc, w_sc),\n",
        "        \"DirAcc\": dir_acc(y_sc, pred_zero_sc),\n",
        "    })\n",
        "\n",
        "    pred_zero_test = np.zeros(len(y_test), dtype=float)\n",
        "    baseline_results.append({\n",
        "        \"model\": \"BASELINE_ZERO\", \"split\": \"TEST\", \"n\": int(len(y_test)),\n",
        "        \"wRMSE\": w_rmse(y_test, pred_zero_test, w_test_arr),\n",
        "        \"wMAE\": w_mae(y_test, pred_zero_test, w_test_arr),\n",
        "        \"DirAcc\": dir_acc(y_test, pred_zero_test),\n",
        "    })\n",
        "\n",
        "    for r in baseline_results[-2:]:\n",
        "        print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "    print(\"  [2] BASELINE_NAIVE (last-value forecast):\")\n",
        "    pred_naive_sc = np.roll(np.asarray(y_sc), 1)\n",
        "    pred_naive_sc[0] = 0\n",
        "\n",
        "    pred_naive_test = np.roll(np.asarray(y_test), 1)\n",
        "    pred_naive_test[0] = 0\n",
        "\n",
        "    baseline_results.append({\n",
        "        \"model\": \"BASELINE_NAIVE\", \"split\": \"VALID_SCORE\", \"n\": int(len(y_sc)),\n",
        "        \"wRMSE\": w_rmse(y_sc, pred_naive_sc, w_sc),\n",
        "        \"wMAE\": w_mae(y_sc, pred_naive_sc, w_sc),\n",
        "        \"DirAcc\": dir_acc(y_sc, pred_naive_sc),\n",
        "    })\n",
        "\n",
        "    baseline_results.append({\n",
        "        \"model\": \"BASELINE_NAIVE\", \"split\": \"TEST\", \"n\": int(len(y_test)),\n",
        "        \"wRMSE\": w_rmse(y_test, pred_naive_test, w_test_arr),\n",
        "        \"wMAE\": w_mae(y_test, pred_naive_test, w_test_arr),\n",
        "        \"DirAcc\": dir_acc(y_test, pred_naive_test),\n",
        "    })\n",
        "\n",
        "    for r in baseline_results[-2:]:\n",
        "        print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "    print(\"\\n[INFO] BASELINE Summary:\")\n",
        "    for r in baseline_results:\n",
        "        print(f\"  - {r['model']:15} | {r['split']:12} | wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 4. Train FINAL MODEL\n",
        "    # -------------------------\n",
        "    print(\"\\n[INFO] Training FINAL LightGBM MODEL...\")\n",
        "\n",
        "    N_ESTIMATORS = int(HPO_CFG[\"n_estimators\"])\n",
        "    EARLY_STOP = int(HPO_CFG[\"early_stopping_rounds\"])\n",
        "    RANDOM_SEED = int(HPO_CFG[\"random_state\"])\n",
        "\n",
        "    best_params = dict(best_params)\n",
        "    if \"max_depth\" in best_params:\n",
        "        best_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n",
        "    if \"num_leaves\" in best_params:\n",
        "        best_params[\"num_leaves\"] = int(best_params[\"num_leaves\"])\n",
        "    if \"min_child_samples\" in best_params:\n",
        "        best_params[\"min_child_samples\"] = int(best_params[\"min_child_samples\"])\n",
        "    for k in [\"learning_rate\", \"subsample\", \"colsample_bytree\", \"reg_alpha\", \"reg_lambda\"]:\n",
        "        if k in best_params:\n",
        "            best_params[k] = float(best_params[k])\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        n_estimators=N_ESTIMATORS,\n",
        "        objective=\"regression\",\n",
        "        metric=\"rmse\",\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1,\n",
        "        **best_params\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train_sel, y_train,\n",
        "        sample_weight=w_train_arr,\n",
        "        eval_set=[(X_es, y_es)],\n",
        "        eval_sample_weight=[w_es],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=EARLY_STOP, verbose=False),\n",
        "            lgb.log_evaluation(period=0)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    best_iter = getattr(model, \"best_iteration_\", None)\n",
        "    print(f\"[INFO] Training complete. best_iteration={best_iter}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 5. Evaluate FINAL MODEL\n",
        "    # -------------------------\n",
        "    print(\"\\n[INFO] Evaluating FINAL MODEL...\")\n",
        "\n",
        "    model_results = []\n",
        "\n",
        "    pred_model_sc = model.predict(X_sc)\n",
        "    model_results.append({\n",
        "        \"model\": \"FINAL_LGB\", \"split\": \"VALID_SCORE\", \"n\": int(len(y_sc)),\n",
        "        \"wRMSE\": w_rmse(y_sc, pred_model_sc, w_sc),\n",
        "        \"wMAE\": w_mae(y_sc, pred_model_sc, w_sc),\n",
        "        \"DirAcc\": dir_acc(y_sc, pred_model_sc),\n",
        "    })\n",
        "\n",
        "    pred_model_test = model.predict(X_test_sel)\n",
        "    model_results.append({\n",
        "        \"model\": \"FINAL_LGB\", \"split\": \"TEST\", \"n\": int(len(y_test)),\n",
        "        \"wRMSE\": w_rmse(y_test, pred_model_test, w_test_arr),\n",
        "        \"wMAE\": w_mae(y_test, pred_model_test, w_test_arr),\n",
        "        \"DirAcc\": dir_acc(y_test, pred_model_test),\n",
        "    })\n",
        "\n",
        "    print(\"[INFO] FINAL MODEL results:\")\n",
        "    for r in model_results:\n",
        "        print(f\"  - {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 6. Comparison\n",
        "    # -------------------------\n",
        "    all_results = baseline_results + model_results\n",
        "    metrics_df = pd.DataFrame(all_results)\n",
        "\n",
        "    baseline_zero_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_ZERO\"}\n",
        "    metrics_df[\"wRMSE_vs_zero\"] = metrics_df.apply(\n",
        "        lambda row: baseline_zero_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n",
        "    )\n",
        "\n",
        "    baseline_naive_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_NAIVE\"}\n",
        "    metrics_df[\"wRMSE_vs_naive\"] = metrics_df.apply(\n",
        "        lambda row: baseline_naive_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n",
        "    )\n",
        "\n",
        "    print(\"\\n[INFO] BASELINE vs FINAL MODEL comparison:\")\n",
        "    display(metrics_df)\n",
        "\n",
        "    # -------------------------\n",
        "    # 7. Build predictions DataFrames\n",
        "    # -------------------------\n",
        "    preds_valid_score_df = pd.DataFrame({\n",
        "        \"date\": X_sc.index,\n",
        "        \"actual\": y_sc.values,\n",
        "        \"baseline_zero\": pred_zero_sc,\n",
        "        \"baseline_naive\": pred_naive_sc,\n",
        "        \"predicted\": pred_model_sc,\n",
        "        \"sample_weight\": w_sc,\n",
        "    }).reset_index(drop=True)\n",
        "\n",
        "    preds_test_df = pd.DataFrame({\n",
        "        \"date\": X_test_sel.index,\n",
        "        \"actual\": y_test.values,\n",
        "        \"baseline_zero\": pred_zero_test,\n",
        "        \"baseline_naive\": pred_naive_test,\n",
        "        \"predicted\": pred_model_test,\n",
        "        \"sample_weight\": w_test_arr,\n",
        "    }).reset_index(drop=True)\n",
        "\n",
        "    # -------------------------\n",
        "    # 8. Save artifacts\n",
        "    # -------------------------\n",
        "    model_json_local = MODELS_OUT_LOCAL / \"final_model_lgb.json\"\n",
        "    model_dict = model.booster_.dump_model()\n",
        "    save_json(model_dict, model_json_local)\n",
        "    copy_file(model_json_local, MODELS_OUT_DRIVE / model_json_local.name)\n",
        "\n",
        "    model_pkl_local = MODELS_OUT_LOCAL / \"final_model_lgb.pkl\"\n",
        "    save_pickle(model, model_pkl_local)\n",
        "    copy_file(model_pkl_local, MODELS_OUT_DRIVE / model_pkl_local.name)\n",
        "\n",
        "    metrics_path_local = MODELS_OUT_LOCAL / \"final_metrics_lgb.csv\"\n",
        "    metrics_df.to_csv(metrics_path_local, index=False)\n",
        "    copy_file(metrics_path_local, MODELS_OUT_DRIVE / metrics_path_local.name)\n",
        "\n",
        "    # Predictions -> predictions/lgb/\n",
        "    PRED_LGB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / \"lgb\")\n",
        "    PRED_LGB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / \"lgb\")\n",
        "\n",
        "    preds_valid_path_local = PRED_LGB_LOCAL / \"predictions_valid.csv\"\n",
        "    preds_valid_score_df.to_csv(preds_valid_path_local, index=False)\n",
        "    copy_file(preds_valid_path_local, PRED_LGB_DRIVE / preds_valid_path_local.name)\n",
        "\n",
        "    preds_test_path_local = PRED_LGB_LOCAL / \"predictions_test.csv\"\n",
        "    preds_test_df.to_csv(preds_test_path_local, index=False)\n",
        "    copy_file(preds_test_path_local, PRED_LGB_DRIVE / preds_test_path_local.name)\n",
        "\n",
        "    print(\"\\n[OK] Saved FINAL LightGBM MODEL artifacts:\")\n",
        "    print(\"  -\", model_json_local.name)\n",
        "    print(\"  -\", model_pkl_local.name)\n",
        "    print(\"  -\", metrics_path_local.name)\n",
        "    print(\"  - predictions/lgb/\", preds_valid_path_local.name)\n",
        "    print(\"  - predictions/lgb/\", preds_test_path_local.name)\n",
        "\n",
        "    # -------------------------\n",
        "    # Tomorrow Prediction + Plot\n",
        "    # -------------------------\n",
        "    PLOT_CFG = RUN_PARAMS[\"plot\"]\n",
        "    N_PLOT = int(PLOT_CFG[\"n_plot\"])\n",
        "    FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n",
        "    DPI = int(PLOT_CFG[\"dpi\"])\n",
        "\n",
        "    last_date = X_test_sel.index[-1]\n",
        "    X_last = X_test_sel.iloc[[-1]]\n",
        "    pred_tomorrow = float(model.predict(X_last)[0])\n",
        "\n",
        "    pred_tomorrow_df = pd.DataFrame([{\n",
        "        \"feature_set\": \"xgb_selected\",\n",
        "        \"last_data_date\": last_date,\n",
        "        \"predicted_for\": \"next_trading_day\",\n",
        "        \"pred_logret\": pred_tomorrow,\n",
        "        \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
        "    }])\n",
        "    pred_tomorrow_df.to_csv(PRED_LGB_LOCAL / \"tomorrow.csv\", index=False)\n",
        "    copy_file(PRED_LGB_LOCAL / \"tomorrow.csv\", PRED_LGB_DRIVE / \"tomorrow.csv\")\n",
        "\n",
        "    hist_df = pd.DataFrame({\n",
        "        \"date\": X_test_sel.index,\n",
        "        \"actual\": y_test.values,\n",
        "        \"y_pred\": pred_model_test,\n",
        "    }).set_index(\"date\")\n",
        "\n",
        "    hist_tail = hist_df.tail(N_PLOT).copy()\n",
        "    hist_tail.to_csv(PRED_LGB_LOCAL / \"backtest.csv\")\n",
        "    copy_file(PRED_LGB_LOCAL / \"backtest.csv\", PRED_LGB_DRIVE / \"backtest.csv\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
        "    ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n",
        "    ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted\")\n",
        "    pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n",
        "    ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n",
        "    ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
        "    ax.set_title(f\"LightGBM Predictions â€” last {len(hist_tail)} days + tomorrow\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Log Return\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PRED_LGB_LOCAL / \"plot.png\", dpi=DPI)\n",
        "    copy_file(PRED_LGB_LOCAL / \"plot.png\", PRED_LGB_DRIVE / \"plot.png\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n",
        "    print(f\"[OK] Saved: predictions/lgb/ (tomorrow.csv, backtest.csv, plot.png)\")\n",
        "\n",
        "    # Store for later comparison\n",
        "    LGB_METRICS = metrics_df\n",
        "\n",
        "    print(\"[OK] BLOCK 25B complete.\")\n",
        "else:\n",
        "    LGB_METRICS = None\n",
        "    print(\"[SKIP] LightGBM section skipped\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-1jrVGkbSAXK",
      "metadata": {
        "id": "-1jrVGkbSAXK"
      },
      "source": [
        "---\n",
        "# SECTION 8: LSTM & GRU Neural Networks\n",
        "\n",
        "**Train and predict with LSTM and GRU**\n",
        "\n",
        "**Blocks:** 26-27"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7a68cb",
      "metadata": {
        "id": "6a7a68cb"
      },
      "source": [
        "## BLOCK 26 â€” NEURAL NETWORK TRAINING (LSTM + GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "2634df7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2634df7b",
        "outputId": "f4e651ee-8052-4e2f-8118-40238db60f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Neural Network output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/models\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/models\n",
            "  [LOAD] y_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_test.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_test.pkl <- data/processed (LOCAL)\n",
            "\n",
            "######################################################################\n",
            "# TRAINING LSTM MODELS\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Training LSTM with feature set: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_40.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 40) | VALID=(77, 40) | TEST=(87, 40)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 40) | VALID_ES=(31, 7, 40) | VALID_SCORE=(34, 7, 40) | TEST=(81, 7, 40)\n",
            "[INFO] Model built: 1799 parameters\n",
            "[INFO] Training LSTM (epochs=50, batch_size=4, patience=5)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026797\n",
            "\n",
            "[RESULT] LSTM | neural_40 | n_features=40\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026797 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.020354 | DirAcc=0.4444\n",
            "  Improvement (VALID):  -0.000571\n",
            "[OK] Saved: lstm_neural_40.keras, lstm_neural_40_scaler.pkl\n",
            "[OK] Predictions: predictions/lstm_neural_40/\n",
            "\n",
            "============================================================\n",
            "[INFO] Training LSTM with feature set: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_80.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 80) | VALID=(77, 80) | TEST=(87, 80)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 80) | VALID_ES=(31, 7, 80) | VALID_SCORE=(34, 7, 80) | TEST=(81, 7, 80)\n",
            "[INFO] Model built: 3079 parameters\n",
            "[INFO] Training LSTM (epochs=50, batch_size=4, patience=5)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026733\n",
            "\n",
            "[RESULT] LSTM | neural_80 | n_features=80\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026733 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.020814 | DirAcc=0.4074\n",
            "  Improvement (VALID):  -0.000507\n",
            "[OK] Saved: lstm_neural_80.keras, lstm_neural_80_scaler.pkl\n",
            "[OK] Predictions: predictions/lstm_neural_80/\n",
            "\n",
            "============================================================\n",
            "[INFO] LSTM TRAINING SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  feature_set  n_features  model_valid_wrmse  model_test_wrmse  \\\n",
              "0   neural_40          40           0.026797          0.020354   \n",
              "1   neural_80          80           0.026733          0.020814   \n",
              "\n",
              "   valid_wrmse_improvement  \n",
              "0                -0.000571  \n",
              "1                -0.000507  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-929ed03b-fee2-4e3a-9045-4e1dc93f09e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>model_valid_wrmse</th>\n",
              "      <th>model_test_wrmse</th>\n",
              "      <th>valid_wrmse_improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.026797</td>\n",
              "      <td>0.020354</td>\n",
              "      <td>-0.000571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.026733</td>\n",
              "      <td>0.020814</td>\n",
              "      <td>-0.000507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-929ed03b-fee2-4e3a-9045-4e1dc93f09e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-929ed03b-fee2-4e3a-9045-4e1dc93f09e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-929ed03b-fee2-4e3a-9045-4e1dc93f09e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"[OK] BLOCK 26 complete\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\",\n          \"neural_40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_valid_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.561926757982046e-05,\n        \"min\": 0.02673267269714521,\n        \"max\": 0.026797188084062118,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.02673267269714521,\n          0.026797188084062118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00032523703587298197,\n        \"min\": 0.020353587435204816,\n        \"max\": 0.020813542062322412,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.020813542062322412,\n          0.020353587435204816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_wrmse_improvement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.561926757982046e-05,\n        \"min\": -0.0005714803299165516,\n        \"max\": -0.0005069649429996423,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.0005069649429996423,\n          -0.0005714803299165516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved LSTM summary: lstm_summary.csv\n",
            "\n",
            "######################################################################\n",
            "# TRAINING GRU MODELS\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Training GRU with feature set: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_40.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 40) | VALID=(77, 40) | TEST=(87, 40)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 40) | VALID_ES=(31, 7, 40) | VALID_SCORE=(34, 7, 40) | TEST=(81, 7, 40)\n",
            "[INFO] Model built: 1391 parameters\n",
            "[INFO] Training GRU (epochs=50, batch_size=4, patience=5)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.020328\n",
            "\n",
            "[RESULT] GRU | neural_40 | n_features=40\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.020328 | DirAcc=0.5588\n",
            "  MODEL    TEST:        wRMSE=0.028567 | DirAcc=0.4321\n",
            "  Improvement (VALID):  0.005897\n",
            "[OK] Saved: gru_neural_40.keras, gru_neural_40_scaler.pkl\n",
            "[OK] Predictions: predictions/gru_neural_40/\n",
            "\n",
            "============================================================\n",
            "[INFO] Training GRU with feature set: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_80.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 80) | VALID=(77, 80) | TEST=(87, 80)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 80) | VALID_ES=(31, 7, 80) | VALID_SCORE=(34, 7, 80) | TEST=(81, 7, 80)\n",
            "[INFO] Model built: 2351 parameters\n",
            "[INFO] Training GRU (epochs=50, batch_size=4, patience=5)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026710\n",
            "\n",
            "[RESULT] GRU | neural_80 | n_features=80\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026710 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.021135 | DirAcc=0.4321\n",
            "  Improvement (VALID):  -0.000484\n",
            "[OK] Saved: gru_neural_80.keras, gru_neural_80_scaler.pkl\n",
            "[OK] Predictions: predictions/gru_neural_80/\n",
            "\n",
            "============================================================\n",
            "[INFO] GRU TRAINING SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  feature_set  n_features  model_valid_wrmse  model_test_wrmse  \\\n",
              "0   neural_40          40           0.020328          0.028567   \n",
              "1   neural_80          80           0.026710          0.021135   \n",
              "\n",
              "   valid_wrmse_improvement  \n",
              "0                 0.005897  \n",
              "1                -0.000484  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab7cc120-289a-47c3-97e9-20d04979402e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>model_valid_wrmse</th>\n",
              "      <th>model_test_wrmse</th>\n",
              "      <th>valid_wrmse_improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.020328</td>\n",
              "      <td>0.028567</td>\n",
              "      <td>0.005897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.026710</td>\n",
              "      <td>0.021135</td>\n",
              "      <td>-0.000484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab7cc120-289a-47c3-97e9-20d04979402e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab7cc120-289a-47c3-97e9-20d04979402e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab7cc120-289a-47c3-97e9-20d04979402e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"[OK] BLOCK 26 complete\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\",\n          \"neural_40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_valid_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004512169994030846,\n        \"min\": 0.02032844856243219,\n        \"max\": 0.02670962056372354,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.02670962056372354,\n          0.02032844856243219\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005255796672574832,\n        \"min\": 0.021134607604220645,\n        \"max\": 0.028567426539651358,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.021134607604220645,\n          0.028567426539651358\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_wrmse_improvement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004512169994030846,\n        \"min\": -0.00048391280957797364,\n        \"max\": 0.005897259191713376,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.00048391280957797364,\n          0.005897259191713376\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved GRU summary: gru_summary.csv\n",
            "[OK] BLOCK 26 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if not TF_AVAILABLE:\n",
        "    print(\"[SKIP] BLOCK 26 â€” TensorFlow not available\")\n",
        "else:\n",
        "    # Config\n",
        "    MODEL_TYPES = [\"lstm\", \"gru\"]\n",
        "\n",
        "    # Directories\n",
        "    # Fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE\n",
        "    RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
        "    FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "    RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
        "    FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "    MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n",
        "    MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n",
        "\n",
        "    print(\"[INFO] Neural Network output dirs:\")\n",
        "    print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n",
        "    print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n",
        "\n",
        "    # -------------------------\n",
        "    # Load shared data (y, weights)\n",
        "    # -------------------------\n",
        "    y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    y_train = y_train_t1.astype(float).to_numpy()\n",
        "    y_valid = y_valid_t1.astype(float).to_numpy()\n",
        "    y_test = y_test_t1.astype(float).to_numpy()\n",
        "\n",
        "    w_train_np = np.asarray(w_train, dtype=float)\n",
        "    w_valid_np = np.asarray(w_valid, dtype=float)\n",
        "    w_test_np = np.asarray(w_test, dtype=float)\n",
        "\n",
        "    # -------------------------\n",
        "    # Metrics\n",
        "    # -------------------------\n",
        "    # Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
        "\n",
        "    # -------------------------\n",
        "    # VALID split function (date-based)\n",
        "    # -------------------------\n",
        "    HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
        "    VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
        "    VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
        "    VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
        "    VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
        "\n",
        "    def split_valid_es_score_nn(Xv_df, yv, wv):\n",
        "        \"\"\"Split validation into ES and SCORE sets.\"\"\"\n",
        "        if not isinstance(Xv_df.index, pd.DatetimeIndex):\n",
        "            return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "        yv_s = pd.Series(yv, index=Xv_df.index)\n",
        "        wv_s = pd.Series(wv, index=Xv_df.index)\n",
        "\n",
        "        es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n",
        "        sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n",
        "        m_es = (Xv_df.index >= es_start) & (Xv_df.index <= es_end)\n",
        "        m_sc = (Xv_df.index >= sc_start) & (Xv_df.index <= sc_end)\n",
        "        mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n",
        "\n",
        "        X_es, X_sc = Xv_df.loc[m_es], Xv_df.loc[m_sc]\n",
        "        y_es, y_sc = yv_s.loc[m_es].to_numpy(float), yv_s.loc[m_sc].to_numpy(float)\n",
        "        w_es, w_sc = wv_s.loc[m_es].to_numpy(float), wv_s.loc[m_sc].to_numpy(float)\n",
        "\n",
        "        if len(X_es) > 0 and len(X_sc) > 0:\n",
        "            return (X_es, y_es, w_es), (X_sc, y_sc, w_sc), mode_str\n",
        "        return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "    # -------------------------\n",
        "    # Sequence creation function\n",
        "    # -------------------------\n",
        "    def make_sequences_eod_nn(X_2d, y_1d, w_1d, idx, lookback, stride=1):\n",
        "        \"\"\"Create sequences: use X up to day t (inclusive) -> predict y[t].\"\"\"\n",
        "        X_2d, y_1d, w_1d = np.asarray(X_2d), np.asarray(y_1d), np.asarray(w_1d)\n",
        "        N, F = X_2d.shape\n",
        "        if N < lookback:\n",
        "            raise ValueError(f\"[ERROR] Not enough rows N={N} for lookback={lookback}.\")\n",
        "        X_seq, y_seq, w_seq, idx_seq = [], [], [], []\n",
        "        for t in range(lookback - 1, N, stride):\n",
        "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
        "            y_seq.append(y_1d[t])\n",
        "            w_seq.append(w_1d[t])\n",
        "            idx_seq.append(idx[t])\n",
        "        return (np.asarray(X_seq, dtype=np.float32),\n",
        "                np.asarray(y_seq, dtype=np.float32),\n",
        "                np.asarray(w_seq, dtype=np.float32),\n",
        "                pd.DatetimeIndex(idx_seq))\n",
        "\n",
        "    # -------------------------\n",
        "    # Loop over model types (LSTM, GRU)\n",
        "    # -------------------------\n",
        "    for model_type in MODEL_TYPES:\n",
        "        NN_CFG = RUN_PARAMS[model_type]\n",
        "        LOOKBACK = int(NN_CFG[\"lookback\"])\n",
        "        STRIDE = int(NN_CFG[\"stride\"])\n",
        "        UNITS_1 = int(NN_CFG[\"units_1\"])\n",
        "        UNITS_2 = int(NN_CFG[\"units_2\"])\n",
        "        DENSE_UNITS = int(NN_CFG[\"dense_units\"])\n",
        "        DROPOUT = float(NN_CFG[\"dropout\"])\n",
        "        LR = float(NN_CFG[\"learning_rate\"])\n",
        "        CLIPNORM = float(NN_CFG[\"clipnorm\"])\n",
        "        DENSE_ACT = NN_CFG[\"dense_activation\"]\n",
        "        OUTPUT_ACT = NN_CFG[\"output_activation\"]\n",
        "        EPOCHS = int(NN_CFG[\"epochs\"])\n",
        "        BATCH_SIZE = int(NN_CFG[\"batch_size\"])\n",
        "        PATIENCE = int(NN_CFG[\"patience\"])\n",
        "        RANDOM_SEED = int(NN_CFG[\"random_state\"])\n",
        "        FEATURE_SETS = list(NN_CFG[\"feature_sets\"])  # Copy to allow modification\n",
        "\n",
        "        # Add SHAP feature set if configured\n",
        "        SHAP_CFG = RUN_PARAMS.get(\"shap\", {})\n",
        "        SHAP_TOP_N = SHAP_CFG.get(\"top_n_features\", None)\n",
        "        if SHAP_TOP_N:\n",
        "            shap_features_file = f\"shap_top_{SHAP_TOP_N}_features.pkl\"\n",
        "            shap_path = None\n",
        "            for p in [FALLBACK_PROC_LOCAL / shap_features_file, FALLBACK_PROC_DRIVE / shap_features_file]:\n",
        "                if p.exists():\n",
        "                    shap_path = p\n",
        "                    break\n",
        "\n",
        "            if shap_path:\n",
        "                shap_features = load_pickle(shap_path)\n",
        "                shap_fs_name = f\"shap_top_{SHAP_TOP_N}\"\n",
        "\n",
        "                # Load full X matrices and filter to SHAP features\n",
        "                X_train_full = load_with_fallback(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "                X_valid_full = load_with_fallback(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "                X_test_full = load_with_fallback(\"X_test_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "                # Filter to SHAP features\n",
        "                X_train_shap = X_train_full[shap_features]\n",
        "                X_valid_shap = X_valid_full[shap_features]\n",
        "                X_test_shap = X_test_full[shap_features]\n",
        "\n",
        "                # Save as new feature set files\n",
        "                save_pickle(X_train_shap, FALLBACK_PROC_LOCAL / f\"X_train_{shap_fs_name}.pkl\")\n",
        "                save_pickle(X_valid_shap, FALLBACK_PROC_LOCAL / f\"X_valid_{shap_fs_name}.pkl\")\n",
        "                save_pickle(X_test_shap, FALLBACK_PROC_LOCAL / f\"X_test_{shap_fs_name}.pkl\")\n",
        "                copy_file(FALLBACK_PROC_LOCAL / f\"X_train_{shap_fs_name}.pkl\", FALLBACK_PROC_DRIVE / f\"X_train_{shap_fs_name}.pkl\")\n",
        "                copy_file(FALLBACK_PROC_LOCAL / f\"X_valid_{shap_fs_name}.pkl\", FALLBACK_PROC_DRIVE / f\"X_valid_{shap_fs_name}.pkl\")\n",
        "                copy_file(FALLBACK_PROC_LOCAL / f\"X_test_{shap_fs_name}.pkl\", FALLBACK_PROC_DRIVE / f\"X_test_{shap_fs_name}.pkl\")\n",
        "\n",
        "                # Add to feature sets\n",
        "                if shap_fs_name not in FEATURE_SETS:\n",
        "                    FEATURE_SETS.append(shap_fs_name)\n",
        "                print(f\"[INFO] Added SHAP top {SHAP_TOP_N} to feature sets: {FEATURE_SETS}\")\n",
        "            else:\n",
        "                print(f\"[WARN] {shap_features_file} not found - run XGBoost with SHAP first\")\n",
        "\n",
        "        all_nn_results = []\n",
        "\n",
        "        print(f\"\\n{'#'*70}\")\n",
        "        print(f\"# TRAINING {model_type.upper()} MODELS\")\n",
        "        print(f\"{'#'*70}\")\n",
        "\n",
        "        for feature_set in FEATURE_SETS:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"[INFO] Training {model_type.upper()} with feature set: {feature_set}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Load feature-specific X matrices\n",
        "            X_train_nn = load_with_fallback(f\"X_train_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "            X_valid_nn = load_with_fallback(f\"X_valid_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "            X_test_nn = load_with_fallback(f\"X_test_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "            n_features = X_train_nn.shape[1]\n",
        "            print(f\"[INFO] Shapes: TRAIN={X_train_nn.shape} | VALID={X_valid_nn.shape} | TEST={X_test_nn.shape}\")\n",
        "\n",
        "            # Split VALID -> ES + SCORE\n",
        "            (X_valid_es_df, y_valid_es, w_valid_es), (X_valid_sc_df, y_valid_sc, w_valid_sc), valid_mode = split_valid_es_score_nn(\n",
        "                X_valid_nn, y_valid, w_valid_np\n",
        "            )\n",
        "            print(f\"[INFO] VALID mode: {valid_mode}\")\n",
        "\n",
        "            # Scaling (fit on TRAIN only)\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train_nn.values)\n",
        "            X_valid_es_scaled = scaler.transform(X_valid_es_df.values)\n",
        "            X_valid_sc_scaled = scaler.transform(X_valid_sc_df.values)\n",
        "            X_test_scaled = scaler.transform(X_test_nn.values)\n",
        "\n",
        "            # Create sequences\n",
        "            Xtr_seq, ytr_seq, wtr_seq, idx_tr = make_sequences_eod_nn(X_train_scaled, y_train, w_train_np, X_train_nn.index, LOOKBACK, STRIDE)\n",
        "            Xes_seq, yes_seq, wes_seq, idx_es = make_sequences_eod_nn(X_valid_es_scaled, y_valid_es, w_valid_es, X_valid_es_df.index, LOOKBACK, STRIDE)\n",
        "            Xsc_seq, ysc_seq, wsc_seq, idx_sc = make_sequences_eod_nn(X_valid_sc_scaled, y_valid_sc, w_valid_sc, X_valid_sc_df.index, LOOKBACK, STRIDE)\n",
        "            Xte_seq, yte_seq, wte_seq, idx_te = make_sequences_eod_nn(X_test_scaled, y_test, w_test_np, X_test_nn.index, LOOKBACK, STRIDE)\n",
        "\n",
        "            print(f\"[INFO] Sequence shapes: TRAIN={Xtr_seq.shape} | VALID_ES={Xes_seq.shape} | VALID_SCORE={Xsc_seq.shape} | TEST={Xte_seq.shape}\")\n",
        "\n",
        "            # Build model\n",
        "            tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
        "\n",
        "            inp = keras.Input(shape=(LOOKBACK, n_features))\n",
        "            if model_type == \"lstm\":\n",
        "                x = layers.LSTM(UNITS_1, return_sequences=True, dropout=DROPOUT)(inp)\n",
        "            else:\n",
        "                x = layers.GRU(UNITS_1, return_sequences=True, dropout=DROPOUT)(inp)\n",
        "            x = layers.LayerNormalization()(x)\n",
        "            if model_type == \"lstm\":\n",
        "                x = layers.LSTM(UNITS_2, return_sequences=False, dropout=DROPOUT)(x)\n",
        "            else:\n",
        "                x = layers.GRU(UNITS_2, return_sequences=False, dropout=DROPOUT)(x)\n",
        "            x = layers.Dense(DENSE_UNITS, activation=DENSE_ACT)(x)\n",
        "            x = layers.Dropout(DROPOUT)(x)\n",
        "            out = layers.Dense(1, activation=OUTPUT_ACT)(x)\n",
        "\n",
        "            model = keras.Model(inp, out)\n",
        "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=LR, clipnorm=CLIPNORM), loss=NN_CFG[\"loss\"])\n",
        "            print(f\"[INFO] Model built: {model.count_params()} parameters\")\n",
        "\n",
        "            # Custom callback for SCORE set tracking\n",
        "            class ScoreSetCallback(keras.callbacks.Callback):\n",
        "                def __init__(self, X_score, y_score, w_score):\n",
        "                    super().__init__()\n",
        "                    self.Xs, self.ys, self.ws = X_score, y_score, w_score\n",
        "                    self.best = np.inf\n",
        "                    self.best_weights = None\n",
        "                def on_epoch_end(self, epoch, logs=None):\n",
        "                    pred = self.model.predict(self.Xs, verbose=0).reshape(-1)\n",
        "                    score = w_rmse(self.ys, pred, self.ws)\n",
        "                    if score < self.best:\n",
        "                        self.best = score\n",
        "                        self.best_weights = self.model.get_weights()\n",
        "\n",
        "            score_cb = ScoreSetCallback(Xsc_seq, ysc_seq, wsc_seq)\n",
        "            callbacks = [\n",
        "                keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
        "                score_cb,\n",
        "            ]\n",
        "\n",
        "            # Train\n",
        "            print(f\"[INFO] Training {model_type.upper()} (epochs={EPOCHS}, batch_size={BATCH_SIZE}, patience={PATIENCE})...\")\n",
        "            history = model.fit(\n",
        "                Xtr_seq, ytr_seq,\n",
        "                sample_weight=wtr_seq,\n",
        "                validation_data=(Xes_seq, yes_seq, wes_seq),\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                verbose=0,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "            # Restore best weights by SCORE\n",
        "            if score_cb.best_weights is not None:\n",
        "                model.set_weights(score_cb.best_weights)\n",
        "                print(f\"[INFO] Restored best weights by VALID_SCORE wRMSE = {score_cb.best:.6f}\")\n",
        "\n",
        "            # Evaluate\n",
        "            pred_sc = model.predict(Xsc_seq, verbose=0).reshape(-1)\n",
        "            pred_te = model.predict(Xte_seq, verbose=0).reshape(-1)\n",
        "            baseline_sc, baseline_te = np.zeros_like(ysc_seq), np.zeros_like(yte_seq)\n",
        "\n",
        "            # Metrics\n",
        "            results = {\n",
        "                \"model_type\": model_type,\n",
        "                \"feature_set\": feature_set,\n",
        "                \"n_features\": n_features,\n",
        "                \"valid_mode\": valid_mode,\n",
        "                \"epochs_trained\": len(history.history[\"loss\"]),\n",
        "                # Config params for comparison\n",
        "                f\"{model_type}_units_1\": UNITS_1,\n",
        "                f\"{model_type}_units_2\": UNITS_2,\n",
        "                \"dropout\": DROPOUT,\n",
        "                \"learning_rate\": LR,\n",
        "                \"lookback\": LOOKBACK,\n",
        "                \"epochs\": EPOCHS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"baseline_valid_wrmse\": w_rmse(ysc_seq, baseline_sc, wsc_seq),\n",
        "                \"baseline_valid_diracc\": dir_acc(ysc_seq, baseline_sc),\n",
        "                \"baseline_test_wrmse\": w_rmse(yte_seq, baseline_te, wte_seq),\n",
        "                \"baseline_test_diracc\": dir_acc(yte_seq, baseline_te),\n",
        "                \"model_valid_wrmse\": w_rmse(ysc_seq, pred_sc, wsc_seq),\n",
        "                \"model_valid_wmae\": w_mae(ysc_seq, pred_sc, wsc_seq),\n",
        "                \"model_valid_diracc\": dir_acc(ysc_seq, pred_sc),\n",
        "                \"model_test_wrmse\": w_rmse(yte_seq, pred_te, wte_seq),\n",
        "                \"model_test_wmae\": w_mae(yte_seq, pred_te, wte_seq),\n",
        "                \"model_test_diracc\": dir_acc(yte_seq, pred_te),\n",
        "            }\n",
        "            results[\"valid_wrmse_improvement\"] = results[\"baseline_valid_wrmse\"] - results[\"model_valid_wrmse\"]\n",
        "            results[\"test_wrmse_improvement\"] = results[\"baseline_test_wrmse\"] - results[\"model_test_wrmse\"]\n",
        "            all_nn_results.append(results)\n",
        "\n",
        "            print(f\"\\n[RESULT] {model_type.upper()} | {feature_set} | n_features={n_features}\")\n",
        "            print(f\"  BASELINE VALID_SCORE: wRMSE={results['baseline_valid_wrmse']:.6f} | DirAcc={results['baseline_valid_diracc']:.4f}\")\n",
        "            print(f\"  MODEL    VALID_SCORE: wRMSE={results['model_valid_wrmse']:.6f} | DirAcc={results['model_valid_diracc']:.4f}\")\n",
        "            print(f\"  MODEL    TEST:        wRMSE={results['model_test_wrmse']:.6f} | DirAcc={results['model_test_diracc']:.4f}\")\n",
        "            print(f\"  Improvement (VALID):  {results['valid_wrmse_improvement']:.6f}\")\n",
        "\n",
        "            # Save model + scaler + config\n",
        "            model_path = MODELS_OUT_LOCAL / f\"{model_type}_{feature_set}.keras\"\n",
        "            scaler_path = MODELS_OUT_LOCAL / f\"{model_type}_{feature_set}_scaler.pkl\"\n",
        "            config_path = MODELS_OUT_LOCAL / f\"{model_type}_{feature_set}_config.json\"\n",
        "            model.save(model_path)\n",
        "            save_pickle(scaler, scaler_path)\n",
        "            save_json(NN_CFG, config_path)\n",
        "            copy_file(model_path, MODELS_OUT_DRIVE / model_path.name)\n",
        "            copy_file(scaler_path, MODELS_OUT_DRIVE / scaler_path.name)\n",
        "            copy_file(config_path, MODELS_OUT_DRIVE / config_path.name)\n",
        "\n",
        "            # Predictions -> predictions/{model_type}_{feature_set}/\n",
        "            PRED_NN_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
        "            PRED_NN_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
        "\n",
        "            preds_valid_df = pd.DataFrame({\n",
        "                \"date\": idx_sc, \"actual\": ysc_seq, \"baseline_zero\": baseline_sc,\n",
        "                \"predicted\": pred_sc, \"sample_weight\": wsc_seq,\n",
        "            }).reset_index(drop=True)\n",
        "            preds_test_df = pd.DataFrame({\n",
        "                \"date\": idx_te, \"actual\": yte_seq, \"baseline_zero\": baseline_te,\n",
        "                \"predicted\": pred_te, \"sample_weight\": wte_seq,\n",
        "            }).reset_index(drop=True)\n",
        "\n",
        "            preds_valid_df.to_csv(PRED_NN_LOCAL / \"predictions_valid.csv\", index=False)\n",
        "            preds_test_df.to_csv(PRED_NN_LOCAL / \"predictions_test.csv\", index=False)\n",
        "            copy_file(PRED_NN_LOCAL / \"predictions_valid.csv\", PRED_NN_DRIVE / \"predictions_valid.csv\")\n",
        "            copy_file(PRED_NN_LOCAL / \"predictions_test.csv\", PRED_NN_DRIVE / \"predictions_test.csv\")\n",
        "\n",
        "            print(f\"[OK] Saved: {model_path.name}, {scaler_path.name}\")\n",
        "            print(f\"[OK] Predictions: predictions/{model_type}_{feature_set}/\")\n",
        "\n",
        "        # Summary for this model type\n",
        "        nn_results_df = pd.DataFrame(all_nn_results)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[INFO] {model_type.upper()} TRAINING SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        display(nn_results_df[[\"feature_set\", \"n_features\", \"model_valid_wrmse\", \"model_test_wrmse\", \"valid_wrmse_improvement\"]])\n",
        "\n",
        "        summary_path = MODELS_OUT_LOCAL / f\"{model_type}_summary.csv\"\n",
        "        nn_results_df.to_csv(summary_path, index=False)\n",
        "        copy_file(summary_path, MODELS_OUT_DRIVE / summary_path.name)\n",
        "        print(f\"[OK] Saved {model_type.upper()} summary: {summary_path.name}\")\n",
        "\n",
        "    print(\"[OK] BLOCK 26 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef512111",
      "metadata": {
        "id": "ef512111"
      },
      "source": [
        "## BLOCK 27 â€” NEURAL NETWORK PREDICT TOMORROW + BACKTEST PLOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "5855a53a",
      "metadata": {
        "id": "5855a53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d83afa97-e338-4cee-b26c-66f2a76592ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Neural Network Predictions output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/predictions\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/predictions\n",
            "  [LOAD] y_test.pkl <- RUN_ID (LOCAL)\n",
            "  [LOAD] weights_test.pkl <- RUN_ID (LOCAL)\n",
            "\n",
            "######################################################################\n",
            "# PREDICTING WITH LSTM MODELS\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with LSTM: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_40.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=lstm_neural_40.keras | X_test=(87, 40)\n",
            "[INFO] Sequences: (81, 7, 40)\n",
            "[INFO] TEST wRMSE=0.020354 | DirAcc=0.4444\n",
            "[INFO] Tomorrow prediction: -0.001323 (-0.1322%)\n",
            "[OK] Saved: predictions/lstm_neural_40/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with LSTM: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_80.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=lstm_neural_80.keras | X_test=(87, 80)\n",
            "[INFO] Sequences: (81, 7, 80)\n",
            "[INFO] TEST wRMSE=0.020814 | DirAcc=0.4074\n",
            "[INFO] Tomorrow prediction: -0.001180 (-0.1179%)\n",
            "[OK] Saved: predictions/lstm_neural_80/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] LSTM PREDICTION SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  model_type feature_set  n_features  test_wrmse  test_dir_acc last_data_date  \\\n",
              "0       lstm   neural_40          40    0.020354      0.444444     2026-01-14   \n",
              "1       lstm   neural_80          80    0.020814      0.407407     2026-01-14   \n",
              "\n",
              "   pred_tomorrow_logret  pred_tomorrow_pct  \n",
              "0             -0.001323          -0.132202  \n",
              "1             -0.001180          -0.117881  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84cbae4b-318f-4e63-981a-306d61249b6f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_type</th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>test_wrmse</th>\n",
              "      <th>test_dir_acc</th>\n",
              "      <th>last_data_date</th>\n",
              "      <th>pred_tomorrow_logret</th>\n",
              "      <th>pred_tomorrow_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lstm</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.020354</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.001323</td>\n",
              "      <td>-0.132202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lstm</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.020814</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.001180</td>\n",
              "      <td>-0.117881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84cbae4b-318f-4e63-981a-306d61249b6f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84cbae4b-318f-4e63-981a-306d61249b6f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84cbae4b-318f-4e63-981a-306d61249b6f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_3f9ee831-29d0-43ce-bec0-75366c46b0c7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pred_summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3f9ee831-29d0-43ce-bec0-75366c46b0c7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pred_summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_summary_df",
              "summary": "{\n  \"name\": \"pred_summary_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"lstm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0003252370109591008,\n        \"min\": 0.020353587310277896,\n        \"max\": 0.020813541902161944,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.020813541902161944\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_dir_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.026189140043946204,\n        \"min\": 0.4074074074074074,\n        \"max\": 0.4444444444444444,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.4074074074074074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_data_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-01-14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_logret\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00010139091185609892,\n        \"min\": -0.0013228983152657747,\n        \"max\": -0.0011795099126175046,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.0011795099126175046\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010126413054805985,\n        \"min\": -0.13220236710204894,\n        \"max\": -0.11788145642175035,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.11788145642175035\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved summary: lstm_predictions_summary.csv\n",
            "\n",
            "######################################################################\n",
            "# PREDICTING WITH GRU MODELS\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with GRU: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_40.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=gru_neural_40.keras | X_test=(87, 40)\n",
            "[INFO] Sequences: (81, 7, 40)\n",
            "[INFO] TEST wRMSE=0.028567 | DirAcc=0.4321\n",
            "[INFO] Tomorrow prediction: -0.001245 (-0.1244%)\n",
            "[OK] Saved: predictions/gru_neural_40/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with GRU: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_80.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=gru_neural_80.keras | X_test=(87, 80)\n",
            "[INFO] Sequences: (81, 7, 80)\n",
            "[INFO] TEST wRMSE=0.021135 | DirAcc=0.4321\n",
            "[INFO] Tomorrow prediction: -0.001128 (-0.1127%)\n",
            "[OK] Saved: predictions/gru_neural_80/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] GRU PREDICTION SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  model_type feature_set  n_features  test_wrmse  test_dir_acc last_data_date  \\\n",
              "0        gru   neural_40          40    0.028567      0.432099     2026-01-14   \n",
              "1        gru   neural_80          80    0.021135      0.432099     2026-01-14   \n",
              "\n",
              "   pred_tomorrow_logret  pred_tomorrow_pct  \n",
              "0             -0.001245          -0.124401  \n",
              "1             -0.001128          -0.112731  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dae1805e-ef26-434a-b8c4-147272137904\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_type</th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>test_wrmse</th>\n",
              "      <th>test_dir_acc</th>\n",
              "      <th>last_data_date</th>\n",
              "      <th>pred_tomorrow_logret</th>\n",
              "      <th>pred_tomorrow_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gru</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.028567</td>\n",
              "      <td>0.432099</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.124401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gru</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.021135</td>\n",
              "      <td>0.432099</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.001128</td>\n",
              "      <td>-0.112731</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dae1805e-ef26-434a-b8c4-147272137904')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dae1805e-ef26-434a-b8c4-147272137904 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dae1805e-ef26-434a-b8c4-147272137904');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_b427f054-57ac-49d2-9db6-64698a38f07a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pred_summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b427f054-57ac-49d2-9db6-64698a38f07a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pred_summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_summary_df",
              "summary": "{\n  \"name\": \"pred_summary_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"gru\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005255796581266983,\n        \"min\": 0.02113460791520729,\n        \"max\": 0.028567426721509206,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.02113460791520729\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_dir_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.43209876543209874,\n        \"max\": 0.43209876543209874,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.43209876543209874\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_data_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-01-14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_logret\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.261687806833707e-05,\n        \"min\": -0.0012447797926142812,\n        \"max\": -0.0011279418831691146,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.0011279418831691146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008251892280324625,\n        \"min\": -0.12440053756077732,\n        \"max\": -0.11273059958270039,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.11273059958270039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved summary: gru_predictions_summary.csv\n",
            "[OK] BLOCK 27 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if not TF_AVAILABLE:\n",
        "    print(\"[SKIP] BLOCK 27 â€” TensorFlow not available\")\n",
        "else:\n",
        "    MODEL_TYPES = [\"lstm\", \"gru\"]\n",
        "    PLOT_CFG = RUN_PARAMS[\"plot\"]\n",
        "    N_PLOT = int(PLOT_CFG[\"n_plot\"])\n",
        "    FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n",
        "    DPI = int(PLOT_CFG[\"dpi\"])\n",
        "\n",
        "    # Directories\n",
        "    PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
        "    MODELS_DIR_LOCAL = Path(LOCAL_PATHS[\"models_dir\"])\n",
        "    PRED_OUT_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\")\n",
        "    PRED_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\")\n",
        "\n",
        "    print(\"[INFO] Neural Network Predictions output dirs:\")\n",
        "    print(\"  - LOCAL:\", PRED_OUT_LOCAL)\n",
        "    print(\"  - DRIVE:\", PRED_OUT_DRIVE)\n",
        "\n",
        "    # Sequence creation\n",
        "    def make_sequences_pred_nn(X_2d, y_1d, idx, lookback):\n",
        "        X_2d, y_1d = np.asarray(X_2d), np.asarray(y_1d)\n",
        "        N, F = X_2d.shape\n",
        "        X_seq, y_seq, idx_seq = [], [], []\n",
        "        for t in range(lookback - 1, N):\n",
        "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
        "            y_seq.append(y_1d[t])\n",
        "            idx_seq.append(idx[t])\n",
        "        return (np.asarray(X_seq, dtype=np.float32),\n",
        "                np.asarray(y_seq, dtype=np.float32),\n",
        "                pd.DatetimeIndex(idx_seq))\n",
        "\n",
        "    # Load shared data\n",
        "    y_test = load_with_fallback(\"y_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
        "    w_test = load_with_fallback(\"weights_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
        "    y_test_arr = y_test.astype(float).to_numpy()\n",
        "    w_test_arr = np.asarray(w_test, dtype=float)\n",
        "\n",
        "    # Loop over model types\n",
        "    for model_type in MODEL_TYPES:\n",
        "        NN_CFG = RUN_PARAMS[model_type]\n",
        "        LOOKBACK = int(NN_CFG[\"lookback\"])\n",
        "        FEATURE_SETS = list(NN_CFG[\"feature_sets\"])  # Copy to allow modification\n",
        "\n",
        "        # Add SHAP feature set if configured and model exists\n",
        "        SHAP_CFG = RUN_PARAMS.get(\"shap\", {})\n",
        "        SHAP_TOP_N = SHAP_CFG.get(\"top_n_features\", None)\n",
        "        if SHAP_TOP_N:\n",
        "            shap_fs_name = f\"shap_top_{SHAP_TOP_N}\"\n",
        "            shap_model_path = MODELS_DIR_LOCAL / f\"{model_type}_{shap_fs_name}.keras\"\n",
        "            if shap_model_path.exists() and shap_fs_name not in FEATURE_SETS:\n",
        "                FEATURE_SETS.append(shap_fs_name)\n",
        "                print(f\"[INFO] Added SHAP feature set to predictions: {shap_fs_name}\")\n",
        "\n",
        "        all_pred_results = []\n",
        "\n",
        "        print(f\"\\n{'#'*70}\")\n",
        "        print(f\"# PREDICTING WITH {model_type.upper()} MODELS\")\n",
        "        print(f\"{'#'*70}\")\n",
        "\n",
        "        for feature_set in FEATURE_SETS:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"[INFO] Predicting with {model_type.upper()}: {feature_set}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Load model + scaler\n",
        "            model_path = MODELS_DIR_LOCAL / f\"{model_type}_{feature_set}.keras\"\n",
        "            scaler_path = MODELS_DIR_LOCAL / f\"{model_type}_{feature_set}_scaler.pkl\"\n",
        "\n",
        "            if not model_path.exists():\n",
        "                print(f\"[WARN] Model not found: {model_path}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            model = keras.models.load_model(model_path)\n",
        "            scaler = load_pickle(scaler_path)\n",
        "\n",
        "            # Load X_test\n",
        "            X_test = load_with_fallback(f\"X_test_{feature_set}.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR, use_pandas=True)\n",
        "            n_features = X_test.shape[1]\n",
        "            print(f\"[INFO] Loaded: model={model_path.name} | X_test={X_test.shape}\")\n",
        "\n",
        "            # Scale + Sequences\n",
        "            X_test_scaled = scaler.transform(X_test.values)\n",
        "            X_seq, y_seq, idx_seq = make_sequences_pred_nn(X_test_scaled, y_test_arr, X_test.index, LOOKBACK)\n",
        "            print(f\"[INFO] Sequences: {X_seq.shape}\")\n",
        "\n",
        "            # Predict\n",
        "            pred_seq = model.predict(X_seq, verbose=0).reshape(-1)\n",
        "            hist_df = pd.DataFrame({\"date\": idx_seq, \"actual\": y_seq, \"y_pred\": pred_seq}).set_index(\"date\")\n",
        "\n",
        "            # Tomorrow prediction\n",
        "            last_date = X_test.index[-1]\n",
        "            X_last_window = X_test_scaled[-LOOKBACK:, :].reshape(1, LOOKBACK, -1).astype(np.float32)\n",
        "            pred_tomorrow = float(model.predict(X_last_window, verbose=0).reshape(-1)[0])\n",
        "            pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n",
        "\n",
        "            # Metrics\n",
        "            w_seq = w_test_arr[LOOKBACK - 1:]\n",
        "            w_norm = w_seq / (w_seq.sum() + EPS)\n",
        "            test_wrmse = float(np.sqrt(np.sum(w_norm * (y_seq - pred_seq) ** 2)))\n",
        "            test_dir_acc = float(np.mean((y_seq > 0) == (pred_seq > 0)))\n",
        "\n",
        "            print(f\"[INFO] TEST wRMSE={test_wrmse:.6f} | DirAcc={test_dir_acc:.4f}\")\n",
        "            print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n",
        "\n",
        "            result = {\n",
        "                \"model_type\": model_type,\n",
        "                \"feature_set\": feature_set,\n",
        "                \"n_features\": n_features,\n",
        "                \"test_wrmse\": test_wrmse,\n",
        "                \"test_dir_acc\": test_dir_acc,\n",
        "                \"last_data_date\": str(last_date.date()),\n",
        "                \"pred_tomorrow_logret\": pred_tomorrow,\n",
        "                \"pred_tomorrow_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
        "            }\n",
        "            all_pred_results.append(result)\n",
        "\n",
        "            # Save -> predictions/{model_type}_{feature_set}/\n",
        "            PRED_NN_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
        "            PRED_NN_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
        "\n",
        "            pred_tomorrow_df = pd.DataFrame([{\n",
        "                \"feature_set\": feature_set, \"last_data_date\": last_date,\n",
        "                \"predicted_for\": \"next_trading_day\",\n",
        "                \"pred_logret\": pred_tomorrow,\n",
        "                \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
        "            }])\n",
        "            pred_tomorrow_df.to_csv(PRED_NN_LOCAL / \"tomorrow.csv\", index=False)\n",
        "            copy_file(PRED_NN_LOCAL / \"tomorrow.csv\", PRED_NN_DRIVE / \"tomorrow.csv\")\n",
        "\n",
        "            hist_tail = hist_df.tail(N_PLOT).copy()\n",
        "            hist_tail.to_csv(PRED_NN_LOCAL / \"backtest.csv\")\n",
        "            copy_file(PRED_NN_LOCAL / \"backtest.csv\", PRED_NN_DRIVE / \"backtest.csv\")\n",
        "\n",
        "            # Plot\n",
        "            fig, ax = plt.subplots(figsize=FIGSIZE)\n",
        "            ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n",
        "            ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted (y_pred)\")\n",
        "            ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n",
        "            ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
        "            ax.set_title(f\"{model_type.upper()} Predictions â€” {feature_set} â€” last {len(hist_tail)} days + tomorrow\")\n",
        "            ax.set_xlabel(\"Date\")\n",
        "            ax.set_ylabel(\"Log Return\")\n",
        "            ax.legend(loc=\"upper right\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(PRED_NN_LOCAL / \"plot.png\", dpi=DPI)\n",
        "            copy_file(PRED_NN_LOCAL / \"plot.png\", PRED_NN_DRIVE / \"plot.png\")\n",
        "            plt.close(fig)\n",
        "\n",
        "            print(f\"[OK] Saved: predictions/{model_type}_{feature_set}/ (tomorrow.csv, backtest.csv, plot.png)\")\n",
        "\n",
        "        # Summary for this model type\n",
        "        pred_summary_df = pd.DataFrame(all_pred_results)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[INFO] {model_type.upper()} PREDICTION SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        display(pred_summary_df)\n",
        "\n",
        "        summary_path = PRED_OUT_LOCAL / f\"{model_type}_predictions_summary.csv\"\n",
        "        pred_summary_df.to_csv(summary_path, index=False)\n",
        "        copy_file(summary_path, PRED_OUT_DRIVE / summary_path.name)\n",
        "        print(f\"[OK] Saved summary: {summary_path.name}\")\n",
        "\n",
        "    print(\"[OK] BLOCK 27 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ey5IVR54SAXL",
      "metadata": {
        "id": "Ey5IVR54SAXL"
      },
      "source": [
        "---\n",
        "# SECTION 9: Hybrid Neural Networks\n",
        "\n",
        "**Sequential and Parallel hybrid architectures**\n",
        "\n",
        "**Blocks:** 28-29"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45bccee",
      "metadata": {
        "id": "b45bccee"
      },
      "source": [
        "## BLOCK 28 â€” HYBRID NEURAL NETWORK TRAINING (Sequential + Parallel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "e8be8074",
      "metadata": {
        "id": "e8be8074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5833eb10-85c6-4ae7-91b0-eaad4cba8beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Hybrid Neural Network output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/models\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/models\n",
            "  [LOAD] y_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] y_test.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_train.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_valid.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] weights_test.pkl <- data/processed (LOCAL)\n",
            "\n",
            "######################################################################\n",
            "# TRAINING HYBRID_SEQ â€” Sequential (LSTMâ†’GRU)\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Training HYBRID_SEQ with feature set: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_40.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 40) | VALID=(77, 40) | TEST=(87, 40)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 40) | VALID_ES=(31, 7, 40) | VALID_SCORE=(34, 7, 40) | TEST=(81, 7, 40)\n",
            "[INFO] Model built: 7671 parameters\n",
            "[INFO] Training HYBRID_SEQ (epochs=50, batch_size=4, patience=6)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026550\n",
            "\n",
            "[RESULT] HYBRID_SEQ | neural_40 | n_features=40\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026550 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.020188 | DirAcc=0.4444\n",
            "  Improvement (VALID):  -0.000324\n",
            "[OK] Saved: hybrid_seq_neural_40.keras, hybrid_seq_neural_40_scaler.pkl\n",
            "[OK] Predictions: predictions/hybrid_seq_neural_40/\n",
            "\n",
            "============================================================\n",
            "[INFO] Training HYBRID_SEQ with feature set: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_80.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 80) | VALID=(77, 80) | TEST=(87, 80)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 80) | VALID_ES=(31, 7, 80) | VALID_SCORE=(34, 7, 80) | TEST=(81, 7, 80)\n",
            "[INFO] Model built: 11511 parameters\n",
            "[INFO] Training HYBRID_SEQ (epochs=50, batch_size=4, patience=6)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026575\n",
            "\n",
            "[RESULT] HYBRID_SEQ | neural_80 | n_features=80\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026575 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.020204 | DirAcc=0.4444\n",
            "  Improvement (VALID):  -0.000349\n",
            "[OK] Saved: hybrid_seq_neural_80.keras, hybrid_seq_neural_80_scaler.pkl\n",
            "[OK] Predictions: predictions/hybrid_seq_neural_80/\n",
            "\n",
            "============================================================\n",
            "[INFO] HYBRID_SEQ TRAINING SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  feature_set  n_features  model_valid_wrmse  model_test_wrmse  \\\n",
              "0   neural_40          40           0.026550          0.020188   \n",
              "1   neural_80          80           0.026575          0.020204   \n",
              "\n",
              "   valid_wrmse_improvement  \n",
              "0                -0.000324  \n",
              "1                -0.000349  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2528ece-3fc7-4175-88ed-daadf28abca1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>model_valid_wrmse</th>\n",
              "      <th>model_test_wrmse</th>\n",
              "      <th>valid_wrmse_improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.026550</td>\n",
              "      <td>0.020188</td>\n",
              "      <td>-0.000324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.026575</td>\n",
              "      <td>0.020204</td>\n",
              "      <td>-0.000349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2528ece-3fc7-4175-88ed-daadf28abca1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2528ece-3fc7-4175-88ed-daadf28abca1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2528ece-3fc7-4175-88ed-daadf28abca1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"[OK] BLOCK 28 complete\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\",\n          \"neural_40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_valid_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.72524981705453e-05,\n        \"min\": 0.026550204479442903,\n        \"max\": 0.026574603196340505,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.026574603196340505,\n          0.026550204479442903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1303357571861094e-05,\n        \"min\": 0.020188241634982076,\n        \"max\": 0.020204226996560555,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.020204226996560555,\n          0.020188241634982076\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_wrmse_improvement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.72524981705453e-05,\n        \"min\": -0.00034889544219493876,\n        \"max\": -0.0003244967252973366,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.00034889544219493876,\n          -0.0003244967252973366\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved HYBRID_SEQ summary: hybrid_seq_summary.csv\n",
            "\n",
            "######################################################################\n",
            "# TRAINING HYBRID_PAR â€” Parallel (LSTMâˆ¥GRU)\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Training HYBRID_PAR with feature set: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_40.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_40.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 40) | VALID=(77, 40) | TEST=(87, 40)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 40) | VALID_ES=(31, 7, 40) | VALID_SCORE=(34, 7, 40) | TEST=(81, 7, 40)\n",
            "[INFO] Model built: 6467 parameters\n",
            "[INFO] Training HYBRID_PAR (epochs=50, batch_size=4, patience=6)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026677\n",
            "\n",
            "[RESULT] HYBRID_PAR | neural_40 | n_features=40\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026677 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.020272 | DirAcc=0.4444\n",
            "  Improvement (VALID):  -0.000451\n",
            "[OK] Saved: hybrid_par_neural_40.keras, hybrid_par_neural_40_scaler.pkl\n",
            "[OK] Predictions: predictions/hybrid_par_neural_40/\n",
            "\n",
            "============================================================\n",
            "[INFO] Training HYBRID_PAR with feature set: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_train_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_valid_neural_80.pkl <- data/processed (LOCAL)\n",
            "  [LOAD] X_test_neural_80.pkl <- data/processed (LOCAL)\n",
            "[INFO] Shapes: TRAIN=(347, 80) | VALID=(77, 80) | TEST=(87, 80)\n",
            "[INFO] VALID mode: VALID_ES=2025-05-21:2025-07-15 / VALID_SCORE=2025-07-16:2025-09-10\n",
            "[INFO] Sequence shapes: TRAIN=(341, 7, 80) | VALID_ES=(31, 7, 80) | VALID_SCORE=(34, 7, 80) | TEST=(81, 7, 80)\n",
            "[INFO] Model built: 10947 parameters\n",
            "[INFO] Training HYBRID_PAR (epochs=50, batch_size=4, patience=6)...\n",
            "[INFO] Restored best weights by VALID_SCORE wRMSE = 0.026616\n",
            "\n",
            "[RESULT] HYBRID_PAR | neural_80 | n_features=80\n",
            "  BASELINE VALID_SCORE: wRMSE=0.026226 | DirAcc=0.3824\n",
            "  MODEL    VALID_SCORE: wRMSE=0.026616 | DirAcc=0.3824\n",
            "  MODEL    TEST:        wRMSE=0.021824 | DirAcc=0.4691\n",
            "  Improvement (VALID):  -0.000390\n",
            "[OK] Saved: hybrid_par_neural_80.keras, hybrid_par_neural_80_scaler.pkl\n",
            "[OK] Predictions: predictions/hybrid_par_neural_80/\n",
            "\n",
            "============================================================\n",
            "[INFO] HYBRID_PAR TRAINING SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  feature_set  n_features  model_valid_wrmse  model_test_wrmse  \\\n",
              "0   neural_40          40           0.026677          0.020272   \n",
              "1   neural_80          80           0.026616          0.021824   \n",
              "\n",
              "   valid_wrmse_improvement  \n",
              "0                -0.000451  \n",
              "1                -0.000390  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3a8e57c-8d0e-483f-9a1b-47a1fc0d350d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>model_valid_wrmse</th>\n",
              "      <th>model_test_wrmse</th>\n",
              "      <th>valid_wrmse_improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.026677</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>-0.000451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.026616</td>\n",
              "      <td>0.021824</td>\n",
              "      <td>-0.000390</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3a8e57c-8d0e-483f-9a1b-47a1fc0d350d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3a8e57c-8d0e-483f-9a1b-47a1fc0d350d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3a8e57c-8d0e-483f-9a1b-47a1fc0d350d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"[OK] BLOCK 28 complete\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\",\n          \"neural_40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_valid_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.309054180220247e-05,\n        \"min\": 0.026615836678415265,\n        \"max\": 0.026676775907041945,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.026615836678415265,\n          0.026676775907041945\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001097508382079864,\n        \"min\": 0.020272016136742833,\n        \"max\": 0.02182412737549833,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.02182412737549833,\n          0.020272016136742833\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_wrmse_improvement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.309054180220247e-05,\n        \"min\": -0.0004510681528963785,\n        \"max\": -0.000390128924269699,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.000390128924269699,\n          -0.0004510681528963785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved HYBRID_PAR summary: hybrid_par_summary.csv\n",
            "[OK] BLOCK 28 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if not TF_AVAILABLE:\n",
        "    print(\"[SKIP] BLOCK 28 â€” TensorFlow not available\")\n",
        "else:\n",
        "    # Config\n",
        "    HYBRID_TYPES = [\"hybrid_seq\", \"hybrid_par\"]\n",
        "\n",
        "    # Directories\n",
        "    # Fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE\n",
        "    RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
        "    FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
        "    RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
        "    FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
        "    MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n",
        "    MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n",
        "\n",
        "    print(\"[INFO] Hybrid Neural Network output dirs:\")\n",
        "    print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n",
        "    print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n",
        "\n",
        "    # -------------------------\n",
        "    # Load shared data (y, weights)\n",
        "    # -------------------------\n",
        "    y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "    w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
        "\n",
        "    y_train = y_train_t1.astype(float).to_numpy()\n",
        "    y_valid = y_valid_t1.astype(float).to_numpy()\n",
        "    y_test = y_test_t1.astype(float).to_numpy()\n",
        "\n",
        "    w_train_np = np.asarray(w_train, dtype=float)\n",
        "    w_valid_np = np.asarray(w_valid, dtype=float)\n",
        "    w_test_np = np.asarray(w_test, dtype=float)\n",
        "\n",
        "    # -------------------------\n",
        "    # Metrics\n",
        "    # -------------------------\n",
        "    # Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
        "\n",
        "    # -------------------------\n",
        "    # VALID split function (date-based)\n",
        "    # -------------------------\n",
        "    HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
        "    VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
        "    VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
        "    VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
        "    VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
        "\n",
        "    def split_valid_es_score_hybrid(Xv_df, yv, wv):\n",
        "        if not isinstance(Xv_df.index, pd.DatetimeIndex):\n",
        "            return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "        yv_s = pd.Series(yv, index=Xv_df.index)\n",
        "        wv_s = pd.Series(wv, index=Xv_df.index)\n",
        "\n",
        "        es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n",
        "        sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n",
        "        m_es = (Xv_df.index >= es_start) & (Xv_df.index <= es_end)\n",
        "        m_sc = (Xv_df.index >= sc_start) & (Xv_df.index <= sc_end)\n",
        "        mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n",
        "\n",
        "        X_es, X_sc = Xv_df.loc[m_es], Xv_df.loc[m_sc]\n",
        "        y_es, y_sc = yv_s.loc[m_es].to_numpy(float), yv_s.loc[m_sc].to_numpy(float)\n",
        "        w_es, w_sc = wv_s.loc[m_es].to_numpy(float), wv_s.loc[m_sc].to_numpy(float)\n",
        "\n",
        "        if len(X_es) > 0 and len(X_sc) > 0:\n",
        "            return (X_es, y_es, w_es), (X_sc, y_sc, w_sc), mode_str\n",
        "        return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
        "\n",
        "    # -------------------------\n",
        "    # Sequence creation function\n",
        "    # -------------------------\n",
        "    def make_sequences_eod_hybrid(X_2d, y_1d, w_1d, idx, lookback, stride=1):\n",
        "        X_2d, y_1d, w_1d = np.asarray(X_2d), np.asarray(y_1d), np.asarray(w_1d)\n",
        "        N, F = X_2d.shape\n",
        "        if N < lookback:\n",
        "            raise ValueError(f\"[ERROR] Not enough rows N={N} for lookback={lookback}.\")\n",
        "        X_seq, y_seq, w_seq, idx_seq = [], [], [], []\n",
        "        for t in range(lookback - 1, N, stride):\n",
        "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
        "            y_seq.append(y_1d[t])\n",
        "            w_seq.append(w_1d[t])\n",
        "            idx_seq.append(idx[t])\n",
        "        return (np.asarray(X_seq, dtype=np.float32),\n",
        "                np.asarray(y_seq, dtype=np.float32),\n",
        "                np.asarray(w_seq, dtype=np.float32),\n",
        "                pd.DatetimeIndex(idx_seq))\n",
        "\n",
        "    # -------------------------\n",
        "    # Loop over hybrid types (Sequential, Parallel)\n",
        "    # -------------------------\n",
        "    for hybrid_type in HYBRID_TYPES:\n",
        "        HYB_CFG = RUN_PARAMS[hybrid_type]\n",
        "        LOOKBACK = int(HYB_CFG[\"lookback\"])\n",
        "        STRIDE = int(HYB_CFG[\"stride\"])\n",
        "        LSTM_UNITS = int(HYB_CFG[\"lstm_units\"])\n",
        "        GRU_UNITS = int(HYB_CFG[\"gru_units\"])\n",
        "        DROPOUT = float(HYB_CFG[\"dropout\"])\n",
        "        LR = float(HYB_CFG[\"learning_rate\"])\n",
        "        EPOCHS = int(HYB_CFG[\"epochs\"])\n",
        "        BATCH_SIZE = int(HYB_CFG[\"batch_size\"])\n",
        "        PATIENCE = int(HYB_CFG[\"patience\"])\n",
        "        RANDOM_SEED = int(HYB_CFG[\"random_state\"])\n",
        "        FEATURE_SETS = list(HYB_CFG[\"feature_sets\"])  # Copy to allow modification\n",
        "        DENSE_UNITS = int(HYB_CFG[\"dense_units\"])\n",
        "        CLIPNORM = float(HYB_CFG[\"clipnorm\"])\n",
        "        LOSS = HYB_CFG[\"loss\"]\n",
        "        DENSE_ACT = HYB_CFG[\"dense_activation\"]\n",
        "        OUTPUT_ACT = HYB_CFG[\"output_activation\"]\n",
        "\n",
        "        # Add SHAP feature set if configured\n",
        "        SHAP_CFG = RUN_PARAMS.get(\"shap\", {})\n",
        "        SHAP_TOP_N = SHAP_CFG.get(\"top_n_features\", None)\n",
        "        if SHAP_TOP_N:\n",
        "            shap_features_file = f\"shap_top_{SHAP_TOP_N}_features.pkl\"\n",
        "            shap_path = None\n",
        "            for p in [FALLBACK_PROC_LOCAL / shap_features_file, FALLBACK_PROC_DRIVE / shap_features_file]:\n",
        "                if p.exists():\n",
        "                    shap_path = p\n",
        "                    break\n",
        "\n",
        "            if shap_path:\n",
        "                shap_fs_name = f\"shap_top_{SHAP_TOP_N}\"\n",
        "                # Check if SHAP feature files already exist (created by LSTM/GRU block)\n",
        "                shap_train_path = FALLBACK_PROC_LOCAL / f\"X_train_{shap_fs_name}.pkl\"\n",
        "                if not shap_train_path.exists():\n",
        "                    shap_features = load_pickle(shap_path)\n",
        "                    X_train_full = load_with_fallback(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "                    X_valid_full = load_with_fallback(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "                    X_test_full = load_with_fallback(\"X_test_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "                    X_train_shap = X_train_full[shap_features]\n",
        "                    X_valid_shap = X_valid_full[shap_features]\n",
        "                    X_test_shap = X_test_full[shap_features]\n",
        "\n",
        "                    save_pickle(X_train_shap, FALLBACK_PROC_LOCAL / f\"X_train_{shap_fs_name}.pkl\")\n",
        "                    save_pickle(X_valid_shap, FALLBACK_PROC_LOCAL / f\"X_valid_{shap_fs_name}.pkl\")\n",
        "                    save_pickle(X_test_shap, FALLBACK_PROC_LOCAL / f\"X_test_{shap_fs_name}.pkl\")\n",
        "                    copy_file(FALLBACK_PROC_LOCAL / f\"X_train_{shap_fs_name}.pkl\", FALLBACK_PROC_DRIVE / f\"X_train_{shap_fs_name}.pkl\")\n",
        "                    copy_file(FALLBACK_PROC_LOCAL / f\"X_valid_{shap_fs_name}.pkl\", FALLBACK_PROC_DRIVE / f\"X_valid_{shap_fs_name}.pkl\")\n",
        "                    copy_file(FALLBACK_PROC_LOCAL / f\"X_test_{shap_fs_name}.pkl\", FALLBACK_PROC_DRIVE / f\"X_test_{shap_fs_name}.pkl\")\n",
        "\n",
        "                if shap_fs_name not in FEATURE_SETS:\n",
        "                    FEATURE_SETS.append(shap_fs_name)\n",
        "                print(f\"[INFO] Added SHAP top {SHAP_TOP_N} to hybrid feature sets: {FEATURE_SETS}\")\n",
        "            else:\n",
        "                print(f\"[WARN] {shap_features_file} not found - run XGBoost with SHAP first\")\n",
        "\n",
        "        all_hybrid_results = []\n",
        "\n",
        "        arch_name = \"Sequential (LSTMâ†’GRU)\" if hybrid_type == \"hybrid_seq\" else \"Parallel (LSTMâˆ¥GRU)\"\n",
        "        print(f\"\\n{'#'*70}\")\n",
        "        print(f\"# TRAINING {hybrid_type.upper()} â€” {arch_name}\")\n",
        "        print(f\"{'#'*70}\")\n",
        "\n",
        "        for feature_set in FEATURE_SETS:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"[INFO] Training {hybrid_type.upper()} with feature set: {feature_set}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Load feature-specific X matrices\n",
        "            X_train_nn = load_with_fallback(f\"X_train_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "            X_valid_nn = load_with_fallback(f\"X_valid_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "            X_test_nn = load_with_fallback(f\"X_test_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
        "\n",
        "            n_features = X_train_nn.shape[1]\n",
        "            print(f\"[INFO] Shapes: TRAIN={X_train_nn.shape} | VALID={X_valid_nn.shape} | TEST={X_test_nn.shape}\")\n",
        "\n",
        "            # Split VALID -> ES + SCORE\n",
        "            (X_valid_es_df, y_valid_es, w_valid_es), (X_valid_sc_df, y_valid_sc, w_valid_sc), valid_mode = split_valid_es_score_hybrid(\n",
        "                X_valid_nn, y_valid, w_valid_np\n",
        "            )\n",
        "            print(f\"[INFO] VALID mode: {valid_mode}\")\n",
        "\n",
        "            # Scaling (fit on TRAIN only)\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train_nn.values)\n",
        "            X_valid_es_scaled = scaler.transform(X_valid_es_df.values)\n",
        "            X_valid_sc_scaled = scaler.transform(X_valid_sc_df.values)\n",
        "            X_test_scaled = scaler.transform(X_test_nn.values)\n",
        "\n",
        "            # Create sequences\n",
        "            Xtr_seq, ytr_seq, wtr_seq, idx_tr = make_sequences_eod_hybrid(X_train_scaled, y_train, w_train_np, X_train_nn.index, LOOKBACK, STRIDE)\n",
        "            Xes_seq, yes_seq, wes_seq, idx_es = make_sequences_eod_hybrid(X_valid_es_scaled, y_valid_es, w_valid_es, X_valid_es_df.index, LOOKBACK, STRIDE)\n",
        "            Xsc_seq, ysc_seq, wsc_seq, idx_sc = make_sequences_eod_hybrid(X_valid_sc_scaled, y_valid_sc, w_valid_sc, X_valid_sc_df.index, LOOKBACK, STRIDE)\n",
        "            Xte_seq, yte_seq, wte_seq, idx_te = make_sequences_eod_hybrid(X_test_scaled, y_test, w_test_np, X_test_nn.index, LOOKBACK, STRIDE)\n",
        "\n",
        "            print(f\"[INFO] Sequence shapes: TRAIN={Xtr_seq.shape} | VALID_ES={Xes_seq.shape} | VALID_SCORE={Xsc_seq.shape} | TEST={Xte_seq.shape}\")\n",
        "\n",
        "            # Build model\n",
        "            tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
        "\n",
        "            inp = keras.Input(shape=(LOOKBACK, n_features))\n",
        "\n",
        "            if hybrid_type == \"hybrid_seq\":\n",
        "                # Sequential: Input â†’ LSTM â†’ LayerNorm â†’ GRU â†’ Dense â†’ Output\n",
        "                x = layers.LSTM(LSTM_UNITS, return_sequences=True, dropout=DROPOUT)(inp)\n",
        "                x = layers.LayerNormalization()(x)\n",
        "                x = layers.GRU(GRU_UNITS, return_sequences=False, dropout=DROPOUT)(x)\n",
        "            else:\n",
        "                # Parallel: Input â†’ [LSTM, GRU] â†’ Concat â†’ Dense â†’ Output\n",
        "                lstm_out = layers.LSTM(LSTM_UNITS, return_sequences=False, dropout=DROPOUT)(inp)\n",
        "                gru_out = layers.GRU(GRU_UNITS, return_sequences=False, dropout=DROPOUT)(inp)\n",
        "                x = layers.Concatenate()([lstm_out, gru_out])\n",
        "\n",
        "            x = layers.Dense(DENSE_UNITS, activation=DENSE_ACT)(x)\n",
        "            x = layers.Dropout(DROPOUT)(x)\n",
        "            out = layers.Dense(1, activation=OUTPUT_ACT)(x)\n",
        "\n",
        "            model = keras.Model(inp, out)\n",
        "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=LR, clipnorm=CLIPNORM), loss=LOSS)\n",
        "            print(f\"[INFO] Model built: {model.count_params()} parameters\")\n",
        "\n",
        "            # Custom callback for SCORE set tracking\n",
        "            class ScoreSetCallback(keras.callbacks.Callback):\n",
        "                def __init__(self, X_score, y_score, w_score):\n",
        "                    super().__init__()\n",
        "                    self.Xs, self.ys, self.ws = X_score, y_score, w_score\n",
        "                    self.best = np.inf\n",
        "                    self.best_weights = None\n",
        "                def on_epoch_end(self, epoch, logs=None):\n",
        "                    pred = self.model.predict(self.Xs, verbose=0).reshape(-1)\n",
        "                    score = w_rmse(self.ys, pred, self.ws)\n",
        "                    if score < self.best:\n",
        "                        self.best = score\n",
        "                        self.best_weights = self.model.get_weights()\n",
        "\n",
        "            score_cb = ScoreSetCallback(Xsc_seq, ysc_seq, wsc_seq)\n",
        "            callbacks = [\n",
        "                keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
        "                score_cb,\n",
        "            ]\n",
        "\n",
        "            # Train\n",
        "            print(f\"[INFO] Training {hybrid_type.upper()} (epochs={EPOCHS}, batch_size={BATCH_SIZE}, patience={PATIENCE})...\")\n",
        "            history = model.fit(\n",
        "                Xtr_seq, ytr_seq,\n",
        "                sample_weight=wtr_seq,\n",
        "                validation_data=(Xes_seq, yes_seq, wes_seq),\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                verbose=0,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "            # Restore best weights by SCORE\n",
        "            if score_cb.best_weights is not None:\n",
        "                model.set_weights(score_cb.best_weights)\n",
        "                print(f\"[INFO] Restored best weights by VALID_SCORE wRMSE = {score_cb.best:.6f}\")\n",
        "\n",
        "            # Evaluate\n",
        "            pred_sc = model.predict(Xsc_seq, verbose=0).reshape(-1)\n",
        "            pred_te = model.predict(Xte_seq, verbose=0).reshape(-1)\n",
        "            baseline_sc, baseline_te = np.zeros_like(ysc_seq), np.zeros_like(yte_seq)\n",
        "\n",
        "            # Metrics\n",
        "            results = {\n",
        "                \"model_type\": hybrid_type,\n",
        "                \"feature_set\": feature_set,\n",
        "                \"n_features\": n_features,\n",
        "                \"valid_mode\": valid_mode,\n",
        "                \"epochs_trained\": len(history.history[\"loss\"]),\n",
        "                # Config params for comparison\n",
        "                \"hybrid_lstm_units\": LSTM_UNITS,\n",
        "                \"hybrid_gru_units\": GRU_UNITS,\n",
        "                \"dropout\": DROPOUT,\n",
        "                \"learning_rate\": LR,\n",
        "                \"lookback\": LOOKBACK,\n",
        "                \"epochs\": EPOCHS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"baseline_valid_wrmse\": w_rmse(ysc_seq, baseline_sc, wsc_seq),\n",
        "                \"baseline_valid_diracc\": dir_acc(ysc_seq, baseline_sc),\n",
        "                \"baseline_test_wrmse\": w_rmse(yte_seq, baseline_te, wte_seq),\n",
        "                \"baseline_test_diracc\": dir_acc(yte_seq, baseline_te),\n",
        "                \"model_valid_wrmse\": w_rmse(ysc_seq, pred_sc, wsc_seq),\n",
        "                \"model_valid_wmae\": w_mae(ysc_seq, pred_sc, wsc_seq),\n",
        "                \"model_valid_diracc\": dir_acc(ysc_seq, pred_sc),\n",
        "                \"model_test_wrmse\": w_rmse(yte_seq, pred_te, wte_seq),\n",
        "                \"model_test_wmae\": w_mae(yte_seq, pred_te, wte_seq),\n",
        "                \"model_test_diracc\": dir_acc(yte_seq, pred_te),\n",
        "            }\n",
        "            results[\"valid_wrmse_improvement\"] = results[\"baseline_valid_wrmse\"] - results[\"model_valid_wrmse\"]\n",
        "            results[\"test_wrmse_improvement\"] = results[\"baseline_test_wrmse\"] - results[\"model_test_wrmse\"]\n",
        "            all_hybrid_results.append(results)\n",
        "\n",
        "            print(f\"\\n[RESULT] {hybrid_type.upper()} | {feature_set} | n_features={n_features}\")\n",
        "            print(f\"  BASELINE VALID_SCORE: wRMSE={results['baseline_valid_wrmse']:.6f} | DirAcc={results['baseline_valid_diracc']:.4f}\")\n",
        "            print(f\"  MODEL    VALID_SCORE: wRMSE={results['model_valid_wrmse']:.6f} | DirAcc={results['model_valid_diracc']:.4f}\")\n",
        "            print(f\"  MODEL    TEST:        wRMSE={results['model_test_wrmse']:.6f} | DirAcc={results['model_test_diracc']:.4f}\")\n",
        "            print(f\"  Improvement (VALID):  {results['valid_wrmse_improvement']:.6f}\")\n",
        "\n",
        "            # Save model + scaler + config\n",
        "            model_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_{feature_set}.keras\"\n",
        "            scaler_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_{feature_set}_scaler.pkl\"\n",
        "            config_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_{feature_set}_config.json\"\n",
        "            model.save(model_path)\n",
        "            save_pickle(scaler, scaler_path)\n",
        "            save_json(HYB_CFG, config_path)\n",
        "            copy_file(model_path, MODELS_OUT_DRIVE / model_path.name)\n",
        "            copy_file(scaler_path, MODELS_OUT_DRIVE / scaler_path.name)\n",
        "            copy_file(config_path, MODELS_OUT_DRIVE / config_path.name)\n",
        "\n",
        "            # Predictions -> predictions/{hybrid_type}_{feature_set}/\n",
        "            PRED_HYB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
        "            PRED_HYB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
        "\n",
        "            preds_valid_df = pd.DataFrame({\n",
        "                \"date\": idx_sc, \"actual\": ysc_seq, \"baseline_zero\": baseline_sc,\n",
        "                \"predicted\": pred_sc, \"sample_weight\": wsc_seq,\n",
        "            }).reset_index(drop=True)\n",
        "            preds_test_df = pd.DataFrame({\n",
        "                \"date\": idx_te, \"actual\": yte_seq, \"baseline_zero\": baseline_te,\n",
        "                \"predicted\": pred_te, \"sample_weight\": wte_seq,\n",
        "            }).reset_index(drop=True)\n",
        "\n",
        "            preds_valid_df.to_csv(PRED_HYB_LOCAL / \"predictions_valid.csv\", index=False)\n",
        "            preds_test_df.to_csv(PRED_HYB_LOCAL / \"predictions_test.csv\", index=False)\n",
        "            copy_file(PRED_HYB_LOCAL / \"predictions_valid.csv\", PRED_HYB_DRIVE / \"predictions_valid.csv\")\n",
        "            copy_file(PRED_HYB_LOCAL / \"predictions_test.csv\", PRED_HYB_DRIVE / \"predictions_test.csv\")\n",
        "\n",
        "            print(f\"[OK] Saved: {model_path.name}, {scaler_path.name}\")\n",
        "            print(f\"[OK] Predictions: predictions/{hybrid_type}_{feature_set}/\")\n",
        "\n",
        "        # Summary for this hybrid type\n",
        "        hybrid_results_df = pd.DataFrame(all_hybrid_results)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[INFO] {hybrid_type.upper()} TRAINING SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        display(hybrid_results_df[[\"feature_set\", \"n_features\", \"model_valid_wrmse\", \"model_test_wrmse\", \"valid_wrmse_improvement\"]])\n",
        "\n",
        "        summary_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_summary.csv\"\n",
        "        hybrid_results_df.to_csv(summary_path, index=False)\n",
        "        copy_file(summary_path, MODELS_OUT_DRIVE / summary_path.name)\n",
        "        print(f\"[OK] Saved {hybrid_type.upper()} summary: {summary_path.name}\")\n",
        "\n",
        "    print(\"[OK] BLOCK 28 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9342fa",
      "metadata": {
        "id": "6d9342fa"
      },
      "source": [
        "## BLOCK 29 â€” HYBRID NEURAL NETWORK PREDICT TOMORROW + BACKTEST PLOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "1ae412c6",
      "metadata": {
        "id": "1ae412c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "022024eb-dc61-4bdf-c9dd-f24201ab5112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Hybrid Predictions output dirs:\n",
            "  - LOCAL: /content/my_project/runs/20260118_092035/predictions\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/runs/20260118_092035/predictions\n",
            "  [LOAD] y_test.pkl <- RUN_ID (LOCAL)\n",
            "  [LOAD] weights_test.pkl <- RUN_ID (LOCAL)\n",
            "\n",
            "######################################################################\n",
            "# PREDICTING WITH HYBRID_SEQ â€” Sequential (LSTMâ†’GRU)\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with HYBRID_SEQ: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_40.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=hybrid_seq_neural_40.keras | X_test=(87, 40)\n",
            "[INFO] Sequences: (81, 7, 40)\n",
            "[INFO] TEST wRMSE=0.020188 | DirAcc=0.4444\n",
            "[INFO] Tomorrow prediction: -0.000766 (-0.0766%)\n",
            "[OK] Saved: predictions/hybrid_seq_neural_40/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with HYBRID_SEQ: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_80.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=hybrid_seq_neural_80.keras | X_test=(87, 80)\n",
            "[INFO] Sequences: (81, 7, 80)\n",
            "[INFO] TEST wRMSE=0.020204 | DirAcc=0.4444\n",
            "[INFO] Tomorrow prediction: -0.000822 (-0.0822%)\n",
            "[OK] Saved: predictions/hybrid_seq_neural_80/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] HYBRID_SEQ PREDICTION SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   model_type feature_set  n_features  test_wrmse  test_dir_acc  \\\n",
              "0  hybrid_seq   neural_40          40    0.020188      0.444444   \n",
              "1  hybrid_seq   neural_80          80    0.020204      0.444444   \n",
              "\n",
              "  last_data_date  pred_tomorrow_logret  pred_tomorrow_pct  \n",
              "0     2026-01-14             -0.000766          -0.076599  \n",
              "1     2026-01-14             -0.000822          -0.082189  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06797ba9-595b-4b74-a892-d955487bb905\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_type</th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>test_wrmse</th>\n",
              "      <th>test_dir_acc</th>\n",
              "      <th>last_data_date</th>\n",
              "      <th>pred_tomorrow_logret</th>\n",
              "      <th>pred_tomorrow_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hybrid_seq</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.020188</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.000766</td>\n",
              "      <td>-0.076599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hybrid_seq</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.020204</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.000822</td>\n",
              "      <td>-0.082189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06797ba9-595b-4b74-a892-d955487bb905')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06797ba9-595b-4b74-a892-d955487bb905 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06797ba9-595b-4b74-a892-d955487bb905');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_5a68ad22-872a-46b8-9986-2dd4d94359c6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pred_summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5a68ad22-872a-46b8-9986-2dd4d94359c6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pred_summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_summary_df",
              "summary": "{\n  \"name\": \"pred_summary_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hybrid_seq\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1303252486723415e-05,\n        \"min\": 0.020188241770118378,\n        \"max\": 0.02020422698308403,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.02020422698308403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_dir_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4444444444444444,\n        \"max\": 0.4444444444444444,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4444444444444444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_data_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-01-14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_logret\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.955522891779154e-05,\n        \"min\": -0.0008222269243560731,\n        \"max\": -0.0007662873831577599,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.0008222269243560731\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003952382437270375,\n        \"min\": -0.08218889884248581,\n        \"max\": -0.07659938599601282,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.08218889884248581\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved summary: hybrid_seq_predictions_summary.csv\n",
            "\n",
            "######################################################################\n",
            "# PREDICTING WITH HYBRID_PAR â€” Parallel (LSTMâˆ¥GRU)\n",
            "######################################################################\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with HYBRID_PAR: neural_40\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_40.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=hybrid_par_neural_40.keras | X_test=(87, 40)\n",
            "[INFO] Sequences: (81, 7, 40)\n",
            "[INFO] TEST wRMSE=0.020272 | DirAcc=0.4444\n",
            "[INFO] Tomorrow prediction: -0.001054 (-0.1054%)\n",
            "[OK] Saved: predictions/hybrid_par_neural_40/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] Predicting with HYBRID_PAR: neural_80\n",
            "============================================================\n",
            "  [LOAD] X_test_neural_80.pkl <- RUN_ID (LOCAL)\n",
            "[INFO] Loaded: model=hybrid_par_neural_80.keras | X_test=(87, 80)\n",
            "[INFO] Sequences: (81, 7, 80)\n",
            "[INFO] TEST wRMSE=0.021824 | DirAcc=0.4691\n",
            "[INFO] Tomorrow prediction: -0.000916 (-0.0916%)\n",
            "[OK] Saved: predictions/hybrid_par_neural_80/ (tomorrow.csv, backtest.csv, plot.png)\n",
            "\n",
            "============================================================\n",
            "[INFO] HYBRID_PAR PREDICTION SUMMARY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   model_type feature_set  n_features  test_wrmse  test_dir_acc  \\\n",
              "0  hybrid_par   neural_40          40    0.020272      0.444444   \n",
              "1  hybrid_par   neural_80          80    0.021824      0.469136   \n",
              "\n",
              "  last_data_date  pred_tomorrow_logret  pred_tomorrow_pct  \n",
              "0     2026-01-14             -0.001054          -0.105360  \n",
              "1     2026-01-14             -0.000916          -0.091585  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40cd58c0-b670-4200-a6f0-cfa903c2a6c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_type</th>\n",
              "      <th>feature_set</th>\n",
              "      <th>n_features</th>\n",
              "      <th>test_wrmse</th>\n",
              "      <th>test_dir_acc</th>\n",
              "      <th>last_data_date</th>\n",
              "      <th>pred_tomorrow_logret</th>\n",
              "      <th>pred_tomorrow_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hybrid_par</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>40</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.001054</td>\n",
              "      <td>-0.105360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hybrid_par</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>80</td>\n",
              "      <td>0.021824</td>\n",
              "      <td>0.469136</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>-0.000916</td>\n",
              "      <td>-0.091585</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40cd58c0-b670-4200-a6f0-cfa903c2a6c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-40cd58c0-b670-4200-a6f0-cfa903c2a6c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-40cd58c0-b670-4200-a6f0-cfa903c2a6c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_1802e612-c5d5-4f73-a662-7fdac0f1e74a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pred_summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1802e612-c5d5-4f73-a662-7fdac0f1e74a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pred_summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_summary_df",
              "summary": "{\n  \"name\": \"pred_summary_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hybrid_par\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neural_80\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_features\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 40,\n        \"max\": 80,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0010975085548798045,\n        \"min\": 0.02027201586627703,\n        \"max\": 0.021824127349408546,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.021824127349408546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_dir_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017459426695964134,\n        \"min\": 0.4444444444444444,\n        \"max\": 0.4691358024691358,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.4691358024691358\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_data_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-01-14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_logret\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.749977815358777e-05,\n        \"min\": -0.0010541575029492378,\n        \"max\": -0.000916271994356066,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.000916271994356066\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_tomorrow_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00974037673147736,\n        \"min\": -0.10536020741156937,\n        \"max\": -0.09158523453529077,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.09158523453529077\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Saved summary: hybrid_par_predictions_summary.csv\n",
            "[OK] BLOCK 29 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if not TF_AVAILABLE:\n",
        "    print(\"[SKIP] BLOCK 29 â€” TensorFlow not available\")\n",
        "else:\n",
        "    HYBRID_TYPES = [\"hybrid_seq\", \"hybrid_par\"]\n",
        "    PLOT_CFG = RUN_PARAMS[\"plot\"]\n",
        "    N_PLOT = int(PLOT_CFG[\"n_plot\"])\n",
        "    FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n",
        "    DPI = int(PLOT_CFG[\"dpi\"])\n",
        "\n",
        "    # Directories\n",
        "    PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
        "    MODELS_DIR_LOCAL = Path(LOCAL_PATHS[\"models_dir\"])\n",
        "    PRED_OUT_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\")\n",
        "    PRED_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\")\n",
        "\n",
        "    print(\"[INFO] Hybrid Predictions output dirs:\")\n",
        "    print(\"  - LOCAL:\", PRED_OUT_LOCAL)\n",
        "    print(\"  - DRIVE:\", PRED_OUT_DRIVE)\n",
        "\n",
        "    # Sequence creation\n",
        "    def make_sequences_pred_hybrid(X_2d, y_1d, idx, lookback):\n",
        "        X_2d, y_1d = np.asarray(X_2d), np.asarray(y_1d)\n",
        "        N, F = X_2d.shape\n",
        "        X_seq, y_seq, idx_seq = [], [], []\n",
        "        for t in range(lookback - 1, N):\n",
        "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
        "            y_seq.append(y_1d[t])\n",
        "            idx_seq.append(idx[t])\n",
        "        return (np.asarray(X_seq, dtype=np.float32),\n",
        "                np.asarray(y_seq, dtype=np.float32),\n",
        "                pd.DatetimeIndex(idx_seq))\n",
        "\n",
        "    # Load shared data\n",
        "    y_test = load_with_fallback(\"y_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
        "    w_test = load_with_fallback(\"weights_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
        "    y_test_arr = y_test.astype(float).to_numpy()\n",
        "    w_test_arr = np.asarray(w_test, dtype=float)\n",
        "\n",
        "    # Loop over hybrid types\n",
        "    for hybrid_type in HYBRID_TYPES:\n",
        "        HYB_CFG = RUN_PARAMS[hybrid_type]\n",
        "        LOOKBACK = int(HYB_CFG[\"lookback\"])\n",
        "        FEATURE_SETS = list(HYB_CFG[\"feature_sets\"])  # Copy to allow modification\n",
        "\n",
        "        # Add SHAP feature set if configured and model exists\n",
        "        SHAP_CFG = RUN_PARAMS.get(\"shap\", {})\n",
        "        SHAP_TOP_N = SHAP_CFG.get(\"top_n_features\", None)\n",
        "        if SHAP_TOP_N:\n",
        "            shap_fs_name = f\"shap_top_{SHAP_TOP_N}\"\n",
        "            shap_model_path = MODELS_DIR_LOCAL / f\"{hybrid_type}_{shap_fs_name}.keras\"\n",
        "            if shap_model_path.exists() and shap_fs_name not in FEATURE_SETS:\n",
        "                FEATURE_SETS.append(shap_fs_name)\n",
        "                print(f\"[INFO] Added SHAP feature set to hybrid predictions: {shap_fs_name}\")\n",
        "\n",
        "        all_pred_results = []\n",
        "\n",
        "        arch_name = \"Sequential (LSTMâ†’GRU)\" if hybrid_type == \"hybrid_seq\" else \"Parallel (LSTMâˆ¥GRU)\"\n",
        "        print(f\"\\n{'#'*70}\")\n",
        "        print(f\"# PREDICTING WITH {hybrid_type.upper()} â€” {arch_name}\")\n",
        "        print(f\"{'#'*70}\")\n",
        "\n",
        "        for feature_set in FEATURE_SETS:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"[INFO] Predicting with {hybrid_type.upper()}: {feature_set}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Load model + scaler\n",
        "            model_path = MODELS_DIR_LOCAL / f\"{hybrid_type}_{feature_set}.keras\"\n",
        "            scaler_path = MODELS_DIR_LOCAL / f\"{hybrid_type}_{feature_set}_scaler.pkl\"\n",
        "\n",
        "            if not model_path.exists():\n",
        "                print(f\"[WARN] Model not found: {model_path}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            model = keras.models.load_model(model_path)\n",
        "            scaler = load_pickle(scaler_path)\n",
        "\n",
        "            # Load X_test\n",
        "            X_test = load_with_fallback(f\"X_test_{feature_set}.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR, use_pandas=True)\n",
        "            n_features = X_test.shape[1]\n",
        "            print(f\"[INFO] Loaded: model={model_path.name} | X_test={X_test.shape}\")\n",
        "\n",
        "            # Scale + Sequences\n",
        "            X_test_scaled = scaler.transform(X_test.values)\n",
        "            X_seq, y_seq, idx_seq = make_sequences_pred_hybrid(X_test_scaled, y_test_arr, X_test.index, LOOKBACK)\n",
        "            print(f\"[INFO] Sequences: {X_seq.shape}\")\n",
        "\n",
        "            # Predict\n",
        "            pred_seq = model.predict(X_seq, verbose=0).reshape(-1)\n",
        "            hist_df = pd.DataFrame({\"date\": idx_seq, \"actual\": y_seq, \"y_pred\": pred_seq}).set_index(\"date\")\n",
        "\n",
        "            # Tomorrow prediction\n",
        "            last_date = X_test.index[-1]\n",
        "            X_last_window = X_test_scaled[-LOOKBACK:, :].reshape(1, LOOKBACK, -1).astype(np.float32)\n",
        "            pred_tomorrow = float(model.predict(X_last_window, verbose=0).reshape(-1)[0])\n",
        "            pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n",
        "\n",
        "            # Metrics\n",
        "            w_seq = w_test_arr[LOOKBACK - 1:]\n",
        "            w_norm = w_seq / (w_seq.sum() + EPS)\n",
        "            test_wrmse = float(np.sqrt(np.sum(w_norm * (y_seq - pred_seq) ** 2)))\n",
        "            test_dir_acc = float(np.mean((y_seq > 0) == (pred_seq > 0)))\n",
        "\n",
        "            print(f\"[INFO] TEST wRMSE={test_wrmse:.6f} | DirAcc={test_dir_acc:.4f}\")\n",
        "            print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n",
        "\n",
        "            result = {\n",
        "                \"model_type\": hybrid_type,\n",
        "                \"feature_set\": feature_set,\n",
        "                \"n_features\": n_features,\n",
        "                \"test_wrmse\": test_wrmse,\n",
        "                \"test_dir_acc\": test_dir_acc,\n",
        "                \"last_data_date\": str(last_date.date()),\n",
        "                \"pred_tomorrow_logret\": pred_tomorrow,\n",
        "                \"pred_tomorrow_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
        "            }\n",
        "            all_pred_results.append(result)\n",
        "\n",
        "            # Save -> predictions/{hybrid_type}_{feature_set}/\n",
        "            PRED_HYB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
        "            PRED_HYB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
        "\n",
        "            pred_tomorrow_df = pd.DataFrame([{\n",
        "                \"feature_set\": feature_set, \"last_data_date\": last_date,\n",
        "                \"predicted_for\": \"next_trading_day\",\n",
        "                \"pred_logret\": pred_tomorrow,\n",
        "                \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
        "            }])\n",
        "            pred_tomorrow_df.to_csv(PRED_HYB_LOCAL / \"tomorrow.csv\", index=False)\n",
        "            copy_file(PRED_HYB_LOCAL / \"tomorrow.csv\", PRED_HYB_DRIVE / \"tomorrow.csv\")\n",
        "\n",
        "            hist_tail = hist_df.tail(N_PLOT).copy()\n",
        "            hist_tail.to_csv(PRED_HYB_LOCAL / \"backtest.csv\")\n",
        "            copy_file(PRED_HYB_LOCAL / \"backtest.csv\", PRED_HYB_DRIVE / \"backtest.csv\")\n",
        "\n",
        "            # Plot\n",
        "            fig, ax = plt.subplots(figsize=FIGSIZE)\n",
        "            ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n",
        "            ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted (y_pred)\")\n",
        "            ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n",
        "            ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
        "            ax.set_title(f\"{hybrid_type.upper()} Predictions â€” {feature_set} â€” last {len(hist_tail)} days + tomorrow\")\n",
        "            ax.set_xlabel(\"Date\")\n",
        "            ax.set_ylabel(\"Log Return\")\n",
        "            ax.legend(loc=\"upper right\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(PRED_HYB_LOCAL / \"plot.png\", dpi=DPI)\n",
        "            copy_file(PRED_HYB_LOCAL / \"plot.png\", PRED_HYB_DRIVE / \"plot.png\")\n",
        "            plt.close(fig)\n",
        "\n",
        "            print(f\"[OK] Saved: predictions/{hybrid_type}_{feature_set}/ (tomorrow.csv, backtest.csv, plot.png)\")\n",
        "\n",
        "        # Summary for this hybrid type\n",
        "        pred_summary_df = pd.DataFrame(all_pred_results)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[INFO] {hybrid_type.upper()} PREDICTION SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        display(pred_summary_df)\n",
        "\n",
        "        summary_path = PRED_OUT_LOCAL / f\"{hybrid_type}_predictions_summary.csv\"\n",
        "        pred_summary_df.to_csv(summary_path, index=False)\n",
        "        copy_file(summary_path, PRED_OUT_DRIVE / summary_path.name)\n",
        "        print(f\"[OK] Saved summary: {summary_path.name}\")\n",
        "\n",
        "    print(\"[OK] BLOCK 29 complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51jdgjwaSAXL",
      "metadata": {
        "id": "51jdgjwaSAXL"
      },
      "source": [
        "---\n",
        "# SECTION 9B: Ensemble Model\n",
        "\n",
        "**Combine predictions from all models**\n",
        "\n",
        "Methods:\n",
        "- Simple Average\n",
        "- Weighted Average (by inverse wRMSE)\n",
        "- Stacking (meta-model)\n",
        "- Rank Average\n",
        "\n",
        "**Prerequisites:** Run Sections 7, 7B, 8, 9 first"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vWfy4mRXSAXL",
      "metadata": {
        "id": "vWfy4mRXSAXL"
      },
      "source": [
        "## BLOCK 30 â€” ENSEMBLE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "iJoVa7pgSAXL",
      "metadata": {
        "id": "iJoVa7pgSAXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47be0b57-66af-4ea3-da56-697d54f79d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Ensemble method: weighted_average\n",
            "[INFO] Weight method: inverse_wrmse_squared\n",
            "[INFO] Models to combine: ['xgb', 'lgb', 'lstm', 'gru', 'hybrid_seq', 'hybrid_par']\n",
            "\n",
            "[INFO] Loading predictions...\n",
            "  [OK] xgb: 40 predictions\n",
            "  [OK] lgb: 40 predictions\n",
            "  [OK] lstm: 34 predictions\n",
            "  [OK] gru: 34 predictions\n",
            "  [OK] hybrid_seq: 34 predictions\n",
            "  [OK] hybrid_par: 34 predictions\n",
            "  [OK] xgb: 81 predictions\n",
            "  [OK] lgb: 81 predictions\n",
            "  [OK] lstm: 81 predictions\n",
            "  [OK] gru: 81 predictions\n",
            "  [OK] hybrid_seq: 81 predictions\n",
            "  [OK] hybrid_par: 81 predictions\n",
            "\n",
            "[INFO] Loaded models: ['xgb', 'lgb', 'lstm', 'gru', 'hybrid_seq', 'hybrid_par']\n",
            "[INFO] Valid predictions: 34 samples\n",
            "[INFO] Test predictions: 81 samples\n",
            "\n",
            "[INFO] Loading model metrics...\n",
            "  xgb: wRMSE = 0.019984 | DirAcc = 0.4444\n",
            "  lgb: wRMSE = 0.019984 | DirAcc = 0.4444\n",
            "  lstm: wRMSE = 0.020354 | DirAcc = 0.4444\n",
            "  gru: wRMSE = 0.028567 | DirAcc = 0.4321\n",
            "  hybrid_seq: wRMSE = 0.020188 | DirAcc = 0.4444\n",
            "  hybrid_par: wRMSE = 0.020272 | DirAcc = 0.4444\n",
            "\n",
            "[INFO] Applying model filter: {'min_diracc': 0.55, 'max_wrmse': 0.02, 'top_n': 4}\n",
            "  [FILTER] Removed 6 models with DirAcc < 0.55\n",
            "  [WARN] All models filtered out! Using original.\n",
            "\n",
            "[INFO] Computing ensemble weights (inverse_wrmse_squared)...\n",
            "  Weights: {'xgb': 0.21504507588893057, 'lgb': 0.21504507588893057, 'lstm': 0.11959278150119868, 'gru': 0.20781406857636636, 'hybrid_seq': 0.12182815731670807, 'hybrid_par': 0.12067484082786585}\n",
            "\n",
            "[INFO] Running ensemble (weighted_average)...\n",
            "\n",
            "[INFO] Computing ensemble metrics...\n",
            "\n",
            "[RESULTS] Ensemble (weighted_average):\n",
            "  Valid wRMSE: 0.025084 | wMAE: 0.016309 | DirAcc: 0.5000\n",
            "  Test  wRMSE: 0.020214 | wMAE: 0.015943 | DirAcc: 0.4321\n",
            "\n",
            "[COMPARE] Individual models vs Ensemble:\n",
            "  Model           Test wRMSE   Improvement \n",
            "  ---------------------------------------\n",
            "  xgb             0.019984     -1.15%\n",
            "  lgb             0.019984     -1.15%\n",
            "  lstm            0.020354     +0.68%\n",
            "  gru             0.028567     +29.24%\n",
            "  hybrid_seq      0.020188     -0.13%\n",
            "  hybrid_par      0.020272     +0.29%\n",
            "  ENSEMBLE        0.020214     ---         \n",
            "\n",
            "[INFO] Saving ensemble outputs...\n",
            "  LOCAL:  /content/my_project/runs/20260118_092035/outputs\n",
            "  DRIVE:  /content/drive/MyDrive/my_project/runs/20260118_092035/outputs\n",
            "\n",
            "[INFO] Tomorrow predictions from 2 models\n",
            "  xgb: 0.000854 (weight: 0.2150)\n",
            "  lgb: 0.000118 (weight: 0.2150)\n",
            "\n",
            "[INFO] ENSEMBLE Tomorrow prediction: 0.000486 (0.0486%)\n",
            "[OK] Saved: tomorrow.csv\n",
            "\n",
            "[OK] Saved ensemble outputs:\n",
            "  - ensemble_predictions_valid.csv\n",
            "  - ensemble_predictions_test.csv\n",
            "  - ensemble_metrics.csv\n",
            "  - ensemble_results.json\n",
            "  - ensemble_weights.json\n",
            "\n",
            "[OK] BLOCK 30 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# BLOCK 30 â€” ENSEMBLE MODEL\n",
        "# ============================================================\n",
        "# Supports: simple_average, weighted_average, rank_average, stacking\n",
        "# Weight methods: inverse_wrmse, inverse_wrmse_squared\n",
        "\n",
        "# Config\n",
        "ENSEMBLE_CFG = RUN_PARAMS.get(\"ensemble\", {})\n",
        "ENSEMBLE_METHOD = ENSEMBLE_CFG.get(\"method\", \"weighted_average\")\n",
        "ENSEMBLE_MODELS = ENSEMBLE_CFG.get(\"models\", [\"xgb\", \"lgb\", \"lstm\", \"gru\", \"hybrid_seq\", \"hybrid_par\"])\n",
        "WEIGHT_METHOD = ENSEMBLE_CFG.get(\"weight_method\", \"inverse_wrmse\")\n",
        "\n",
        "print(f\"[INFO] Ensemble method: {ENSEMBLE_METHOD}\")\n",
        "print(f\"[INFO] Weight method: {WEIGHT_METHOD}\")\n",
        "print(f\"[INFO] Models to combine: {ENSEMBLE_MODELS}\")\n",
        "\n",
        "# Directories\n",
        "MODELS_DIR_LOCAL = Path(LOCAL_PATHS[\"models_dir\"])\n",
        "MODELS_DIR_DRIVE = Path(DRIVE_PATHS[\"models_dir\"])\n",
        "OUTPUTS_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"outputs_dir\"]))\n",
        "OUTPUTS_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"outputs_dir\"]))\n",
        "\n",
        "# -------------------------\n",
        "# 1. Load predictions from all models\n",
        "# -------------------------\n",
        "print(\"\\n[INFO] Loading predictions...\")\n",
        "\n",
        "def load_predictions_ensemble(run_dir: Path, split: str, models: list) -> Tuple[pd.DataFrame, Optional[np.ndarray], Optional[np.ndarray]]:\n",
        "    \"\"\"Load predictions from available models.\n",
        "\n",
        "    All predictions are stored in: run_dir/predictions/{model}/ or run_dir/predictions/{model}_{feature_set}/\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (predictions_df, actual_values, sample_weights)\n",
        "    \"\"\"\n",
        "    pred_base = run_dir / \"predictions\"\n",
        "\n",
        "    # Model prediction paths (model_key -> list of possible paths)\n",
        "    model_paths = {\n",
        "        \"xgb\": [pred_base / \"xgb\" / f\"predictions_{split}.csv\"],\n",
        "        \"lgb\": [pred_base / \"lgb\" / f\"predictions_{split}.csv\"],\n",
        "        \"lstm\": [\n",
        "            pred_base / \"lstm_neural_40\" / f\"predictions_{split}.csv\",\n",
        "            pred_base / \"lstm_xgb_selected\" / f\"predictions_{split}.csv\",\n",
        "        ],\n",
        "        \"gru\": [\n",
        "            pred_base / \"gru_neural_40\" / f\"predictions_{split}.csv\",\n",
        "            pred_base / \"gru_xgb_selected\" / f\"predictions_{split}.csv\",\n",
        "        ],\n",
        "        \"hybrid_seq\": [\n",
        "            pred_base / \"hybrid_seq_neural_40\" / f\"predictions_{split}.csv\",\n",
        "            pred_base / \"hybrid_seq_xgb_selected\" / f\"predictions_{split}.csv\",\n",
        "        ],\n",
        "        \"hybrid_par\": [\n",
        "            pred_base / \"hybrid_par_neural_40\" / f\"predictions_{split}.csv\",\n",
        "            pred_base / \"hybrid_par_xgb_selected\" / f\"predictions_{split}.csv\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    predictions = {}\n",
        "    actual = None\n",
        "    weights = None\n",
        "\n",
        "    for model_name in models:\n",
        "        if model_name not in model_paths:\n",
        "            continue\n",
        "\n",
        "        # Find first existing file\n",
        "        file_path = None\n",
        "        for path in model_paths[model_name]:\n",
        "            if path.exists():\n",
        "                file_path = path\n",
        "                break\n",
        "\n",
        "        if file_path is None:\n",
        "            print(f\"  [SKIP] {model_name}: file not found\")\n",
        "            continue\n",
        "\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Handle different column names\n",
        "        pred_col = None\n",
        "        for col in [\"predicted\", \"prediction\", \"y_pred_model\", \"y_pred\"]:\n",
        "            if col in df.columns:\n",
        "                pred_col = col\n",
        "                break\n",
        "\n",
        "        actual_col = None\n",
        "        for col in [\"actual\", \"y_true\"]:\n",
        "            if col in df.columns:\n",
        "                actual_col = col\n",
        "                break\n",
        "\n",
        "        if pred_col:\n",
        "            predictions[model_name] = df[pred_col].values\n",
        "            if actual is None and actual_col:\n",
        "                actual = df[actual_col].values\n",
        "            # Load sample weights if available\n",
        "            if weights is None and \"sample_weight\" in df.columns:\n",
        "                weights = df[\"sample_weight\"].values\n",
        "            print(f\"  [OK] {model_name}: {len(predictions[model_name])} predictions\")\n",
        "\n",
        "    if len(predictions) == 0:\n",
        "        return pd.DataFrame(), None, None\n",
        "\n",
        "    # Align lengths\n",
        "    min_len = min(len(p) for p in predictions.values())\n",
        "    pred_df = pd.DataFrame({name: pred[:min_len] for name, pred in predictions.items()})\n",
        "\n",
        "    return (pred_df,\n",
        "            actual[:min_len] if actual is not None else None,\n",
        "            weights[:min_len] if weights is not None else None)\n",
        "\n",
        "\n",
        "# Load from local first, then drive\n",
        "valid_pred, valid_actual, valid_weights_loaded = None, None, None\n",
        "test_pred, test_actual, test_weights_loaded = None, None, None\n",
        "\n",
        "# Get run directories (parent of models_dir)\n",
        "RUN_DIR_LOCAL = MODELS_DIR_LOCAL.parent\n",
        "RUN_DIR_DRIVE = MODELS_DIR_DRIVE.parent\n",
        "\n",
        "for run_dir in [RUN_DIR_LOCAL, RUN_DIR_DRIVE]:\n",
        "    if (run_dir / \"predictions\").exists():\n",
        "        if valid_pred is None or len(valid_pred.columns) == 0:\n",
        "            valid_pred, valid_actual, valid_weights_loaded = load_predictions_ensemble(run_dir, \"valid\", ENSEMBLE_MODELS)\n",
        "        if test_pred is None or len(test_pred.columns) == 0:\n",
        "            test_pred, test_actual, test_weights_loaded = load_predictions_ensemble(run_dir, \"test\", ENSEMBLE_MODELS)\n",
        "\n",
        "if valid_pred is None or len(valid_pred.columns) == 0 or test_pred is None or len(test_pred.columns) == 0:\n",
        "    print(\"[ERROR] No predictions found! Run model training first.\")\n",
        "    ENSEMBLE_METRICS = None\n",
        "else:\n",
        "    print(f\"\\n[INFO] Loaded models: {list(valid_pred.columns)}\")\n",
        "    print(f\"[INFO] Valid predictions: {len(valid_pred)} samples\")\n",
        "    print(f\"[INFO] Test predictions: {len(test_pred)} samples\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 2. Load metrics for weighting\n",
        "    # -------------------------\n",
        "    print(\"\\n[INFO] Loading model metrics...\")\n",
        "\n",
        "    model_metrics = {}\n",
        "\n",
        "    metric_files = {\n",
        "        \"xgb\": \"final_metrics.csv\",\n",
        "        \"lgb\": \"final_metrics_lgb.csv\",\n",
        "    }\n",
        "\n",
        "    for model_name in valid_pred.columns:\n",
        "        if model_name in metric_files:\n",
        "            for models_dir in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n",
        "                file_path = models_dir / metric_files[model_name]\n",
        "                if file_path.exists():\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    wrmse = None\n",
        "                    diracc = None\n",
        "\n",
        "                    # Extract wRMSE\n",
        "                    # Extract TEST wRMSE\n",
        "                    if \"test_wrmse\" in df.columns:\n",
        "                        wrmse = float(df[\"test_wrmse\"].iloc[0])\n",
        "                    elif \"wRMSE\" in df.columns:\n",
        "                        test_row = df[df.get(\"split\", \"\") == \"TEST\"]\n",
        "                        if len(test_row) > 0:\n",
        "                            wrmse = float(test_row[\"wRMSE\"].iloc[0])\n",
        "\n",
        "                    # Extract VALID wRMSE (for weight calculation)\n",
        "                    valid_wrmse = None\n",
        "                    if \"valid_wrmse\" in df.columns:\n",
        "                        valid_wrmse = float(df[\"valid_wrmse\"].iloc[0])\n",
        "                    elif \"wRMSE\" in df.columns:\n",
        "                        valid_row = df[df.get(\"split\", \"\") == \"VALID\"]\n",
        "                        if len(valid_row) > 0:\n",
        "                            valid_wrmse = float(valid_row[\"wRMSE\"].iloc[0])\n",
        "\n",
        "                    # Extract TEST DirAcc\n",
        "                    if \"test_diracc\" in df.columns:\n",
        "                        diracc = float(df[\"test_diracc\"].iloc[0])\n",
        "                    elif \"DirAcc\" in df.columns:\n",
        "                        test_row = df[df.get(\"split\", \"\") == \"TEST\"]\n",
        "                        if len(test_row) > 0:\n",
        "                            diracc = float(test_row[\"DirAcc\"].iloc[0])\n",
        "\n",
        "                    # Extract VALID DirAcc\n",
        "                    valid_diracc = None\n",
        "                    if \"valid_diracc\" in df.columns:\n",
        "                        valid_diracc = float(df[\"valid_diracc\"].iloc[0])\n",
        "                    elif \"DirAcc\" in df.columns:\n",
        "                        valid_row = df[df.get(\"split\", \"\") == \"VALID\"]\n",
        "                        if len(valid_row) > 0:\n",
        "                            valid_diracc = float(valid_row[\"DirAcc\"].iloc[0])\n",
        "\n",
        "                    if wrmse is not None:\n",
        "                        model_metrics[model_name] = {\"valid_wrmse\": valid_wrmse if valid_wrmse else wrmse, \"test_wrmse\": wrmse, \"valid_diracc\": valid_diracc, \"test_diracc\": diracc}\n",
        "                    break\n",
        "\n",
        "        # Check for summary files (neural networks)\n",
        "        for models_dir in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n",
        "            summary_path = models_dir / f\"{model_name}_summary.csv\"\n",
        "            if summary_path.exists() and model_name not in model_metrics:\n",
        "                df = pd.read_csv(summary_path)\n",
        "                wrmse = None\n",
        "                diracc = None\n",
        "                valid_wrmse = None\n",
        "                valid_diracc = None\n",
        "\n",
        "                # Extract test metrics\n",
        "                if \"model_test_wrmse\" in df.columns:\n",
        "                    wrmse = float(df[\"model_test_wrmse\"].iloc[0])\n",
        "                if \"model_test_diracc\" in df.columns:\n",
        "                    diracc = float(df[\"model_test_diracc\"].iloc[0])\n",
        "\n",
        "                # Extract valid metrics (for weight calculation)\n",
        "                if \"model_valid_wrmse\" in df.columns:\n",
        "                    valid_wrmse = float(df[\"model_valid_wrmse\"].iloc[0])\n",
        "                if \"model_valid_diracc\" in df.columns:\n",
        "                    valid_diracc = float(df[\"model_valid_diracc\"].iloc[0])\n",
        "\n",
        "                if wrmse is not None:\n",
        "                    model_metrics[model_name] = {\"valid_wrmse\": valid_wrmse if valid_wrmse else wrmse, \"test_wrmse\": wrmse, \"valid_diracc\": valid_diracc, \"test_diracc\": diracc}\n",
        "                break\n",
        "\n",
        "    for name, metrics in model_metrics.items():\n",
        "        diracc_str = f\"{metrics.get('test_diracc', 0):.4f}\" if metrics.get('test_diracc') else \"N/A\"\n",
        "        print(f\"  {name}: wRMSE = {metrics['test_wrmse']:.6f} | DirAcc = {diracc_str}\")\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2B. Apply model filtering (based on config)\n",
        "# -------------------------\n",
        "FILTER_CFG = ENSEMBLE_CFG.get(\"filter\", {})\n",
        "if FILTER_CFG and any(v is not None for v in FILTER_CFG.values()):\n",
        "    print(f\"\\n[INFO] Applying model filter: {FILTER_CFG}\")\n",
        "\n",
        "    filtered_metrics = dict(model_metrics)\n",
        "\n",
        "    # Filter by min_diracc\n",
        "    min_diracc = FILTER_CFG.get(\"min_diracc\")\n",
        "    if min_diracc is not None:\n",
        "        before = len(filtered_metrics)\n",
        "        filtered_metrics = {\n",
        "            name: m for name, m in filtered_metrics.items()\n",
        "            if m.get(\"test_diracc\") is not None and m[\"test_diracc\"] >= min_diracc\n",
        "        }\n",
        "        removed = before - len(filtered_metrics)\n",
        "        if removed > 0:\n",
        "            print(f\"  [FILTER] Removed {removed} models with DirAcc < {min_diracc}\")\n",
        "\n",
        "    # Filter by max_wrmse\n",
        "    max_wrmse = FILTER_CFG.get(\"max_wrmse\")\n",
        "    if max_wrmse is not None:\n",
        "        before = len(filtered_metrics)\n",
        "        filtered_metrics = {\n",
        "            name: m for name, m in filtered_metrics.items()\n",
        "            if m[\"test_wrmse\"] <= max_wrmse\n",
        "        }\n",
        "        removed = before - len(filtered_metrics)\n",
        "        if removed > 0:\n",
        "            print(f\"  [FILTER] Removed {removed} models with wRMSE > {max_wrmse}\")\n",
        "\n",
        "    # Keep only top_n by wRMSE\n",
        "    top_n = FILTER_CFG.get(\"top_n\")\n",
        "    if top_n is not None and len(filtered_metrics) > top_n:\n",
        "        sorted_models = sorted(filtered_metrics.items(), key=lambda x: x[1][\"test_wrmse\"])\n",
        "        kept = [name for name, _ in sorted_models[:top_n]]\n",
        "        removed_models = [name for name, _ in sorted_models[top_n:]]\n",
        "        filtered_metrics = {name: filtered_metrics[name] for name in kept}\n",
        "        print(f\"  [FILTER] Kept top {top_n} models, removed: {removed_models}\")\n",
        "\n",
        "    if len(filtered_metrics) == 0:\n",
        "        print(\"  [WARN] All models filtered out! Using original.\")\n",
        "    else:\n",
        "        model_metrics = filtered_metrics\n",
        "        # Filter predictions too\n",
        "        valid_pred = valid_pred[[m for m in valid_pred.columns if m in model_metrics]]\n",
        "        test_pred = test_pred[[m for m in test_pred.columns if m in model_metrics]]\n",
        "        print(f\"  [FILTER] Final models: {list(model_metrics.keys())}\")\n",
        "\n",
        "# -------------------------\n",
        "    # 3. Compute ensemble weights\n",
        "    # -------------------------\n",
        "    print(f\"\\n[INFO] Computing ensemble weights ({WEIGHT_METHOD})...\")\n",
        "\n",
        "    if ENSEMBLE_METHOD == \"simple_average\":\n",
        "        weights = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n",
        "        print(\"  Using equal weights (simple_average)\")\n",
        "\n",
        "    elif ENSEMBLE_METHOD in [\"weighted_average\", \"stacking\"]:\n",
        "        if len(model_metrics) > 0:\n",
        "            if WEIGHT_METHOD == \"inverse_wrmse\":\n",
        "                raw_weights = {m: 1.0 / (model_metrics[m].get(\"valid_wrmse\", model_metrics[m][\"test_wrmse\"]) + EPS)\n",
        "                              for m in valid_pred.columns if m in model_metrics}\n",
        "            elif WEIGHT_METHOD == \"inverse_wrmse_squared\":\n",
        "                raw_weights = {m: 1.0 / ((model_metrics[m].get(\"valid_wrmse\", model_metrics[m][\"test_wrmse\"]) ** 2) + EPS)\n",
        "                              for m in valid_pred.columns if m in model_metrics}\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown weight_method: {WEIGHT_METHOD}\")\n",
        "\n",
        "            total = sum(raw_weights.values())\n",
        "            weights = {m: w / total for m, w in raw_weights.items()}\n",
        "\n",
        "            # Add equal weight for models without metrics\n",
        "            for m in valid_pred.columns:\n",
        "                if m not in weights:\n",
        "                    weights[m] = 1.0 / len(valid_pred.columns)\n",
        "                    print(f\"  [WARN] No metrics for {m}, using equal weight\")\n",
        "        else:\n",
        "            weights = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n",
        "            print(\"  [WARN] No model metrics found, using equal weights\")\n",
        "\n",
        "    elif ENSEMBLE_METHOD == \"rank_average\":\n",
        "        weights = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n",
        "        print(\"  Using equal weights (rank_average)\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown ensemble method: {ENSEMBLE_METHOD}\")\n",
        "\n",
        "    print(f\"  Weights: {weights}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 4. Generate ensemble predictions\n",
        "    # -------------------------\n",
        "    print(f\"\\n[INFO] Running ensemble ({ENSEMBLE_METHOD})...\")\n",
        "\n",
        "    if ENSEMBLE_METHOD == \"simple_average\":\n",
        "        ensemble_valid = valid_pred.mean(axis=1).values\n",
        "        ensemble_test = test_pred.mean(axis=1).values\n",
        "\n",
        "    elif ENSEMBLE_METHOD == \"weighted_average\":\n",
        "        ensemble_valid = np.zeros(len(valid_pred))\n",
        "        ensemble_test = np.zeros(len(test_pred))\n",
        "        for model, weight in weights.items():\n",
        "            if model in valid_pred.columns:\n",
        "                ensemble_valid += weight * valid_pred[model].values\n",
        "                ensemble_test += weight * test_pred[model].values\n",
        "\n",
        "    elif ENSEMBLE_METHOD == \"rank_average\":\n",
        "        # Rank-based averaging\n",
        "        valid_ranks = valid_pred.rank(pct=True)\n",
        "        test_ranks = test_pred.rank(pct=True)\n",
        "\n",
        "        avg_valid_rank = valid_ranks.mean(axis=1)\n",
        "        avg_test_rank = test_ranks.mean(axis=1)\n",
        "\n",
        "        # Convert back to prediction scale using inverse normal CDF\n",
        "        from scipy import stats\n",
        "        mean_pred = valid_pred.mean(axis=1).mean()\n",
        "        std_pred = valid_pred.std(axis=1).mean()\n",
        "\n",
        "        ensemble_valid = mean_pred + std_pred * stats.norm.ppf(avg_valid_rank.clip(0.001, 0.999))\n",
        "        ensemble_test = mean_pred + std_pred * stats.norm.ppf(avg_test_rank.clip(0.001, 0.999))\n",
        "\n",
        "    elif ENSEMBLE_METHOD == \"stacking\":\n",
        "        from sklearn.linear_model import Ridge\n",
        "\n",
        "        meta_params = ENSEMBLE_CFG.get(\"meta_params\", {\"alpha\": 1.0})\n",
        "        meta_model = Ridge(**meta_params)\n",
        "        meta_model.fit(valid_pred.values, valid_actual)\n",
        "\n",
        "        ensemble_valid = meta_model.predict(valid_pred.values)\n",
        "        ensemble_test = meta_model.predict(test_pred.values)\n",
        "\n",
        "        # Update weights with stacking coefficients\n",
        "        weights = dict(zip(valid_pred.columns, meta_model.coef_))\n",
        "        print(f\"  Stacking coefficients: {weights}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 5. Compute metrics\n",
        "    # -------------------------\n",
        "    print(\"\\n[INFO] Computing ensemble metrics...\")\n",
        "\n",
        "    # Load sample weights (fallback to loaded or uniform)\n",
        "    valid_weights = valid_weights_loaded\n",
        "    test_weights = test_weights_loaded\n",
        "\n",
        "    if valid_weights is None:\n",
        "        try:\n",
        "            valid_weights = load_with_fallback(\"weights_valid.pkl\",\n",
        "                Path(LOCAL_PATHS[\"proc_dir\"]), DATA_DIRS_LOCAL[\"processed\"],\n",
        "                Path(DRIVE_PATHS[\"proc_dir\"]), DATA_DIRS_DRIVE[\"processed\"])[:len(valid_actual)]\n",
        "        except:\n",
        "            valid_weights = np.ones(len(valid_actual))\n",
        "            print(\"  [WARN] Using uniform weights for validation\")\n",
        "\n",
        "    if test_weights is None:\n",
        "        try:\n",
        "            test_weights = load_with_fallback(\"weights_test.pkl\",\n",
        "                Path(LOCAL_PATHS[\"proc_dir\"]), DATA_DIRS_LOCAL[\"processed\"],\n",
        "                Path(DRIVE_PATHS[\"proc_dir\"]), DATA_DIRS_DRIVE[\"processed\"])[:len(test_actual)]\n",
        "        except:\n",
        "            test_weights = np.ones(len(test_actual))\n",
        "            print(\"  [WARN] Using uniform weights for test\")\n",
        "\n",
        "    ensemble_metrics = {\n",
        "        \"model\": f\"Ensemble-{ENSEMBLE_METHOD}\",\n",
        "        \"method\": ENSEMBLE_METHOD,\n",
        "        \"weight_method\": WEIGHT_METHOD,\n",
        "        \"models\": list(valid_pred.columns),\n",
        "        \"weights\": weights,\n",
        "        \"valid_wrmse\": w_rmse(valid_actual, ensemble_valid, valid_weights),\n",
        "        \"valid_wmae\": w_mae(valid_actual, ensemble_valid, valid_weights),\n",
        "        \"valid_diracc\": dir_acc(valid_actual, ensemble_valid),\n",
        "        \"test_wrmse\": w_rmse(test_actual, ensemble_test, test_weights),\n",
        "        \"test_wmae\": w_mae(test_actual, ensemble_test, test_weights),\n",
        "        \"test_diracc\": dir_acc(test_actual, ensemble_test),\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[RESULTS] Ensemble ({ENSEMBLE_METHOD}):\")\n",
        "    print(f\"  Valid wRMSE: {ensemble_metrics['valid_wrmse']:.6f} | wMAE: {ensemble_metrics['valid_wmae']:.6f} | DirAcc: {ensemble_metrics['valid_diracc']:.4f}\")\n",
        "    print(f\"  Test  wRMSE: {ensemble_metrics['test_wrmse']:.6f} | wMAE: {ensemble_metrics['test_wmae']:.6f} | DirAcc: {ensemble_metrics['test_diracc']:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 6. Compare with individual models\n",
        "    # -------------------------\n",
        "    print(\"\\n[COMPARE] Individual models vs Ensemble:\")\n",
        "    print(f\"  {'Model':<15} {'Test wRMSE':<12} {'Improvement':<12}\")\n",
        "    print(f\"  {'-'*39}\")\n",
        "    for model_name, metrics in model_metrics.items():\n",
        "        model_wrmse = metrics[\"test_wrmse\"]\n",
        "        improvement = (model_wrmse - ensemble_metrics[\"test_wrmse\"]) / model_wrmse * 100\n",
        "        print(f\"  {model_name:<15} {model_wrmse:<12.6f} {improvement:>+.2f}%\")\n",
        "    print(f\"  {'ENSEMBLE':<15} {ensemble_metrics['test_wrmse']:<12.6f} {'---':<12}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # 7. Save outputs (LOCAL + DRIVE)\n",
        "    # -------------------------\n",
        "    print(f\"\\n[INFO] Saving ensemble outputs...\")\n",
        "    print(f\"  LOCAL:  {OUTPUTS_LOCAL}\")\n",
        "    print(f\"  DRIVE:  {OUTPUTS_DRIVE}\")\n",
        "\n",
        "    # 7.1 Predictions with sample weights - Valid\n",
        "    valid_df = pd.DataFrame({\n",
        "        \"actual\": valid_actual,\n",
        "        \"predicted\": ensemble_valid,\n",
        "        \"sample_weight\": valid_weights[:len(valid_actual)],\n",
        "    })\n",
        "    valid_df.to_csv(OUTPUTS_LOCAL / \"ensemble_predictions_valid.csv\", index=False)\n",
        "    copy_file(OUTPUTS_LOCAL / \"ensemble_predictions_valid.csv\", OUTPUTS_DRIVE / \"ensemble_predictions_valid.csv\")\n",
        "\n",
        "    # 7.2 Predictions with sample weights - Test\n",
        "    test_df = pd.DataFrame({\n",
        "        \"actual\": test_actual,\n",
        "        \"predicted\": ensemble_test,\n",
        "        \"sample_weight\": test_weights[:len(test_actual)],\n",
        "    })\n",
        "    test_df.to_csv(OUTPUTS_LOCAL / \"ensemble_predictions_test.csv\", index=False)\n",
        "    copy_file(OUTPUTS_LOCAL / \"ensemble_predictions_test.csv\", OUTPUTS_DRIVE / \"ensemble_predictions_test.csv\")\n",
        "\n",
        "    # 7.3 Metrics CSV\n",
        "    metrics_df = pd.DataFrame([{\n",
        "        \"model\": ensemble_metrics[\"model\"],\n",
        "        \"method\": ensemble_metrics[\"method\"],\n",
        "        \"weight_method\": ensemble_metrics[\"weight_method\"],\n",
        "        \"n_models\": len(ensemble_metrics[\"models\"]),\n",
        "        \"valid_wrmse\": ensemble_metrics[\"valid_wrmse\"],\n",
        "        \"valid_wmae\": ensemble_metrics[\"valid_wmae\"],\n",
        "        \"valid_diracc\": ensemble_metrics[\"valid_diracc\"],\n",
        "        \"test_wrmse\": ensemble_metrics[\"test_wrmse\"],\n",
        "        \"test_wmae\": ensemble_metrics[\"test_wmae\"],\n",
        "        \"test_diracc\": ensemble_metrics[\"test_diracc\"],\n",
        "    }])\n",
        "    metrics_df.to_csv(OUTPUTS_LOCAL / \"ensemble_metrics.csv\", index=False)\n",
        "    copy_file(OUTPUTS_LOCAL / \"ensemble_metrics.csv\", OUTPUTS_DRIVE / \"ensemble_metrics.csv\")\n",
        "\n",
        "    # 7.4 Full results JSON\n",
        "    save_json(ensemble_metrics, OUTPUTS_LOCAL / \"ensemble_results.json\")\n",
        "    copy_file(OUTPUTS_LOCAL / \"ensemble_results.json\", OUTPUTS_DRIVE / \"ensemble_results.json\")\n",
        "\n",
        "    # 7.5 Weights JSON\n",
        "    save_json(weights, OUTPUTS_LOCAL / \"ensemble_weights.json\")\n",
        "    copy_file(OUTPUTS_LOCAL / \"ensemble_weights.json\", OUTPUTS_DRIVE / \"ensemble_weights.json\")\n",
        "\n",
        "    # 7.6 Save stacking meta-model if used\n",
        "    if ENSEMBLE_METHOD == \"stacking\":\n",
        "        save_pickle(meta_model, OUTPUTS_LOCAL / \"ensemble_meta_model.pkl\")\n",
        "        copy_file(OUTPUTS_LOCAL / \"ensemble_meta_model.pkl\", OUTPUTS_DRIVE / \"ensemble_meta_model.pkl\")\n",
        "        print(f\"  - ensemble_meta_model.pkl\")\n",
        "\n",
        "    # 7.7 Tomorrow prediction (ensemble of individual model tomorrow predictions)\n",
        "    PRED_BASE = Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\"\n",
        "\n",
        "    tomorrow_paths = {\n",
        "        \"xgb\": PRED_BASE / \"xgb\" / \"tomorrow.csv\",\n",
        "        \"lgb\": PRED_BASE / \"lgb\" / \"tomorrow.csv\",\n",
        "        \"lstm\": PRED_BASE / \"lstm_xgb_selected\" / \"tomorrow.csv\",\n",
        "        \"gru\": PRED_BASE / \"gru_xgb_selected\" / \"tomorrow.csv\",\n",
        "        \"hybrid_seq\": PRED_BASE / \"hybrid_seq_xgb_selected\" / \"tomorrow.csv\",\n",
        "        \"hybrid_par\": PRED_BASE / \"hybrid_par_xgb_selected\" / \"tomorrow.csv\",\n",
        "    }\n",
        "\n",
        "    tomorrow_preds = {}\n",
        "    for model_name, path in tomorrow_paths.items():\n",
        "        if path.exists():\n",
        "            df = pd.read_csv(path)\n",
        "            if \"pred_logret\" in df.columns and len(df) > 0:\n",
        "                tomorrow_preds[model_name] = float(df[\"pred_logret\"].iloc[0])\n",
        "\n",
        "    if tomorrow_preds:\n",
        "        print(f\"\\n[INFO] Tomorrow predictions from {len(tomorrow_preds)} models\")\n",
        "\n",
        "        # Calculate weighted average\n",
        "        total_weight = 0.0\n",
        "        weighted_sum = 0.0\n",
        "\n",
        "        for model_name, pred in tomorrow_preds.items():\n",
        "            weight = weights.get(model_name, 0.0)\n",
        "            if weight > 0:\n",
        "                weighted_sum += weight * pred\n",
        "                total_weight += weight\n",
        "                print(f\"  {model_name}: {pred:.6f} (weight: {weight:.4f})\")\n",
        "\n",
        "        if total_weight > 0:\n",
        "            ensemble_tomorrow = weighted_sum / total_weight\n",
        "            ensemble_tomorrow_pct = float(np.expm1(ensemble_tomorrow) * 100)\n",
        "\n",
        "            print(f\"\\n[INFO] ENSEMBLE Tomorrow prediction: {ensemble_tomorrow:.6f} ({ensemble_tomorrow_pct:.4f}%)\")\n",
        "\n",
        "            # Save tomorrow prediction\n",
        "            tomorrow_df = pd.DataFrame([{\n",
        "                \"method\": ENSEMBLE_METHOD,\n",
        "                \"n_models\": len(tomorrow_preds),\n",
        "                \"predicted_for\": \"next_trading_day\",\n",
        "                \"pred_logret\": ensemble_tomorrow,\n",
        "                \"pred_return_pct\": ensemble_tomorrow_pct,\n",
        "            }])\n",
        "            tomorrow_df.to_csv(OUTPUTS_LOCAL / \"tomorrow.csv\", index=False)\n",
        "            copy_file(OUTPUTS_LOCAL / \"tomorrow.csv\", OUTPUTS_DRIVE / \"tomorrow.csv\")\n",
        "\n",
        "            # Update metrics with tomorrow prediction\n",
        "            ensemble_metrics[\"pred_tomorrow_logret\"] = ensemble_tomorrow\n",
        "            ensemble_metrics[\"pred_tomorrow_pct\"] = ensemble_tomorrow_pct\n",
        "            save_json(ensemble_metrics, OUTPUTS_LOCAL / \"ensemble_results.json\")\n",
        "            copy_file(OUTPUTS_LOCAL / \"ensemble_results.json\", OUTPUTS_DRIVE / \"ensemble_results.json\")\n",
        "\n",
        "            print(f\"[OK] Saved: tomorrow.csv\")\n",
        "\n",
        "    print(f\"\\n[OK] Saved ensemble outputs:\")\n",
        "    print(f\"  - ensemble_predictions_valid.csv\")\n",
        "    print(f\"  - ensemble_predictions_test.csv\")\n",
        "    print(f\"  - ensemble_metrics.csv\")\n",
        "    print(f\"  - ensemble_results.json\")\n",
        "    print(f\"  - ensemble_weights.json\")\n",
        "\n",
        "    # Store for summary\n",
        "    ENSEMBLE_METRICS = ensemble_metrics\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# COMPARE ENSEMBLE METHODS (Optional)\n",
        "# ============================================================\n",
        "def compare_ensemble_methods(\n",
        "    run_dir_local: Path,\n",
        "    run_dir_drive: Path,\n",
        "    models: list,\n",
        "    methods: list = None,\n",
        "    weight_methods: list = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Compare different ensemble methods.\n",
        "\n",
        "    Args:\n",
        "        run_dir_local: Local run directory\n",
        "        run_dir_drive: Drive run directory\n",
        "        models: List of models to include\n",
        "        methods: List of ensemble methods to compare (default: all)\n",
        "        weight_methods: List of weight methods to compare (default: all)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with comparison results\n",
        "    \"\"\"\n",
        "    if methods is None:\n",
        "        methods = [\"simple_average\", \"weighted_average\", \"rank_average\", \"stacking\"]\n",
        "    if weight_methods is None:\n",
        "        weight_methods = [\"inverse_wrmse\", \"inverse_wrmse_squared\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ENSEMBLE METHOD COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for method in methods:\n",
        "        for wm in weight_methods:\n",
        "            # Skip weight_method for methods that don't use it\n",
        "            if method in [\"simple_average\", \"rank_average\"] and wm != \"inverse_wrmse\":\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n[INFO] Testing: {method} + {wm}\")\n",
        "\n",
        "            try:\n",
        "                # Load predictions\n",
        "                valid_pred, valid_actual, valid_weights = None, None, None\n",
        "                test_pred, test_actual, test_weights = None, None, None\n",
        "\n",
        "                for run_dir in [run_dir_local, run_dir_drive]:\n",
        "                    if (run_dir / \"predictions\").exists():\n",
        "                        if valid_pred is None or len(valid_pred.columns) == 0:\n",
        "                            valid_pred, valid_actual, valid_weights = load_predictions_ensemble(run_dir, \"valid\", models)\n",
        "                        if test_pred is None or len(test_pred.columns) == 0:\n",
        "                            test_pred, test_actual, test_weights = load_predictions_ensemble(run_dir, \"test\", models)\n",
        "\n",
        "                if valid_pred is None or len(valid_pred.columns) == 0:\n",
        "                    print(\"  [SKIP] No predictions found\")\n",
        "                    continue\n",
        "\n",
        "                # Load model metrics\n",
        "                model_metrics_cmp = {}\n",
        "                metric_files_cmp = {\"xgb\": \"final_metrics.csv\", \"lgb\": \"final_metrics_lgb.csv\"}\n",
        "\n",
        "                for mn in valid_pred.columns:\n",
        "                    if mn in metric_files_cmp:\n",
        "                        for md in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n",
        "                            fp = md / metric_files_cmp[mn]\n",
        "                            if fp.exists():\n",
        "                                df = pd.read_csv(fp)\n",
        "                                if \"test_wrmse\" in df.columns:\n",
        "                                    model_metrics_cmp[mn] = {\"test_wrmse\": float(df[\"test_wrmse\"].iloc[0])}\n",
        "                                elif \"wRMSE\" in df.columns:\n",
        "                                    tr = df[df.get(\"split\", \"\") == \"TEST\"]\n",
        "                                    if len(tr) > 0:\n",
        "                                        model_metrics_cmp[mn] = {\"test_wrmse\": float(tr[\"wRMSE\"].iloc[0])}\n",
        "                                break\n",
        "\n",
        "                    for md in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n",
        "                        sp = md / f\"{mn}_summary.csv\"\n",
        "                        if sp.exists() and mn not in model_metrics_cmp:\n",
        "                            df = pd.read_csv(sp)\n",
        "                            if \"model_test_wrmse\" in df.columns:\n",
        "                                model_metrics_cmp[mn] = {\"test_wrmse\": float(df[\"model_test_wrmse\"].iloc[0])}\n",
        "                            break\n",
        "\n",
        "                # Compute weights\n",
        "                if method == \"simple_average\" or method == \"rank_average\":\n",
        "                    w = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n",
        "                elif len(model_metrics_cmp) > 0:\n",
        "                    if wm == \"inverse_wrmse\":\n",
        "                        raw_w = {m: 1.0 / (model_metrics_cmp[m].get(\"valid_wrmse\", model_metrics_cmp[m][\"test_wrmse\"]) + EPS)\n",
        "                                for m in valid_pred.columns if m in model_metrics_cmp}\n",
        "                    else:  # inverse_wrmse_squared\n",
        "                        raw_w = {m: 1.0 / ((model_metrics_cmp[m].get(\"valid_wrmse\", model_metrics_cmp[m][\"test_wrmse\"]) ** 2) + EPS)\n",
        "                                for m in valid_pred.columns if m in model_metrics_cmp}\n",
        "                    total_w = sum(raw_w.values())\n",
        "                    w = {m: ww / total_w for m, ww in raw_w.items()}\n",
        "                    for m in valid_pred.columns:\n",
        "                        if m not in w:\n",
        "                            w[m] = 1.0 / len(valid_pred.columns)\n",
        "                else:\n",
        "                    w = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n",
        "\n",
        "                # Generate predictions\n",
        "                if method == \"simple_average\":\n",
        "                    ens_valid = valid_pred.mean(axis=1).values\n",
        "                    ens_test = test_pred.mean(axis=1).values\n",
        "                elif method == \"weighted_average\":\n",
        "                    ens_valid = np.zeros(len(valid_pred))\n",
        "                    ens_test = np.zeros(len(test_pred))\n",
        "                    for m, ww in w.items():\n",
        "                        if m in valid_pred.columns:\n",
        "                            ens_valid += ww * valid_pred[m].values\n",
        "                            ens_test += ww * test_pred[m].values\n",
        "                elif method == \"rank_average\":\n",
        "                    from scipy import stats\n",
        "                    v_ranks = valid_pred.rank(pct=True).mean(axis=1)\n",
        "                    t_ranks = test_pred.rank(pct=True).mean(axis=1)\n",
        "                    mean_p = valid_pred.mean(axis=1).mean()\n",
        "                    std_p = valid_pred.std(axis=1).mean()\n",
        "                    ens_valid = mean_p + std_p * stats.norm.ppf(v_ranks.clip(0.001, 0.999))\n",
        "                    ens_test = mean_p + std_p * stats.norm.ppf(t_ranks.clip(0.001, 0.999))\n",
        "                elif method == \"stacking\":\n",
        "                    from sklearn.linear_model import Ridge\n",
        "                    meta = Ridge(alpha=1.0)\n",
        "                    meta.fit(valid_pred.values, valid_actual)\n",
        "                    ens_valid = meta.predict(valid_pred.values)\n",
        "                    ens_test = meta.predict(test_pred.values)\n",
        "                    w = dict(zip(valid_pred.columns, meta.coef_))\n",
        "\n",
        "                # Load sample weights\n",
        "                sw_valid = valid_weights if valid_weights is not None else np.ones(len(valid_actual))\n",
        "                sw_test = test_weights if test_weights is not None else np.ones(len(test_actual))\n",
        "\n",
        "                # Compute metrics\n",
        "                result = {\n",
        "                    \"method\": method,\n",
        "                    \"weight_method\": wm,\n",
        "                    \"n_models\": len(valid_pred.columns),\n",
        "                    \"valid_wrmse\": w_rmse(valid_actual, ens_valid, sw_valid),\n",
        "                    \"valid_diracc\": dir_acc(valid_actual, ens_valid),\n",
        "                    \"test_wrmse\": w_rmse(test_actual, ens_test, sw_test),\n",
        "                    \"test_diracc\": dir_acc(test_actual, ens_test),\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                print(f\"  Test wRMSE: {result['test_wrmse']:.6f} | DirAcc: {result['test_diracc']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  [ERROR] {e}\")\n",
        "\n",
        "    if len(all_results) == 0:\n",
        "        print(\"\\n[WARN] No results to compare\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Create comparison DataFrame\n",
        "    cmp_df = pd.DataFrame(all_results).sort_values(\"test_wrmse\").reset_index(drop=True)\n",
        "    cmp_df[\"rank\"] = range(1, len(cmp_df) + 1)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPARISON RESULTS (sorted by Test wRMSE)\")\n",
        "    print(\"=\"*60)\n",
        "    print(cmp_df[[\"rank\", \"method\", \"weight_method\", \"test_wrmse\", \"test_diracc\"]].to_string(index=False))\n",
        "\n",
        "    # Save comparison\n",
        "    cmp_df.to_csv(OUTPUTS_LOCAL / \"ensemble_comparison.csv\", index=False)\n",
        "    copy_file(OUTPUTS_LOCAL / \"ensemble_comparison.csv\", OUTPUTS_DRIVE / \"ensemble_comparison.csv\")\n",
        "    print(f\"\\n[OK] Saved ensemble_comparison.csv\")\n",
        "\n",
        "    return cmp_df\n",
        "\n",
        "\n",
        "# Run comparison if enabled in config\n",
        "if ENSEMBLE_CFG.get(\"compare_methods\", False):\n",
        "    ENSEMBLE_COMPARISON = compare_ensemble_methods(\n",
        "        run_dir_local=RUN_DIR_LOCAL,\n",
        "        run_dir_drive=RUN_DIR_DRIVE,\n",
        "        models=ENSEMBLE_MODELS,\n",
        "    )\n",
        "else:\n",
        "    ENSEMBLE_COMPARISON = None\n",
        "\n",
        "print(\"\\n[OK] BLOCK 30 complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z5oqIe08SAXM",
      "metadata": {
        "id": "Z5oqIe08SAXM"
      },
      "source": [
        "## BLOCK 31 â€” SUMMARY & RESULTS EXPORT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "M9a7Zt6OSAXM",
      "metadata": {
        "id": "M9a7Zt6OSAXM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c15f824-da0b-4756-cddd-dbd7e1689828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Scanning all runs in: /content/drive/MyDrive/my_project/runs\n",
            "[INFO] Found 14 runs\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_092035\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019984\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025313\n",
            "  âœ“ XGBoost: wRMSE=0.019788\n",
            "  âœ“ LightGBM: wRMSE=0.019934\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.020214\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_074328\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.021568\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.027276\n",
            "  âœ“ XGBoost: wRMSE=0.021248\n",
            "  âœ“ LightGBM: wRMSE=0.021264\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 3 configurations\n",
            "  âœ“ hybrid_seq: 3 configurations\n",
            "  âœ“ hybrid_par: 3 configurations\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_060912\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019742\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025035\n",
            "  âœ“ XGBoost: wRMSE=0.019590\n",
            "  âœ“ LightGBM: wRMSE=0.019692\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.019587\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_060905\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019984\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025313\n",
            "  âœ“ XGBoost: wRMSE=0.019833\n",
            "  âœ“ LightGBM: wRMSE=0.019934\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.020170\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_055903\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019862\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025166\n",
            "  âœ“ XGBoost: wRMSE=0.019707\n",
            "  âœ“ LightGBM: wRMSE=0.019810\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.019922\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_055737\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019923\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025272\n",
            "  âœ“ XGBoost: wRMSE=0.019770\n",
            "  âœ“ LightGBM: wRMSE=0.019872\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.020317\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_054449\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019923\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025272\n",
            "  âœ“ XGBoost: wRMSE=0.019770\n",
            "  âœ“ LightGBM: wRMSE=0.019872\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.019983\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260118_054329\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019923\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025272\n",
            "  âœ“ XGBoost: wRMSE=0.019770\n",
            "  âœ“ LightGBM: wRMSE=0.019872\n",
            "  âœ“ LSTM: 2 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.020098\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260117_225823\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019923\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025272\n",
            "  âœ“ XGBoost: wRMSE=0.019670\n",
            "  âœ“ LightGBM: wRMSE=0.019891\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.019298\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260117_224157\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.019984\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025313\n",
            "  âœ“ XGBoost: wRMSE=0.019734\n",
            "  âœ“ LightGBM: wRMSE=0.019955\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 2 configurations\n",
            "  âœ“ hybrid_seq: 2 configurations\n",
            "  âœ“ hybrid_par: 2 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.019498\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260117_214245\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.020138\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.025559\n",
            "  âœ“ XGBoost: wRMSE=0.019868\n",
            "  âœ“ LightGBM: wRMSE=0.020111\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 3 configurations\n",
            "  âœ“ hybrid_seq: 3 configurations\n",
            "  âœ“ hybrid_par: 3 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.022855\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260117_211536\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.021521\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.027168\n",
            "  âœ“ XGBoost: wRMSE=0.021449\n",
            "  âœ“ LightGBM: wRMSE=0.021256\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 3 configurations\n",
            "  âœ“ hybrid_seq: 3 configurations\n",
            "  âœ“ hybrid_par: 3 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.021531\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260117_201530\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.023594\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.031224\n",
            "  âœ“ XGBoost: wRMSE=0.023513\n",
            "  âœ“ LightGBM: wRMSE=0.023135\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 3 configurations\n",
            "  âœ“ hybrid_seq: 3 configurations\n",
            "  âœ“ hybrid_par: 3 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.073243\n",
            "\n",
            "[INFO] Loading from RUN_ID: 20260117_195253\n",
            "  âœ“ BASELINE_ZERO: wRMSE=0.022572\n",
            "  âœ“ BASELINE_NAIVE: wRMSE=0.029883\n",
            "  âœ“ XGBoost: wRMSE=0.022493\n",
            "  âœ“ LightGBM: wRMSE=0.022595\n",
            "  âœ“ LSTM: 3 configurations\n",
            "  âœ“ GRU: 3 configurations\n",
            "  âœ“ hybrid_seq: 3 configurations\n",
            "  âœ“ hybrid_par: 3 configurations\n",
            "  âœ“ Ensemble (weighted_average): wRMSE=0.022548\n",
            "\n",
            "[OK] Loaded 203 total results from 14 runs\n",
            "\n",
            "====================================================================================================\n",
            "REPORT 1: ALL RESULTS (203 results from 14 runs)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     rank           run_id                      model   feature_set  \\\n",
              "0       1  20260117_225823  Ensemble-weighted_average    all_models   \n",
              "1       2  20260117_224157                        GRU     neural_80   \n",
              "2       3  20260117_224157  Ensemble-weighted_average    all_models   \n",
              "3       4  20260118_060912                        GRU     neural_40   \n",
              "4       5  20260118_060912                       LSTM     neural_80   \n",
              "..    ...              ...                        ...           ...   \n",
              "198   199  20260117_201530                 Hybrid-Par     neural_40   \n",
              "199   200  20260117_211536                        GRU  xgb_selected   \n",
              "200   201  20260117_195253                 Hybrid-Par  xgb_selected   \n",
              "201   202  20260117_211536                 Hybrid-Par     neural_80   \n",
              "202   203  20260117_211536                 Hybrid-Par  xgb_selected   \n",
              "\n",
              "                train_period             valid_period          test_period  \\\n",
              "0    2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "1    2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "2    2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "3    2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "4    2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "..                       ...                      ...                  ...   \n",
              "198  2015-12-31 - 2023-12-31  2024-01-01 - 2024-12-31  2025-01-01 - latest   \n",
              "199  2020-12-31 - 2024-06-30  2024-07-01 - 2025-03-31  2025-04-01 - latest   \n",
              "200  2004-12-31 - 2021-12-31  2022-01-01 - 2023-12-31  2024-01-01 - latest   \n",
              "201  2020-12-31 - 2024-06-30  2024-07-01 - 2025-03-31  2025-04-01 - latest   \n",
              "202  2020-12-31 - 2024-06-30  2024-07-01 - 2025-03-31  2025-04-01 - latest   \n",
              "\n",
              "     data_start    data_end  test_wrmse  test_diracc  \n",
              "0    2024-01-02  2026-01-14    0.019298     0.560976  \n",
              "1    2024-01-02  2026-01-14    0.019490     0.555556  \n",
              "2    2024-01-02  2026-01-14    0.019498     0.567901  \n",
              "3    2024-01-02  2026-01-14    0.019513     0.552941  \n",
              "4    2024-01-02  2026-01-14    0.019527     0.552941  \n",
              "..          ...         ...         ...          ...  \n",
              "198  2024-01-02  2026-01-14    0.373161     0.577406  \n",
              "199  2024-01-02  2026-01-14    0.388880     0.572973  \n",
              "200  2024-01-02  2026-01-14    0.432002     0.564155  \n",
              "201  2024-01-02  2026-01-14    0.849918     0.518919  \n",
              "202  2024-01-02  2026-01-14    1.253612     0.562162  \n",
              "\n",
              "[203 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e872f29f-b794-4fc6-b1ea-61a562b12bbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>run_id</th>\n",
              "      <th>model</th>\n",
              "      <th>feature_set</th>\n",
              "      <th>train_period</th>\n",
              "      <th>valid_period</th>\n",
              "      <th>test_period</th>\n",
              "      <th>data_start</th>\n",
              "      <th>data_end</th>\n",
              "      <th>test_wrmse</th>\n",
              "      <th>test_diracc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20260117_225823</td>\n",
              "      <td>Ensemble-weighted_average</td>\n",
              "      <td>all_models</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019298</td>\n",
              "      <td>0.560976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20260117_224157</td>\n",
              "      <td>GRU</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019490</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>20260117_224157</td>\n",
              "      <td>Ensemble-weighted_average</td>\n",
              "      <td>all_models</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019498</td>\n",
              "      <td>0.567901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>GRU</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019513</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019527</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>199</td>\n",
              "      <td>20260117_201530</td>\n",
              "      <td>Hybrid-Par</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>2015-12-31 - 2023-12-31</td>\n",
              "      <td>2024-01-01 - 2024-12-31</td>\n",
              "      <td>2025-01-01 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.373161</td>\n",
              "      <td>0.577406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>200</td>\n",
              "      <td>20260117_211536</td>\n",
              "      <td>GRU</td>\n",
              "      <td>xgb_selected</td>\n",
              "      <td>2020-12-31 - 2024-06-30</td>\n",
              "      <td>2024-07-01 - 2025-03-31</td>\n",
              "      <td>2025-04-01 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.388880</td>\n",
              "      <td>0.572973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>201</td>\n",
              "      <td>20260117_195253</td>\n",
              "      <td>Hybrid-Par</td>\n",
              "      <td>xgb_selected</td>\n",
              "      <td>2004-12-31 - 2021-12-31</td>\n",
              "      <td>2022-01-01 - 2023-12-31</td>\n",
              "      <td>2024-01-01 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.432002</td>\n",
              "      <td>0.564155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>202</td>\n",
              "      <td>20260117_211536</td>\n",
              "      <td>Hybrid-Par</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>2020-12-31 - 2024-06-30</td>\n",
              "      <td>2024-07-01 - 2025-03-31</td>\n",
              "      <td>2025-04-01 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.849918</td>\n",
              "      <td>0.518919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>203</td>\n",
              "      <td>20260117_211536</td>\n",
              "      <td>Hybrid-Par</td>\n",
              "      <td>xgb_selected</td>\n",
              "      <td>2020-12-31 - 2024-06-30</td>\n",
              "      <td>2024-07-01 - 2025-03-31</td>\n",
              "      <td>2025-04-01 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>1.253612</td>\n",
              "      <td>0.562162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>203 rows Ã— 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e872f29f-b794-4fc6-b1ea-61a562b12bbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e872f29f-b794-4fc6-b1ea-61a562b12bbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e872f29f-b794-4fc6-b1ea-61a562b12bbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n[OK] BLOCK 31 complete\",\n  \"rows\": 203,\n  \"fields\": [\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 58,\n        \"min\": 1,\n        \"max\": 203,\n        \"num_unique_values\": 203,\n        \"samples\": [\n          16,\n          10,\n          116\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"20260117_214245\",\n          \"20260117_211536\",\n          \"20260117_225823\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"BASELINE_ZERO\",\n          \"GRU\",\n          \"XGBoost\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"all_models\",\n          \"neural_80\",\n          \"shap_top_10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_period\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2023-12-31 - 2024-12-31\",\n          \"2015-12-31 - 2023-12-31\",\n          \"2020-12-31 - 2024-06-30\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_period\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-01-01 - 2025-06-30\",\n          \"2024-01-01 - 2024-12-31\",\n          \"2024-07-01 - 2025-03-31\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_period\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-07-01 - latest\",\n          \"2025-01-01 - latest\",\n          \"2025-04-01 - latest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_start\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2024-01-02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_end\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-01-14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1192558570910519,\n        \"min\": 0.019297763459798976,\n        \"max\": 1.2536118515317711,\n        \"num_unique_values\": 182,\n        \"samples\": [\n          0.0197697330010743\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_diracc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05655169622807044,\n        \"min\": 0.3902439024390244,\n        \"max\": 0.6025641025641025,\n        \"num_unique_values\": 93,\n        \"samples\": [\n          0.4867617107942973\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "REPORT 2: BEST PER MODEL TYPE (across all runs)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   rank                      model   feature_set           run_id  \\\n",
              "0     1  Ensemble-weighted_average    all_models  20260117_225823   \n",
              "1     2                        GRU     neural_80  20260117_224157   \n",
              "2     3                       LSTM     neural_80  20260118_060912   \n",
              "3     4                 Hybrid-Seq     neural_80  20260118_060912   \n",
              "4     5                 Hybrid-Par     neural_40  20260118_060912   \n",
              "5     6                    XGBoost  xgb_selected  20260118_060912   \n",
              "6     7                   LightGBM  xgb_selected  20260118_060912   \n",
              "7     8              BASELINE_ZERO      baseline  20260118_060912   \n",
              "8     9             BASELINE_NAIVE      baseline  20260118_060912   \n",
              "\n",
              "              train_period             valid_period          test_period  \\\n",
              "0  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "1  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "2  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "3  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "4  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "5  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "6  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "7  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "8  2023-12-31 - 2025-05-20  2025-05-21 - 2025-09-10  2025-09-11 - latest   \n",
              "\n",
              "   data_start    data_end  test_wrmse  test_diracc  \n",
              "0  2024-01-02  2026-01-14    0.019298     0.560976  \n",
              "1  2024-01-02  2026-01-14    0.019490     0.555556  \n",
              "2  2024-01-02  2026-01-14    0.019527     0.552941  \n",
              "3  2024-01-02  2026-01-14    0.019555     0.552941  \n",
              "4  2024-01-02  2026-01-14    0.019581     0.552941  \n",
              "5  2024-01-02  2026-01-14    0.019590     0.552941  \n",
              "6  2024-01-02  2026-01-14    0.019692     0.552941  \n",
              "7  2024-01-02  2026-01-14    0.019742     0.447059  \n",
              "8  2024-01-02  2026-01-14    0.025035     0.505882  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23af6d0b-9c1c-42e2-8834-7995bc9e4167\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>model</th>\n",
              "      <th>feature_set</th>\n",
              "      <th>run_id</th>\n",
              "      <th>train_period</th>\n",
              "      <th>valid_period</th>\n",
              "      <th>test_period</th>\n",
              "      <th>data_start</th>\n",
              "      <th>data_end</th>\n",
              "      <th>test_wrmse</th>\n",
              "      <th>test_diracc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Ensemble-weighted_average</td>\n",
              "      <td>all_models</td>\n",
              "      <td>20260117_225823</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019298</td>\n",
              "      <td>0.560976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>GRU</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>20260117_224157</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019490</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019527</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Hybrid-Seq</td>\n",
              "      <td>neural_80</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019555</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Hybrid-Par</td>\n",
              "      <td>neural_40</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019581</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>xgb_selected</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019590</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>LightGBM</td>\n",
              "      <td>xgb_selected</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019692</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>BASELINE_ZERO</td>\n",
              "      <td>baseline</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.019742</td>\n",
              "      <td>0.447059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>BASELINE_NAIVE</td>\n",
              "      <td>baseline</td>\n",
              "      <td>20260118_060912</td>\n",
              "      <td>2023-12-31 - 2025-05-20</td>\n",
              "      <td>2025-05-21 - 2025-09-10</td>\n",
              "      <td>2025-09-11 - latest</td>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>2026-01-14</td>\n",
              "      <td>0.025035</td>\n",
              "      <td>0.505882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23af6d0b-9c1c-42e2-8834-7995bc9e4167')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23af6d0b-9c1c-42e2-8834-7995bc9e4167 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23af6d0b-9c1c-42e2-8834-7995bc9e4167');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n[OK] BLOCK 31 complete\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"BASELINE_ZERO\",\n          \"GRU\",\n          \"XGBoost\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"neural_80\",\n          \"baseline\",\n          \"neural_40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"20260117_225823\",\n          \"20260117_224157\",\n          \"20260118_060912\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_period\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2023-12-31 - 2025-05-20\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_period\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-05-21 - 2025-09-10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_period\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-09-11 - latest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_start\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2024-01-02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_end\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-01-14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001829719332454252,\n        \"min\": 0.019297763459798976,\n        \"max\": 0.0250354406362625,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.0197420871185384\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_diracc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.037495521364365764,\n        \"min\": 0.4470588235294118,\n        \"max\": 0.5609756097560976,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5555555555555556\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "OVERALL BEST MODEL (across all runs)\n",
            "====================================================================================================\n",
            "  Run ID:       20260117_225823\n",
            "  Model:        Ensemble-weighted_average\n",
            "  Feature Set:  all_models\n",
            "  Train Period: 2023-12-31 - 2025-05-20\n",
            "  Valid Period: 2025-05-21 - 2025-09-10\n",
            "  Test Period:  2025-09-11 - latest\n",
            "  Data Range:   2024-01-02 to 2026-01-14\n",
            "  Test wRMSE:   0.019298\n",
            "  Test DirAcc:  0.5610\n",
            "\n",
            "====================================================================================================\n",
            "[INFO] Results output dirs:\n",
            "  - LOCAL: /content/my_project/results_summary\n",
            "  - DRIVE: /content/drive/MyDrive/my_project/results_summary\n",
            "[OK] Saved: all_results.csv\n",
            "[OK] Saved: best_per_model.csv\n",
            "[OK] Saved: all_results_with_params.csv\n",
            "[OK] Saved: RESULTS.md\n",
            "[OK] Saved: bootstrap_ci.csv\n",
            "\n",
            "[INFO] Collecting tomorrow predictions...\n",
            "  âœ“ XGBoost (xgb_selected): 0.0854%\n",
            "  âœ“ LightGBM (xgb_selected): 0.0118%\n",
            "  âœ“ LSTM (neural_40): -0.1322%\n",
            "  âœ“ LSTM (neural_80): -0.1179%\n",
            "  âœ“ GRU (neural_40): -0.1244%\n",
            "  âœ“ GRU (neural_80): -0.1127%\n",
            "  âœ“ Hybrid-Seq (neural_40): -0.0766%\n",
            "  âœ“ Hybrid-Seq (neural_80): -0.0822%\n",
            "  âœ“ Hybrid-Par (neural_40): -0.1054%\n",
            "  âœ“ Hybrid-Par (neural_80): -0.0916%\n",
            "  âœ“ Ensemble (all_models): 0.0486%\n",
            "[OK] Saved: tomorrow_summary.csv (11 predictions)\n",
            "\n",
            "======================================================================\n",
            "TOMORROW PREDICTIONS (next trading day)\n",
            "======================================================================\n",
            " rank      model  feature_set  pred_return_pct\n",
            "    1    XGBoost xgb_selected         0.085397\n",
            "    2   Ensemble   all_models         0.048605\n",
            "    3   LightGBM xgb_selected         0.011827\n",
            "    4 Hybrid-Seq    neural_40        -0.076599\n",
            "    5 Hybrid-Seq    neural_80        -0.082189\n",
            "    6 Hybrid-Par    neural_80        -0.091585\n",
            "    7 Hybrid-Par    neural_40        -0.105360\n",
            "    8        GRU    neural_80        -0.112731\n",
            "    9       LSTM    neural_80        -0.117881\n",
            "   10        GRU    neural_40        -0.124401\n",
            "   11       LSTM    neural_40        -0.132202\n",
            "\n",
            "======================================================================\n",
            "FILES SAVED\n",
            "======================================================================\n",
            "\n",
            "LOCAL: /content/my_project/results_summary/\n",
            "DRIVE: /content/drive/MyDrive/my_project/results_summary/\n",
            "\n",
            "Files:\n",
            "  - all_results.csv          (Report 1: all runs with periods)\n",
            "  - best_per_model.csv       (Report 2: best per model with periods)\n",
            "  - all_results_with_params.csv\n",
            "  - bootstrap_ci.csv         (Report 3: bootstrap confidence intervals)\n",
            "  - tomorrow_summary.csv     (Report 4: tomorrow predictions)\n",
            "  - RESULTS.md\n",
            "[INFO] Downloading files for Git...\n",
            "(Copy these to your repo: results/ folder)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3a5eb533-f181-45c5-8bd5-cb8db22fe142\", \"all_results_with_params.csv\", 88433)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloaded: all_results_with_params.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f87fd9cd-6460-4c17-b04b-0348d8e7a5a5\", \"tomorrow_summary.csv\", 1021)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloaded: tomorrow_summary.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_38ffc0a0-9e75-4a7a-9846-d7eeea7c0d29\", \"best_per_model.csv\", 1807)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloaded: best_per_model.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b2fda901-fb76-4935-8217-c9fde3db16f3\", \"RESULTS.md\", 1980)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloaded: RESULTS.md\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d6b9128a-04b9-40bb-85d3-732b7585ac37\", \"all_results.csv\", 38075)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloaded: all_results.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0afbdd8f-b573-4ba5-9601-ccd0bc6f39f3\", \"bootstrap_ci.csv\", 2871)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloaded: bootstrap_ci.csv\n",
            "\n",
            "[OK] Files downloaded!\n",
            "\n",
            "Next steps:\n",
            "  1. Copy downloaded files to your repo: results/\n",
            "  2. git add results/\n",
            "  3. git commit -m \"Update results\"\n",
            "  4. git push\n",
            "\n",
            "[OK] BLOCK 31 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# BLOCK 31 â€” SUMMARY & RESULTS EXPORT\n",
        "# ============================================================\n",
        "# Scans ALL runs and creates unified comparison tables\n",
        "# Outputs saved to: results_summary/ (project-level, not run-specific)\n",
        "\n",
        "# ============================================================\n",
        "# LOAD ALL MODEL RESULTS (from ALL RUN_IDs)\n",
        "# ============================================================\n",
        "\n",
        "# Scan all runs directories\n",
        "RUNS_DIR_DRIVE = Path(DRIVE_PROJECT_ROOT) / \"runs\"\n",
        "RUNS_DIR_LOCAL = Path(PROJECT_ROOT) / \"runs\"\n",
        "\n",
        "print(f\"[INFO] Scanning all runs in: {RUNS_DIR_DRIVE}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# Get all RUN_ID folders\n",
        "run_folders = []\n",
        "if RUNS_DIR_DRIVE.exists():\n",
        "    run_folders = sorted([d for d in RUNS_DIR_DRIVE.iterdir() if d.is_dir()], reverse=True)\n",
        "    print(f\"[INFO] Found {len(run_folders)} runs\")\n",
        "\n",
        "if len(run_folders) == 0:\n",
        "    print(\"[WARN] No runs found!\")\n",
        "\n",
        "for run_folder in run_folders:\n",
        "    run_id = run_folder.name\n",
        "    models_dir = run_folder / \"models\"\n",
        "    ms_dir = run_folder / \"model_selection\"\n",
        "    config_dir = run_folder / \"config\"\n",
        "\n",
        "    if not models_dir.exists():\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n[INFO] Loading from RUN_ID: {run_id}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Load run config for period info\n",
        "    # --------------------------\n",
        "    run_config = {}\n",
        "    config_path = config_dir / \"run_params.json\"\n",
        "    if config_path.exists():\n",
        "        with open(config_path, \"r\") as f:\n",
        "            run_config = json.load(f)\n",
        "\n",
        "    # Extract period info from config (date-based)\n",
        "    data_cfg = run_config.get(\"data\", {})\n",
        "\n",
        "    train_start = data_cfg.get(\"limit_start_date\", \"N/A\")\n",
        "    train_end = data_cfg.get(\"train_end\", \"N/A\")\n",
        "    train_period = f\"{train_start[:10] if train_start != 'N/A' else 'N/A'} - {train_end}\"\n",
        "\n",
        "    valid_start = data_cfg.get(\"valid_start\", \"N/A\")\n",
        "    valid_end = data_cfg.get(\"valid_end\", \"N/A\")\n",
        "    valid_period = f\"{valid_start} - {valid_end}\"\n",
        "\n",
        "    test_start = data_cfg.get(\"test_start\", \"N/A\")\n",
        "    test_end = data_cfg.get(\"test_end\", \"latest\")\n",
        "    test_period = f\"{test_start} - {test_end if test_end else 'latest'}\"\n",
        "\n",
        "    # Get actual data range from full_df.pkl\n",
        "    data_start = \"N/A\"\n",
        "    data_end = \"N/A\"\n",
        "    try:\n",
        "        # Search paths for full_df.pkl\n",
        "        full_df_paths = [\n",
        "            Path(DRIVE_PROJECT_ROOT) / \"data\" / \"interim\" / \"full_df.pkl\",\n",
        "            Path(PROJECT_ROOT) / \"data\" / \"interim\" / \"full_df.pkl\",\n",
        "        ]\n",
        "\n",
        "        for full_df_path in full_df_paths:\n",
        "            if full_df_path.exists():\n",
        "                full_df = load_pickle(full_df_path)\n",
        "                if hasattr(full_df, 'index') and len(full_df.index) > 0:\n",
        "                    data_start = str(full_df.index.min().date())\n",
        "                    data_end = str(full_df.index.max().date())\n",
        "                break\n",
        "\n",
        "        # Fallback to config if still N/A\n",
        "        if data_start == \"N/A\":\n",
        "            data_start = str(data_cfg.get(\"limit_start_date\", \"N/A\"))[:10]\n",
        "\n",
        "        if data_end == \"N/A\":\n",
        "            test_end = data_cfg.get(\"test_end\")\n",
        "            if test_end:\n",
        "                data_end = test_end\n",
        "            else:\n",
        "                data_end = \"latest\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    [WARN] Could not read data range: {e}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # 0. Baseline Results (from final_metrics.csv)\n",
        "    # --------------------------\n",
        "    xgb_metrics_path = models_dir / \"final_metrics.csv\"\n",
        "\n",
        "    if xgb_metrics_path.exists():\n",
        "        metrics_df = pd.read_csv(xgb_metrics_path)\n",
        "\n",
        "        # Look for baseline rows\n",
        "        for baseline_name in [\"BASELINE_ZERO\", \"BASELINE_NAIVE\"]:\n",
        "            baseline_rows = metrics_df[metrics_df[\"model\"] == baseline_name]\n",
        "            for _, row in baseline_rows.iterrows():\n",
        "                if row[\"split\"] == \"TEST\":\n",
        "                    all_results.append({\n",
        "                        \"run_id\": run_id,\n",
        "                        \"model\": baseline_name,\n",
        "                        \"feature_set\": \"baseline\",\n",
        "                        \"train_period\": train_period,\n",
        "                        \"valid_period\": valid_period,\n",
        "                        \"test_period\": test_period,\n",
        "                        \"data_start\": data_start,\n",
        "                        \"data_end\": data_end,\n",
        "                        \"test_wrmse\": float(row[\"wRMSE\"]),\n",
        "                        \"test_wmae\": float(row.get(\"wMAE\", 0)) if \"wMAE\" in row else None,\n",
        "                        \"test_diracc\": float(row[\"DirAcc\"]),\n",
        "                        \"params\": \"{}\",\n",
        "                    })\n",
        "                    print(f\"  âœ“ {baseline_name}: wRMSE={row['wRMSE']:.6f}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # 1. XGBoost Results\n",
        "    # --------------------------\n",
        "\n",
        "    if xgb_metrics_path.exists():\n",
        "        xgb_metrics = pd.read_csv(xgb_metrics_path)\n",
        "        xgb_test = xgb_metrics[(xgb_metrics[\"model\"] == \"FINAL_XGB\") & (xgb_metrics[\"split\"] == \"TEST\")]\n",
        "\n",
        "        if len(xgb_test) > 0:\n",
        "            xgb_test = xgb_test.iloc[0]\n",
        "\n",
        "            # Load best params\n",
        "            xgb_params = {}\n",
        "            params_path = ms_dir / \"best_params_xgb_reg_t1.pkl\"\n",
        "            if params_path.exists():\n",
        "                xgb_params = load_pickle(params_path)\n",
        "\n",
        "            all_results.append({\n",
        "                \"run_id\": run_id,\n",
        "                \"model\": \"XGBoost\",\n",
        "                \"feature_set\": \"xgb_selected\",\n",
        "                \"train_period\": train_period,\n",
        "                \"valid_period\": valid_period,\n",
        "                \"test_period\": test_period,\n",
        "                \"data_start\": data_start,\n",
        "                \"data_end\": data_end,\n",
        "                \"test_wrmse\": float(xgb_test[\"wRMSE\"]),\n",
        "                \"test_wmae\": float(xgb_test.get(\"wMAE\", 0)) if \"wMAE\" in xgb_test else None,\n",
        "                \"test_diracc\": float(xgb_test[\"DirAcc\"]),\n",
        "                \"params\": str(xgb_params),\n",
        "            })\n",
        "            print(f\"  âœ“ XGBoost: wRMSE={xgb_test['wRMSE']:.6f}\")\n",
        "\n",
        "\n",
        "    # --------------------------\n",
        "    # 1B. LightGBM Results\n",
        "    # --------------------------\n",
        "    lgb_metrics_path = models_dir / \"final_metrics_lgb.csv\"\n",
        "    lgb_json_path = run_folder / \"outputs\" / \"lgb_results.json\"\n",
        "\n",
        "    if lgb_metrics_path.exists():\n",
        "        lgb_metrics = pd.read_csv(lgb_metrics_path)\n",
        "        lgb_test = lgb_metrics[(lgb_metrics[\"model\"] == \"FINAL_LGB\") & (lgb_metrics[\"split\"] == \"TEST\")]\n",
        "        if len(lgb_test) > 0:\n",
        "            lgb_row = lgb_test.iloc[0]\n",
        "\n",
        "            # Load best params\n",
        "            lgb_params = {}\n",
        "            params_path = ms_dir / \"best_params_lgb_reg_t1.pkl\"\n",
        "            if params_path.exists():\n",
        "                lgb_params = load_pickle(params_path)\n",
        "\n",
        "            all_results.append({\n",
        "                \"run_id\": run_id,\n",
        "                \"model\": \"LightGBM\",\n",
        "                \"feature_set\": \"xgb_selected\",\n",
        "                \"train_period\": train_period,\n",
        "                \"valid_period\": valid_period,\n",
        "                \"test_period\": test_period,\n",
        "                \"data_start\": data_start,\n",
        "                \"data_end\": data_end,\n",
        "                \"test_wrmse\": float(lgb_row[\"wRMSE\"]),\n",
        "                \"test_wmae\": float(lgb_row[\"wMAE\"]) if \"wMAE\" in lgb_row.index else None,\n",
        "                \"test_diracc\": float(lgb_row[\"DirAcc\"]),\n",
        "                \"params\": str(lgb_params),\n",
        "            })\n",
        "            print(f\"  âœ“ LightGBM: wRMSE={lgb_row['wRMSE']:.6f}\")\n",
        "    elif lgb_json_path.exists():\n",
        "        # Try loading from JSON\n",
        "        with open(lgb_json_path, \"r\") as f:\n",
        "            lgb_data = json.load(f)\n",
        "\n",
        "        all_results.append({\n",
        "            \"run_id\": run_id,\n",
        "            \"model\": \"LightGBM\",\n",
        "            \"feature_set\": \"xgb_selected\",\n",
        "            \"train_period\": train_period,\n",
        "            \"valid_period\": valid_period,\n",
        "            \"test_period\": test_period,\n",
        "            \"data_start\": data_start,\n",
        "            \"data_end\": data_end,\n",
        "            \"test_wrmse\": float(lgb_data.get(\"test_wrmse\", 0)),\n",
        "            \"test_wmae\": float(lgb_data.get(\"test_wmae\", 0)) if \"test_wmae\" in lgb_data else None,\n",
        "            \"test_diracc\": float(lgb_data.get(\"test_diracc\", 0)),\n",
        "            \"params\": str({k: lgb_data.get(k) for k in [\"num_leaves\", \"max_depth\", \"learning_rate\", \"subsample\", \"colsample_bytree\"] if k in lgb_data}),\n",
        "        })\n",
        "        print(f\"  âœ“ LightGBM: wRMSE={lgb_data.get('test_wrmse', 0):.6f}\")\n",
        "\n",
        "# --------------------------\n",
        "    # 2. LSTM & GRU Results\n",
        "    # --------------------------\n",
        "    for model_type in [\"lstm\", \"gru\"]:\n",
        "        summary_path = models_dir / f\"{model_type}_summary.csv\"\n",
        "\n",
        "        if summary_path.exists():\n",
        "            df = pd.read_csv(summary_path)\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                all_results.append({\n",
        "                    \"run_id\": run_id,\n",
        "                    \"model\": model_type.upper(),\n",
        "                    \"feature_set\": row[\"feature_set\"],\n",
        "                    \"train_period\": train_period,\n",
        "                    \"valid_period\": valid_period,\n",
        "                    \"test_period\": test_period,\n",
        "                    \"data_start\": data_start,\n",
        "                    \"data_end\": data_end,\n",
        "                    \"test_wrmse\": float(row[\"model_test_wrmse\"]),\n",
        "                    \"test_wmae\": float(row.get(\"model_test_wmae\", 0)) if \"model_test_wmae\" in row else None,\n",
        "                    \"test_diracc\": float(row[\"model_test_diracc\"]),\n",
        "                    \"params\": str(run_config.get(model_type, {})),\n",
        "                })\n",
        "            print(f\"  âœ“ {model_type.upper()}: {len(df)} configurations\")\n",
        "\n",
        "    # --------------------------\n",
        "    # 3. Hybrid Results\n",
        "    # --------------------------\n",
        "    for hybrid_type in [\"hybrid_seq\", \"hybrid_par\"]:\n",
        "        summary_path = models_dir / f\"{hybrid_type}_summary.csv\"\n",
        "\n",
        "        if summary_path.exists():\n",
        "            df = pd.read_csv(summary_path)\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                model_name = \"Hybrid-Seq\" if hybrid_type == \"hybrid_seq\" else \"Hybrid-Par\"\n",
        "                all_results.append({\n",
        "                    \"run_id\": run_id,\n",
        "                    \"model\": model_name,\n",
        "                    \"feature_set\": row[\"feature_set\"],\n",
        "                    \"train_period\": train_period,\n",
        "                    \"valid_period\": valid_period,\n",
        "                    \"test_period\": test_period,\n",
        "                    \"data_start\": data_start,\n",
        "                    \"data_end\": data_end,\n",
        "                    \"test_wrmse\": float(row[\"model_test_wrmse\"]),\n",
        "                    \"test_wmae\": float(row.get(\"model_test_wmae\", 0)) if \"model_test_wmae\" in row else None,\n",
        "                    \"test_diracc\": float(row[\"model_test_diracc\"]),\n",
        "                    \"params\": str(run_config.get(hybrid_type, {})),\n",
        "                })\n",
        "            print(f\"  âœ“ {hybrid_type}: {len(df)} configurations\")\n",
        "\n",
        "    # --------------------------\n",
        "    # 4. Ensemble Results\n",
        "    # --------------------------\n",
        "    ensemble_path = run_folder / \"outputs\" / \"ensemble_results.json\"\n",
        "\n",
        "    if ensemble_path.exists():\n",
        "        with open(ensemble_path, \"r\") as f:\n",
        "            ens_data = json.load(f)\n",
        "\n",
        "        ens_method = ens_data.get(\"method\", \"weighted_average\")\n",
        "        all_results.append({\n",
        "            \"run_id\": run_id,\n",
        "            \"model\": f\"Ensemble-{ens_method}\",\n",
        "            \"feature_set\": \"all_models\",\n",
        "            \"train_period\": train_period,\n",
        "            \"valid_period\": valid_period,\n",
        "            \"test_period\": test_period,\n",
        "            \"data_start\": data_start,\n",
        "            \"data_end\": data_end,\n",
        "            \"test_wrmse\": float(ens_data.get(\"test_wrmse\", 0)),\n",
        "            \"test_wmae\": float(ens_data.get(\"test_wmae\", 0)) if \"test_wmae\" in ens_data else None,\n",
        "            \"test_diracc\": float(ens_data.get(\"test_diracc\", 0)),\n",
        "            \"params\": str(ens_data.get(\"weights\", {})),\n",
        "        })\n",
        "        print(f\"  âœ“ Ensemble ({ens_method}): wRMSE={ens_data.get('test_wrmse', 0):.6f}\")\n",
        "\n",
        "print(f\"\\n[OK] Loaded {len(all_results)} total results from {len(run_folders)} runs\")\n",
        "\n",
        "# ============================================================\n",
        "# CREATE COMPARISON TABLES\n",
        "# ============================================================\n",
        "\n",
        "if len(all_results) == 0:\n",
        "    print(\"[ERROR] No results found! Run training sections first.\")\n",
        "else:\n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Filter out invalid results (wRMSE=0 is impossible except for perfect predictions)\n",
        "    # Keep BASELINE_ZERO which has legitimate 0 values for some metrics\n",
        "    invalid_mask = (\n",
        "        (results_df[\"test_wrmse\"] == 0) &\n",
        "        (results_df[\"test_diracc\"] == 0) &\n",
        "        (~results_df[\"model\"].str.contains(\"BASELINE\", case=False, na=False))\n",
        "    )\n",
        "    if invalid_mask.any():\n",
        "        n_invalid = invalid_mask.sum()\n",
        "        print(f\"[WARN] Filtering {n_invalid} invalid results (wRMSE=0 and DirAcc=0)\")\n",
        "        results_df = results_df[~invalid_mask]\n",
        "\n",
        "    # Sort by test_wrmse (lower is better)\n",
        "    results_df = results_df.sort_values(\"test_wrmse\").reset_index(drop=True)\n",
        "\n",
        "    # Add rank\n",
        "    results_df.insert(0, \"rank\", range(1, len(results_df) + 1))\n",
        "\n",
        "    # Count unique runs\n",
        "    n_runs = results_df[\"run_id\"].nunique()\n",
        "    n_models = len(results_df)\n",
        "\n",
        "    # =========================================\n",
        "    # REPORT 1: All Results from All RUN_IDs\n",
        "    # =========================================\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"REPORT 1: ALL RESULTS ({n_models} results from {n_runs} runs)\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    display_cols = [\"rank\", \"run_id\", \"model\", \"feature_set\",\n",
        "                    \"train_period\", \"valid_period\", \"test_period\", \"data_start\", \"data_end\",\n",
        "                    \"test_wrmse\", \"test_diracc\"]\n",
        "    display(results_df[display_cols])\n",
        "\n",
        "    # =========================================\n",
        "    # REPORT 2: Best per Model Type (across ALL runs)\n",
        "    # =========================================\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"REPORT 2: BEST PER MODEL TYPE (across all runs)\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Get best (lowest wRMSE) for each model type across ALL runs\n",
        "    best_per_model = results_df.loc[results_df.groupby(\"model\")[\"test_wrmse\"].idxmin()]\n",
        "    best_per_model = best_per_model.drop(columns=[\"rank\"], errors=\"ignore\")\n",
        "    best_per_model = best_per_model.sort_values(\"test_wrmse\").reset_index(drop=True)\n",
        "    best_per_model.insert(0, \"rank\", range(1, len(best_per_model) + 1))\n",
        "\n",
        "    display_cols_best = [\"rank\", \"model\", \"feature_set\", \"run_id\",\n",
        "                         \"train_period\", \"valid_period\", \"test_period\", \"data_start\", \"data_end\",\n",
        "                         \"test_wrmse\", \"test_diracc\"]\n",
        "    display(best_per_model[display_cols_best])\n",
        "\n",
        "    # =========================================\n",
        "    # Overall Best Model\n",
        "    # =========================================\n",
        "    best = results_df.iloc[0]\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"OVERALL BEST MODEL (across all runs)\")\n",
        "    print(\"=\"*100)\n",
        "    print(f\"  Run ID:       {best['run_id']}\")\n",
        "    print(f\"  Model:        {best['model']}\")\n",
        "    print(f\"  Feature Set:  {best['feature_set']}\")\n",
        "    print(f\"  Train Period: {best['train_period']}\")\n",
        "    print(f\"  Valid Period: {best['valid_period']}\")\n",
        "    print(f\"  Test Period:  {best['test_period']}\")\n",
        "    print(f\"  Data Range:   {best['data_start']} to {best['data_end']}\")\n",
        "    print(f\"  Test wRMSE:   {best['test_wrmse']:.6f}\")\n",
        "    print(f\"  Test DirAcc:  {best['test_diracc']:.4f}\")\n",
        "\n",
        "    # =========================================\n",
        "    # REPORT 3: Bootstrap Confidence Intervals\n",
        "    # =========================================\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "\n",
        "# ============================================================\n",
        "# SAVE RESULTS (LOCAL + DRIVE)\n",
        "# ============================================================\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Directories - save to project level (not run-specific)\n",
        "RESULTS_LOCAL = ensure_dir(Path(PROJECT_ROOT) / \"results_summary\")\n",
        "RESULTS_DRIVE = ensure_dir(Path(DRIVE_PROJECT_ROOT) / \"results_summary\")\n",
        "\n",
        "print(f\"[INFO] Results output dirs:\")\n",
        "print(f\"  - LOCAL: {RESULTS_LOCAL}\")\n",
        "print(f\"  - DRIVE: {RESULTS_DRIVE}\")\n",
        "\n",
        "if len(all_results) > 0:\n",
        "    n_runs = results_df[\"run_id\"].nunique()\n",
        "\n",
        "    # =========================================\n",
        "    # Save Report 1: All Results (all runs)\n",
        "    # =========================================\n",
        "    all_results_cols = [\"rank\", \"run_id\", \"model\", \"feature_set\",\n",
        "                        \"train_period\", \"valid_period\", \"test_period\",\n",
        "                        \"data_start\", \"data_end\",\n",
        "                        \"test_wrmse\", \"test_wmae\", \"test_diracc\"]\n",
        "    all_results_df = results_df[all_results_cols].copy()\n",
        "\n",
        "    all_results_df.to_csv(RESULTS_LOCAL / \"all_results.csv\", index=False)\n",
        "    all_results_df.to_csv(RESULTS_DRIVE / \"all_results.csv\", index=False)\n",
        "    print(\"[OK] Saved: all_results.csv\")\n",
        "\n",
        "    # =========================================\n",
        "    # Save Report 2: Best per Model (all runs)\n",
        "    # =========================================\n",
        "    best_cols = [\"rank\", \"model\", \"feature_set\", \"run_id\",\n",
        "                 \"train_period\", \"valid_period\", \"test_period\",\n",
        "                 \"data_start\", \"data_end\",\n",
        "                 \"test_wrmse\", \"test_wmae\", \"test_diracc\"]\n",
        "    best_per_model_df = best_per_model[best_cols].copy()\n",
        "\n",
        "    best_per_model_df.to_csv(RESULTS_LOCAL / \"best_per_model.csv\", index=False)\n",
        "    best_per_model_df.to_csv(RESULTS_DRIVE / \"best_per_model.csv\", index=False)\n",
        "    print(\"[OK] Saved: best_per_model.csv\")\n",
        "\n",
        "    # =========================================\n",
        "    # Save Full Results with Params\n",
        "    # =========================================\n",
        "    results_df.to_csv(RESULTS_LOCAL / \"all_results_with_params.csv\", index=False)\n",
        "    results_df.to_csv(RESULTS_DRIVE / \"all_results_with_params.csv\", index=False)\n",
        "    print(\"[OK] Saved: all_results_with_params.csv\")\n",
        "\n",
        "    # =========================================\n",
        "    # Generate RESULTS.md\n",
        "    # =========================================\n",
        "    best = results_df.iloc[0]\n",
        "\n",
        "    md_lines = [\n",
        "        \"# Results Summary\",\n",
        "        \"\",\n",
        "        f\"**Last Updated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
        "        \"\",\n",
        "        f\"**Total Runs:** {n_runs} | **Total Configurations:** {len(results_df)}\",\n",
        "        \"\",\n",
        "        \"---\",\n",
        "        \"\",\n",
        "        \"## ðŸ† Best Results (Top 10)\",\n",
        "        \"\",\n",
        "        \"| # | Model | Feature Set | wRMSE | DirAcc | Run |\",\n",
        "        \"|---|-------|-------------|-------|--------|-----|\",\n",
        "    ]\n",
        "\n",
        "    for _, row in results_df.head(10).iterrows():\n",
        "        wrmse = f\"{row['test_wrmse']:.6f}\" if pd.notna(row[\"test_wrmse\"]) else \"-\"\n",
        "        diracc = f\"{row['test_diracc']:.2%}\" if pd.notna(row[\"test_diracc\"]) else \"-\"\n",
        "        run_short = row['run_id'][-6:] if len(str(row['run_id'])) > 6 else row['run_id']\n",
        "        md_lines.append(f\"| {row['rank']} | {row['model']} | {row['feature_set']} | {wrmse} | {diracc} | {run_short} |\")\n",
        "\n",
        "    md_lines.extend([\n",
        "        \"\",\n",
        "        \"---\",\n",
        "        \"\",\n",
        "        \"## ðŸ“Š Best per Model Type\",\n",
        "        \"\",\n",
        "        \"| # | Model | Feature Set | wRMSE | DirAcc |\",\n",
        "        \"|---|-------|-------------|-------|--------|\",\n",
        "    ])\n",
        "\n",
        "    for _, row in best_per_model.iterrows():\n",
        "        wrmse = f\"{row['test_wrmse']:.6f}\" if pd.notna(row[\"test_wrmse\"]) else \"-\"\n",
        "        diracc = f\"{row['test_diracc']:.2%}\" if pd.notna(row[\"test_diracc\"]) else \"-\"\n",
        "        md_lines.append(f\"| {row['rank']} | {row['model']} | {row['feature_set']} | {wrmse} | {diracc} |\")\n",
        "\n",
        "    md_lines.extend([\n",
        "        \"\",\n",
        "        \"---\",\n",
        "        \"\",\n",
        "        \"## ðŸ¥‡ Overall Best\",\n",
        "        \"\",\n",
        "        \"| Metric | Value |\",\n",
        "        \"|--------|-------|\",\n",
        "        f\"| Model | **{best['model']}** |\",\n",
        "        f\"| Feature Set | {best['feature_set']} |\",\n",
        "        f\"| wRMSE | {best['test_wrmse']:.6f} |\",\n",
        "        f\"| DirAcc | {best['test_diracc']:.2%} |\",\n",
        "        f\"| Run ID | {best['run_id']} |\",\n",
        "        f\"| Data Range | {best['data_start']} â†’ {best['data_end']} |\",\n",
        "        \"\",\n",
        "        \"---\",\n",
        "        \"\",\n",
        "        \"## ðŸ“– Metrics\",\n",
        "        \"\",\n",
        "        \"| Metric | Description |\",\n",
        "        \"|--------|-------------|\",\n",
        "        \"| wRMSE | Weighted Root Mean Squared Error (â†“ lower is better) |\",\n",
        "        \"| DirAcc | Directional Accuracy (â†‘ higher is better) |\",\n",
        "        \"\",\n",
        "        \"*Full details with period configurations available in CSV files.*\",\n",
        "    ])\n",
        "\n",
        "    md_content = \"\\n\".join(md_lines)\n",
        "\n",
        "    (RESULTS_LOCAL / \"RESULTS.md\").write_text(md_content, encoding=\"utf-8\")\n",
        "    (RESULTS_DRIVE / \"RESULTS.md\").write_text(md_content, encoding=\"utf-8\")\n",
        "    print(\"[OK] Saved: RESULTS.md\")\n",
        "\n",
        "    # =========================================\n",
        "    # Save Bootstrap CI (if computed)\n",
        "    # =========================================\n",
        "    if 'bootstrap_df' in dir() and len(bootstrap_df) > 0:\n",
        "        bootstrap_df.to_csv(RESULTS_LOCAL / \"bootstrap_ci.csv\", index=False)\n",
        "        bootstrap_df.to_csv(RESULTS_DRIVE / \"bootstrap_ci.csv\", index=False)\n",
        "        print(\"[OK] Saved: bootstrap_ci.csv\")\n",
        "\n",
        "    # =========================================\n",
        "    # TOMORROW PREDICTIONS SUMMARY\n",
        "    # =========================================\n",
        "    # Collect tomorrow.csv from all models across all runs\n",
        "\n",
        "    print(\"\\n[INFO] Collecting tomorrow predictions...\")\n",
        "\n",
        "    tomorrow_results = []\n",
        "\n",
        "    # Use the most recent run for tomorrow predictions\n",
        "    if len(run_folders) > 0:\n",
        "        latest_run = run_folders[0]  # Already sorted desc\n",
        "        run_id = latest_run.name\n",
        "        pred_dir = latest_run / \"predictions\"\n",
        "        outputs_dir = latest_run / \"outputs\"\n",
        "\n",
        "        # Model paths for tomorrow.csv\n",
        "        tomorrow_paths = {\n",
        "            (\"XGBoost\", \"xgb_selected\"): pred_dir / \"xgb\" / \"tomorrow.csv\",\n",
        "            (\"LightGBM\", \"xgb_selected\"): pred_dir / \"lgb\" / \"tomorrow.csv\",\n",
        "            (\"LSTM\", \"xgb_selected\"): pred_dir / \"lstm_xgb_selected\" / \"tomorrow.csv\",\n",
        "            (\"LSTM\", \"neural_40\"): pred_dir / \"lstm_neural_40\" / \"tomorrow.csv\",\n",
        "            (\"LSTM\", \"neural_80\"): pred_dir / \"lstm_neural_80\" / \"tomorrow.csv\",\n",
        "            (\"GRU\", \"xgb_selected\"): pred_dir / \"gru_xgb_selected\" / \"tomorrow.csv\",\n",
        "            (\"GRU\", \"neural_40\"): pred_dir / \"gru_neural_40\" / \"tomorrow.csv\",\n",
        "            (\"GRU\", \"neural_80\"): pred_dir / \"gru_neural_80\" / \"tomorrow.csv\",\n",
        "            (\"Hybrid-Seq\", \"xgb_selected\"): pred_dir / \"hybrid_seq_xgb_selected\" / \"tomorrow.csv\",\n",
        "            (\"Hybrid-Seq\", \"neural_40\"): pred_dir / \"hybrid_seq_neural_40\" / \"tomorrow.csv\",\n",
        "            (\"Hybrid-Seq\", \"neural_80\"): pred_dir / \"hybrid_seq_neural_80\" / \"tomorrow.csv\",\n",
        "            (\"Hybrid-Par\", \"xgb_selected\"): pred_dir / \"hybrid_par_xgb_selected\" / \"tomorrow.csv\",\n",
        "            (\"Hybrid-Par\", \"neural_40\"): pred_dir / \"hybrid_par_neural_40\" / \"tomorrow.csv\",\n",
        "            (\"Hybrid-Par\", \"neural_80\"): pred_dir / \"hybrid_par_neural_80\" / \"tomorrow.csv\",\n",
        "            (\"Ensemble\", \"all_models\"): outputs_dir / \"tomorrow.csv\",\n",
        "        }\n",
        "\n",
        "        for (model, feature_set), path in tomorrow_paths.items():\n",
        "            if path.exists():\n",
        "                try:\n",
        "                    df = pd.read_csv(path)\n",
        "                    if len(df) > 0:\n",
        "                        row = df.iloc[0]\n",
        "                        tomorrow_results.append({\n",
        "                            \"run_id\": run_id,\n",
        "                            \"model\": model,\n",
        "                            \"feature_set\": row.get(\"feature_set\", feature_set),\n",
        "                            \"last_data_date\": row.get(\"last_data_date\", \"\"),\n",
        "                            \"pred_logret\": float(row.get(\"pred_logret\", 0)),\n",
        "                            \"pred_return_pct\": float(row.get(\"pred_return_pct\", row.get(\"pred_logret\", 0) * 100)),\n",
        "                        })\n",
        "                        print(f\"  âœ“ {model} ({feature_set}): {row.get('pred_return_pct', 0):.4f}%\")\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "    if tomorrow_results:\n",
        "        tomorrow_df = pd.DataFrame(tomorrow_results)\n",
        "        # Sort by predicted return (highest first for bullish, lowest first for bearish)\n",
        "        tomorrow_df = tomorrow_df.sort_values(\"pred_return_pct\", ascending=False).reset_index(drop=True)\n",
        "        tomorrow_df.insert(0, \"rank\", range(1, len(tomorrow_df) + 1))\n",
        "\n",
        "        tomorrow_df.to_csv(RESULTS_LOCAL / \"tomorrow_summary.csv\", index=False)\n",
        "        tomorrow_df.to_csv(RESULTS_DRIVE / \"tomorrow_summary.csv\", index=False)\n",
        "        print(f\"[OK] Saved: tomorrow_summary.csv ({len(tomorrow_df)} predictions)\")\n",
        "\n",
        "        # Show summary\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"TOMORROW PREDICTIONS (next trading day)\")\n",
        "        print(\"=\"*70)\n",
        "        print(tomorrow_df[[\"rank\", \"model\", \"feature_set\", \"pred_return_pct\"]].to_string(index=False))\n",
        "    else:\n",
        "        print(\"[WARN] No tomorrow predictions found\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FILES SAVED\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nLOCAL: {RESULTS_LOCAL}/\")\n",
        "    print(f\"DRIVE: {RESULTS_DRIVE}/\")\n",
        "    print(\"\\nFiles:\")\n",
        "    print(\"  - all_results.csv          (Report 1: all runs with periods)\")\n",
        "    print(\"  - best_per_model.csv       (Report 2: best per model with periods)\")\n",
        "    print(\"  - all_results_with_params.csv\")\n",
        "    print(\"  - bootstrap_ci.csv         (Report 3: bootstrap confidence intervals)\")\n",
        "    print(\"  - tomorrow_summary.csv     (Report 4: tomorrow predictions)\")\n",
        "    print(\"  - RESULTS.md\")\n",
        "\n",
        "# ============================================================\n",
        "# DOWNLOAD FILES FOR GIT (Colab only)\n",
        "# ============================================================\n",
        "\n",
        "if IN_COLAB and len(all_results) > 0:\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"[INFO] Downloading files for Git...\")\n",
        "    print(\"(Copy these to your repo: results/ folder)\\n\")\n",
        "\n",
        "    # Download from DRIVE (project level - persistent location)\n",
        "    for f in RESULTS_DRIVE.glob(\"*\"):\n",
        "        if f.is_file() and f.suffix in [\".csv\", \".md\"]:\n",
        "            files.download(str(f))\n",
        "            print(f\"  Downloaded: {f.name}\")\n",
        "\n",
        "    print(\"\\n[OK] Files downloaded!\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"  1. Copy downloaded files to your repo: results/\")\n",
        "    print(\"  2. git add results/\")\n",
        "    print('  3. git commit -m \"Update results\"')\n",
        "    print(\"  4. git push\")\n",
        "else:\n",
        "    if len(all_results) > 0:\n",
        "        print(f\"[INFO] Files saved to: {RESULTS_DRIVE}\")\n",
        "    else:\n",
        "        print(\"[INFO] No results to download\")\n",
        "\n",
        "print(\"\\n[OK] BLOCK 31 complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2xkUBUPgSAXN",
      "metadata": {
        "id": "2xkUBUPgSAXN"
      },
      "source": [
        "## BLOCK 32 â€” BOOTSTRAP CONFIDENCE INTERVALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "xQJnWw2VSAXN",
      "metadata": {
        "id": "xQJnWw2VSAXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f92a8747-a503-45d9-b1bb-b11403a50333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running Bootstrap Confidence Interval Analysis...\n",
            "[INFO] Models to analyze: 9 (from best_per_model)\n",
            "\n",
            "[INFO] Analyzing: Ensemble-weighted_average (run: 20260117_225823)\n",
            "  wRMSE: 0.019298 [0.015746, 0.023338]\n",
            "  DirAcc: 0.5610 [0.4512, 0.6707]\n",
            "\n",
            "[INFO] Analyzing: GRU (run: 20260117_224157)\n",
            "  wRMSE: 0.020152 [0.016277, 0.024232]\n",
            "  DirAcc: 0.5556 [0.4444, 0.6543]\n",
            "\n",
            "[INFO] Analyzing: LSTM (run: 20260118_060912)\n",
            "  wRMSE: 0.019712 [0.015849, 0.023570]\n",
            "  DirAcc: 0.5529 [0.4471, 0.6471]\n",
            "\n",
            "[INFO] Analyzing: Hybrid-Seq (run: 20260118_060912)\n",
            "  wRMSE: 0.019555 [0.015678, 0.023336]\n",
            "  DirAcc: 0.5529 [0.4471, 0.6471]\n",
            "\n",
            "[INFO] Analyzing: Hybrid-Par (run: 20260118_060912)\n",
            "  wRMSE: 0.019581 [0.015702, 0.023368]\n",
            "  DirAcc: 0.5529 [0.4471, 0.6471]\n",
            "\n",
            "[INFO] Analyzing: XGBoost (run: 20260118_060912)\n",
            "  wRMSE: 0.019590 [0.015712, 0.023379]\n",
            "  DirAcc: 0.5529 [0.4471, 0.6471]\n",
            "\n",
            "[INFO] Analyzing: LightGBM (run: 20260118_060912)\n",
            "  wRMSE: 0.019692 [0.015814, 0.023530]\n",
            "  DirAcc: 0.5529 [0.4471, 0.6471]\n",
            "\n",
            "[INFO] Analyzing: BASELINE_ZERO (run: 20260118_060912)\n",
            "  wRMSE: 0.019742 [0.015870, 0.023622]\n",
            "  DirAcc: 0.4471 [0.3529, 0.5529]\n",
            "\n",
            "[INFO] Analyzing: BASELINE_NAIVE (run: 20260118_060912)\n",
            "  wRMSE: 0.025035 [0.020933, 0.028456]\n",
            "  DirAcc: 0.5059 [0.4000, 0.6118]\n",
            "\n",
            "====================================================================================================\n",
            "BOOTSTRAP CONFIDENCE INTERVALS (95% CI, n=1000)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   rank                      model     wrmse  wrmse_ci_lower  wrmse_ci_upper  \\\n",
              "0     1  Ensemble-weighted_average  0.019298        0.015746        0.023338   \n",
              "1     2                 Hybrid-Seq  0.019555        0.015678        0.023336   \n",
              "2     3                 Hybrid-Par  0.019581        0.015702        0.023368   \n",
              "3     4                    XGBoost  0.019590        0.015712        0.023379   \n",
              "4     5                   LightGBM  0.019692        0.015814        0.023530   \n",
              "5     6                       LSTM  0.019712        0.015849        0.023570   \n",
              "6     7              BASELINE_ZERO  0.019742        0.015870        0.023622   \n",
              "7     8                        GRU  0.020152        0.016277        0.024232   \n",
              "8     9             BASELINE_NAIVE  0.025035        0.020933        0.028456   \n",
              "\n",
              "     diracc  diracc_ci_lower  diracc_ci_upper  \n",
              "0  0.560976         0.451220         0.670732  \n",
              "1  0.552941         0.447059         0.647059  \n",
              "2  0.552941         0.447059         0.647059  \n",
              "3  0.552941         0.447059         0.647059  \n",
              "4  0.552941         0.447059         0.647059  \n",
              "5  0.552941         0.447059         0.647059  \n",
              "6  0.447059         0.352941         0.552941  \n",
              "7  0.555556         0.444444         0.654321  \n",
              "8  0.505882         0.400000         0.611765  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-221fa67b-f67c-486b-a330-84654effead3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>model</th>\n",
              "      <th>wrmse</th>\n",
              "      <th>wrmse_ci_lower</th>\n",
              "      <th>wrmse_ci_upper</th>\n",
              "      <th>diracc</th>\n",
              "      <th>diracc_ci_lower</th>\n",
              "      <th>diracc_ci_upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Ensemble-weighted_average</td>\n",
              "      <td>0.019298</td>\n",
              "      <td>0.015746</td>\n",
              "      <td>0.023338</td>\n",
              "      <td>0.560976</td>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.670732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Hybrid-Seq</td>\n",
              "      <td>0.019555</td>\n",
              "      <td>0.015678</td>\n",
              "      <td>0.023336</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Hybrid-Par</td>\n",
              "      <td>0.019581</td>\n",
              "      <td>0.015702</td>\n",
              "      <td>0.023368</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.019590</td>\n",
              "      <td>0.015712</td>\n",
              "      <td>0.023379</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.019692</td>\n",
              "      <td>0.015814</td>\n",
              "      <td>0.023530</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>0.019712</td>\n",
              "      <td>0.015849</td>\n",
              "      <td>0.023570</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>BASELINE_ZERO</td>\n",
              "      <td>0.019742</td>\n",
              "      <td>0.015870</td>\n",
              "      <td>0.023622</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.552941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>GRU</td>\n",
              "      <td>0.020152</td>\n",
              "      <td>0.016277</td>\n",
              "      <td>0.024232</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.654321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>BASELINE_NAIVE</td>\n",
              "      <td>0.025035</td>\n",
              "      <td>0.020933</td>\n",
              "      <td>0.028456</td>\n",
              "      <td>0.505882</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.611765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-221fa67b-f67c-486b-a330-84654effead3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-221fa67b-f67c-486b-a330-84654effead3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-221fa67b-f67c-486b-a330-84654effead3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"[WARN] No bootstrap results computed\\\")\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"GRU\",\n          \"Hybrid-Seq\",\n          \"LSTM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wrmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0018041721280720514,\n        \"min\": 0.019297763459798976,\n        \"max\": 0.025035440636262476,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.02015245730951818,\n          0.01955548944070835,\n          0.019711822220495927\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wrmse_ci_lower\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0017101355146107292,\n        \"min\": 0.015678353339250512,\n        \"max\": 0.020932722930013272,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.01627722011908195,\n          0.015678353339250512,\n          0.01584933029600839\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wrmse_ci_upper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001660002245743824,\n        \"min\": 0.02333596356326942,\n        \"max\": 0.02845593166575725,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.02423184551014407,\n          0.02333596356326942,\n          0.023570101063414714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diracc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.037495521364365764,\n        \"min\": 0.4470588235294118,\n        \"max\": 0.5609756097560976,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5529411764705883,\n          0.5058823529411764,\n          0.4470588235294118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diracc_ci_lower\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0334113075813605,\n        \"min\": 0.35294117647058826,\n        \"max\": 0.45121951219512196,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4470588235294118,\n          0.4,\n          0.35294117647058826\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diracc_ci_upper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0347120881115051,\n        \"min\": 0.5529411764705883,\n        \"max\": 0.6707317073170732,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6470588235294118,\n          0.611764705882353,\n          0.5529411764705883\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] Saved: bootstrap_ci.csv\n",
            "[OK] Saved: bootstrap_ci.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BOOTSTRAP CONFIDENCE INTERVALS\n",
        "# ============================================================\n",
        "\n",
        "# Run bootstrap analysis ONLY on models in best_per_model\n",
        "# This provides statistical confidence for the metrics\n",
        "\n",
        "# --- Bootstrap helper functions (standalone) ---\n",
        "\n",
        "def bootstrap_metric(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    weights = None,\n",
        "    metric_fn = None,\n",
        "    n_bootstrap: int = 1000,\n",
        "    confidence_level: float = 0.95,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    \"\"\"Compute bootstrap confidence interval for a single metric.\"\"\"\n",
        "    y_true = _to_np(y_true)\n",
        "    y_pred = _to_np(y_pred)\n",
        "    n = len(y_true)\n",
        "\n",
        "    if weights is None:\n",
        "        weights = np.ones(n)\n",
        "    weights = _to_np(weights)\n",
        "\n",
        "    rng = np.random.RandomState(random_state)\n",
        "\n",
        "    # Point estimate\n",
        "    point_estimate = metric_fn(y_true, y_pred, weights)\n",
        "\n",
        "    # Bootstrap samples\n",
        "    bootstrap_values = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.randint(0, n, size=n)\n",
        "        val = metric_fn(y_true[idx], y_pred[idx], weights[idx])\n",
        "        bootstrap_values.append(val)\n",
        "\n",
        "    bootstrap_values = np.array(bootstrap_values)\n",
        "\n",
        "    # Confidence interval (percentile method)\n",
        "    alpha = 1 - confidence_level\n",
        "    ci_lower = np.percentile(bootstrap_values, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_values, 100 * (1 - alpha / 2))\n",
        "\n",
        "    return {\n",
        "        \"point_estimate\": float(point_estimate),\n",
        "        \"ci_lower\": float(ci_lower),\n",
        "        \"ci_upper\": float(ci_upper),\n",
        "        \"std\": float(np.std(bootstrap_values)),\n",
        "        \"n_bootstrap\": n_bootstrap,\n",
        "        \"confidence_level\": confidence_level,\n",
        "    }\n",
        "\n",
        "\n",
        "def bootstrap_all_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    weights = None,\n",
        "    n_bootstrap: int = 1000,\n",
        "    confidence_level: float = 0.95,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    \"\"\"Compute bootstrap confidence intervals for all standard metrics.\"\"\"\n",
        "    # Wrapper functions that accept weights\n",
        "    def wrmse_fn(yt, yp, w):\n",
        "        return w_rmse(yt, yp, w)\n",
        "\n",
        "    def wmae_fn(yt, yp, w):\n",
        "        return w_mae(yt, yp, w)\n",
        "\n",
        "    def diracc_fn(yt, yp, _w):\n",
        "        return dir_acc(yt, yp)  # DirAcc doesn't use weights\n",
        "\n",
        "    metrics = {\n",
        "        \"wrmse\": wrmse_fn,\n",
        "        \"wmae\": wmae_fn,\n",
        "        \"diracc\": diracc_fn,\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, fn in metrics.items():\n",
        "        results[name] = bootstrap_metric(\n",
        "            y_true, y_pred, weights, fn,\n",
        "            n_bootstrap=n_bootstrap,\n",
        "            confidence_level=confidence_level,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def format_ci(ci_result, decimals: int = 6) -> str:\n",
        "    \"\"\"Format confidence interval as string: point [lower, upper].\"\"\"\n",
        "    return (\n",
        "        f\"{ci_result['point_estimate']:.{decimals}f} \"\n",
        "        f\"[{ci_result['ci_lower']:.{decimals}f}, {ci_result['ci_upper']:.{decimals}f}]\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Run Bootstrap Analysis ---\n",
        "\n",
        "print(\"[INFO] Running Bootstrap Confidence Interval Analysis...\")\n",
        "print(f\"[INFO] Models to analyze: {len(best_per_model)} (from best_per_model)\")\n",
        "\n",
        "N_BOOTSTRAP = 1000\n",
        "CONFIDENCE_LEVEL = 0.95\n",
        "\n",
        "bootstrap_results = []\n",
        "\n",
        "# Get the best run for each model from best_per_model\n",
        "for _, row in best_per_model.iterrows():\n",
        "    model_name = row[\"model\"]\n",
        "    run_id = row[\"run_id\"]\n",
        "    feature_set = row.get(\"feature_set\", \"unknown\")\n",
        "\n",
        "    print(f\"\\n[INFO] Analyzing: {model_name} (run: {run_id})\")\n",
        "\n",
        "    # Find predictions file for this model/run\n",
        "    run_dir = RUNS_DIR_DRIVE / run_id\n",
        "    models_dir = run_dir / \"models\"\n",
        "    pred_dir = run_dir / \"predictions\"\n",
        "\n",
        "    y_true = None\n",
        "    y_pred = None\n",
        "    weights = None\n",
        "\n",
        "    # Try to load predictions based on model type\n",
        "    # All predictions are in predictions/{model}/ or predictions/{model}_{feature_set}/\n",
        "    try:\n",
        "        pred_path = None\n",
        "\n",
        "        if model_name == \"XGBoost\":\n",
        "            pred_path = pred_dir / \"xgb\" / \"predictions_test.csv\"\n",
        "\n",
        "        elif model_name == \"LightGBM\":\n",
        "            pred_path = pred_dir / \"lgb\" / \"predictions_test.csv\"\n",
        "\n",
        "        elif model_name in [\"LSTM\", \"GRU\"]:\n",
        "            model_lower = model_name.lower()\n",
        "            # Try different feature_set paths\n",
        "            for fs in [\"neural_40\", \"xgb_selected\"]:\n",
        "                p = pred_dir / f\"{model_lower}_{fs}\" / \"predictions_test.csv\"\n",
        "                if p.exists():\n",
        "                    pred_path = p\n",
        "                    break\n",
        "\n",
        "        elif model_name in [\"Hybrid-Seq\", \"Hybrid-Par\"]:\n",
        "            model_key = \"hybrid_seq\" if model_name == \"Hybrid-Seq\" else \"hybrid_par\"\n",
        "            for fs in [\"neural_40\", \"xgb_selected\"]:\n",
        "                p = pred_dir / f\"{model_key}_{fs}\" / \"predictions_test.csv\"\n",
        "                if p.exists():\n",
        "                    pred_path = p\n",
        "                    break\n",
        "\n",
        "        elif \"Ensemble\" in model_name:\n",
        "            pred_path = run_dir / \"outputs\" / \"ensemble_predictions_test.csv\"\n",
        "\n",
        "        elif model_name in [\"BASELINE_ZERO\", \"BASELINE_NAIVE\"]:\n",
        "            # Use XGBoost predictions for y_true\n",
        "            pred_path = pred_dir / \"xgb\" / \"predictions_test.csv\"\n",
        "\n",
        "        # Load and parse predictions\n",
        "        if pred_path and pred_path.exists():\n",
        "            df = pd.read_csv(pred_path)\n",
        "\n",
        "            # Get y_true (different column names)\n",
        "            if \"y_true\" in df.columns:\n",
        "                y_true = df[\"y_true\"].values\n",
        "            elif \"actual\" in df.columns:\n",
        "                y_true = df[\"actual\"].values\n",
        "            else:\n",
        "                raise ValueError(f\"No y_true/actual column in {pred_path}\")\n",
        "\n",
        "            # Get predictions\n",
        "            if model_name == \"BASELINE_ZERO\":\n",
        "                y_pred = np.zeros(len(y_true))\n",
        "            elif model_name == \"BASELINE_NAIVE\":\n",
        "                y_pred = np.roll(y_true, 1)\n",
        "                y_pred[0] = 0\n",
        "            elif \"y_pred_model\" in df.columns:\n",
        "                y_pred = df[\"y_pred_model\"].values\n",
        "            elif \"predicted\" in df.columns:\n",
        "                y_pred = df[\"predicted\"].values\n",
        "            else:\n",
        "                raise ValueError(f\"No prediction column in {pred_path}\")\n",
        "\n",
        "            # Get weights\n",
        "            if \"sample_weight\" in df.columns:\n",
        "                weights = df[\"sample_weight\"].values\n",
        "            else:\n",
        "                weights = np.ones(len(y_true))\n",
        "\n",
        "        if y_true is not None and y_pred is not None:\n",
        "            # Run bootstrap\n",
        "            ci_results = bootstrap_all_metrics(\n",
        "                y_true, y_pred, weights,\n",
        "                n_bootstrap=N_BOOTSTRAP,\n",
        "                confidence_level=CONFIDENCE_LEVEL,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            bootstrap_results.append({\n",
        "                \"model\": model_name,\n",
        "                \"run_id\": run_id,\n",
        "                \"feature_set\": feature_set,\n",
        "                \"n_samples\": len(y_true),\n",
        "                \"n_bootstrap\": N_BOOTSTRAP,\n",
        "                \"confidence_level\": CONFIDENCE_LEVEL,\n",
        "                # wRMSE\n",
        "                \"wrmse\": ci_results[\"wrmse\"][\"point_estimate\"],\n",
        "                \"wrmse_ci_lower\": ci_results[\"wrmse\"][\"ci_lower\"],\n",
        "                \"wrmse_ci_upper\": ci_results[\"wrmse\"][\"ci_upper\"],\n",
        "                \"wrmse_std\": ci_results[\"wrmse\"][\"std\"],\n",
        "                # wMAE\n",
        "                \"wmae\": ci_results[\"wmae\"][\"point_estimate\"],\n",
        "                \"wmae_ci_lower\": ci_results[\"wmae\"][\"ci_lower\"],\n",
        "                \"wmae_ci_upper\": ci_results[\"wmae\"][\"ci_upper\"],\n",
        "                \"wmae_std\": ci_results[\"wmae\"][\"std\"],\n",
        "                # DirAcc\n",
        "                \"diracc\": ci_results[\"diracc\"][\"point_estimate\"],\n",
        "                \"diracc_ci_lower\": ci_results[\"diracc\"][\"ci_lower\"],\n",
        "                \"diracc_ci_upper\": ci_results[\"diracc\"][\"ci_upper\"],\n",
        "                \"diracc_std\": ci_results[\"diracc\"][\"std\"],\n",
        "            })\n",
        "\n",
        "            print(f\"  wRMSE: {format_ci(ci_results['wrmse'])}\")\n",
        "            print(f\"  DirAcc: {format_ci(ci_results['diracc'], decimals=4)}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] Could not load predictions for {model_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "if len(bootstrap_results) > 0:\n",
        "    bootstrap_df = pd.DataFrame(bootstrap_results)\n",
        "    bootstrap_df = bootstrap_df.sort_values(\"wrmse\").reset_index(drop=True)\n",
        "    bootstrap_df.insert(0, \"rank\", range(1, len(bootstrap_df) + 1))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"BOOTSTRAP CONFIDENCE INTERVALS ({CONFIDENCE_LEVEL*100:.0f}% CI, n={N_BOOTSTRAP})\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Display summary\n",
        "    display_cols = [\"rank\", \"model\", \"wrmse\", \"wrmse_ci_lower\", \"wrmse_ci_upper\",\n",
        "                    \"diracc\", \"diracc_ci_lower\", \"diracc_ci_upper\"]\n",
        "    display(bootstrap_df[display_cols])\n",
        "\n",
        "    # Save to results_summary\n",
        "    bootstrap_df.to_csv(RESULTS_LOCAL / \"bootstrap_ci.csv\", index=False)\n",
        "    bootstrap_df.to_csv(RESULTS_DRIVE / \"bootstrap_ci.csv\", index=False)\n",
        "    print(f\"\\n[OK] Saved: bootstrap_ci.csv\")\n",
        "\n",
        "    # Also save as JSON for detailed results\n",
        "    save_json(bootstrap_results, RESULTS_LOCAL / \"bootstrap_ci.json\")\n",
        "    save_json(bootstrap_results, RESULTS_DRIVE / \"bootstrap_ci.json\")\n",
        "    print(\"[OK] Saved: bootstrap_ci.json\")\n",
        "else:\n",
        "    print(\"[WARN] No bootstrap results computed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}