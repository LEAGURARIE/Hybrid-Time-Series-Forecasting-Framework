{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcc8 Google Stock ML Prediction Project\n",
    "\n",
    "End-to-end machine learning pipeline for predicting Google (GOOGL) stock next-day returns.\n",
    "\n",
    "## Models\n",
    "- **XGBoost** \u2014 Gradient boosting with 3-stage HPO\n",
    "- **LSTM & GRU** \u2014 Recurrent neural networks\n",
    "- **Hybrid** \u2014 Sequential & Parallel architectures\n",
    "\n",
    "## Sections\n",
    "1. Configuration & Helpers\n",
    "2. Data Ingestion & Preprocessing\n",
    "3. Exploratory Data Analysis\n",
    "4. Train/Valid/Test Split & NN Features\n",
    "5. Feature Selection\n",
    "6. XGBoost HPO\n",
    "7. XGBoost Final Model\n",
    "8. LSTM & GRU\n",
    "9. Hybrid Neural Networks\n",
    "10. Final Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 1: Configuration & Helpers\n",
    "\n",
    "**Setup, imports, paths, and helper functions**\n",
    "\n",
    "**Blocks:** 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50747bf",
   "metadata": {},
   "source": [
    "## BOOT + BLOCK 0 \u2014 CONFIG + HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51108ec",
   "metadata": {},
   "outputs": [],
   "source": "\nimport os\nimport sys\nimport json\nimport time\nimport pickle\nimport shutil\nimport subprocess\nimport warnings\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\nfrom glob import glob\n\nwarnings.filterwarnings(\"ignore\")\n\n# --- Colab Detection & Drive Mount (once) ---\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    from google.colab import drive\n    if not Path(\"/content/drive/MyDrive\").exists():\n        drive.mount(\"/content/drive\", force_remount=False)\n\n# --- Project Paths ---\nDRIVE_PROJECT_ROOT = Path(os.environ.get(\n    \"DRIVE_PROJECT_ROOT\", \"/content/drive/MyDrive/my_project\"\n)).expanduser()\nLOCAL_PROJECT_ROOT = Path(os.environ.get(\n    \"LOCAL_PROJECT_ROOT\", \"/content/my_project\"\n)).expanduser()\n\nDRIVE_PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\nLOCAL_PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n\n# Sync Drive -> Local if local is empty but Drive has content\nlocal_has_files = any(LOCAL_PROJECT_ROOT.rglob(\"*\"))\ndrive_has_files = any(DRIVE_PROJECT_ROOT.rglob(\"*\"))\n\nif (not local_has_files) and drive_has_files:\n    for item in LOCAL_PROJECT_ROOT.iterdir():\n        if item.is_dir():\n            shutil.rmtree(item)\n        else:\n            item.unlink()\n    shutil.copytree(DRIVE_PROJECT_ROOT, LOCAL_PROJECT_ROOT, dirs_exist_ok=True)\n\n# Active project root (local for fast I/O)\nPROJECT_ROOT = LOCAL_PROJECT_ROOT\n\nprint(\"[BOOT] DRIVE_PROJECT_ROOT:\", DRIVE_PROJECT_ROOT)\nprint(\"[BOOT] LOCAL_PROJECT_ROOT:\", LOCAL_PROJECT_ROOT)\nprint(\"[BOOT] PROJECT_ROOT (active):\", PROJECT_ROOT)\n\n# --- Run ID & Directories ---\nRUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\nLOCAL_RUNS_ROOT = Path(os.environ.get(\n    \"LOCAL_RUNS_ROOT\", str(PROJECT_ROOT / \"runs\")\n)).expanduser()\nDRIVE_RUNS_ROOT = Path(os.environ.get(\n    \"DRIVE_RUNS_ROOT\", str(DRIVE_PROJECT_ROOT / \"runs\")\n)).expanduser()\n\nLOCAL_RUN_DIR = LOCAL_RUNS_ROOT / RUN_ID\nDRIVE_RUN_DIR = DRIVE_RUNS_ROOT / RUN_ID\n\n\ndef _mk_run_dirs(run_dir: Path) -> Dict[str, Path]:\n    \"\"\"Create standard run directory structure.\"\"\"\n    paths = {\n        \"run_dir\": run_dir,\n        \"outputs_dir\": run_dir / \"outputs\",\n        \"models_dir\": run_dir / \"models\",\n        \"reports_dir\": run_dir / \"reports\",\n        \"plots_dir\": run_dir / \"plots\",\n        \"config_dir\": run_dir / \"config\",\n        \"logs_dir\": run_dir / \"logs\",\n        \"proc_dir\": run_dir / \"processed\",\n        \"fs_dir\": run_dir / \"feature_selection\",\n        \"ms_dir\": run_dir / \"model_selection\",\n    }\n    for p in paths.values():\n        p.mkdir(parents=True, exist_ok=True)\n    return paths\n\n\nLOCAL_PATHS = _mk_run_dirs(LOCAL_RUN_DIR)\nDRIVE_PATHS = _mk_run_dirs(DRIVE_RUN_DIR)\n\n# Active runtime uses LOCAL paths (fast I/O)\nRUN_DIR = LOCAL_PATHS[\"run_dir\"]\nOUTPUTS_DIR = LOCAL_PATHS[\"outputs_dir\"]\nMODELS_DIR = LOCAL_PATHS[\"models_dir\"]\nREPORTS_DIR = LOCAL_PATHS[\"reports_dir\"]\nPLOTS_DIR = LOCAL_PATHS[\"plots_dir\"]\nCONFIG_DIR = LOCAL_PATHS[\"config_dir\"]\nLOGS_DIR = LOCAL_PATHS[\"logs_dir\"]\nPROC_DIR = LOCAL_PATHS[\"proc_dir\"]\nFS_DIR = LOCAL_PATHS[\"fs_dir\"]\nMS_DIR = LOCAL_PATHS[\"ms_dir\"]\n\n# Project-level data directories (not run-specific)\nDATA_DIRS_LOCAL = {\n    \"raw\": PROJECT_ROOT / \"data\" / \"raw\",\n    \"interim\": PROJECT_ROOT / \"data\" / \"interim\",\n    \"processed\": PROJECT_ROOT / \"data\" / \"processed\",\n}\nDATA_DIRS_DRIVE = {\n    \"raw\": DRIVE_PROJECT_ROOT / \"data\" / \"raw\",\n    \"interim\": DRIVE_PROJECT_ROOT / \"data\" / \"interim\",\n    \"processed\": DRIVE_PROJECT_ROOT / \"data\" / \"processed\",\n}\nfor _d in list(DATA_DIRS_LOCAL.values()) + list(DATA_DIRS_DRIVE.values()):\n    _d.mkdir(parents=True, exist_ok=True)\n\nprint(\"[CONFIG] RUN_ID:\", RUN_ID)\nprint(\"[CONFIG] LOCAL_RUN_DIR:\", LOCAL_RUN_DIR)\nprint(\"[CONFIG] DRIVE_RUN_DIR:\", DRIVE_RUN_DIR)\n\n\n# --- Helper Functions ---\ndef ensure_dir(p: Path) -> Path:\n    \"\"\"Create directory if not exists, return path.\"\"\"\n    p = Path(p)\n    p.mkdir(parents=True, exist_ok=True)\n    return p\n\n\ndef save_text(text: str, path: Path) -> Path:\n    \"\"\"Save text to file.\"\"\"\n    path = Path(path)\n    ensure_dir(path.parent)\n    path.write_text(text, encoding=\"utf-8\")\n    return path\n\n\ndef save_json(obj: Any, path: Path, indent: int = 2) -> Path:\n    \"\"\"Save object as JSON.\"\"\"\n    path = Path(path)\n    ensure_dir(path.parent)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(obj, f, ensure_ascii=False, indent=indent)\n    return path\n\n\ndef save_pickle(obj: Any, path: Path) -> Path:\n    \"\"\"Save object as pickle.\"\"\"\n    path = Path(path)\n    ensure_dir(path.parent)\n    with open(path, \"wb\") as f:\n        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return path\n\n\ndef load_pickle(path: Path) -> Any:\n    \"\"\"Load object from pickle.\"\"\"\n    path = Path(path)\n    with open(path, \"rb\") as f:\n        return pickle.load(f)\n\n\n\n\ndef load_with_fallback(filename: str, run_dir_local: Path, fallback_dir_local: Path, \n                        run_dir_drive: Path = None, fallback_dir_drive: Path = None,\n                        use_pandas: bool = False) -> Any:\n    \"\"\"Load file with 4-level fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE.\n    \n    Args:\n        filename: Name of the file to load\n        run_dir_local: Local run directory (runs/RUN_ID/...)\n        fallback_dir_local: Local persistent directory (data/processed/)\n        run_dir_drive: Drive run directory (optional)\n        fallback_dir_drive: Drive persistent directory (optional)\n        use_pandas: If True, use pd.read_pickle; otherwise use load_pickle\n    \n    Returns:\n        Loaded object\n    \"\"\"\n    # Build list of paths to try\n    paths_to_try = [\n        (Path(run_dir_local) / filename, \"RUN_ID (LOCAL)\"),\n        (Path(fallback_dir_local) / filename, \"data/processed (LOCAL)\"),\n    ]\n    \n    if run_dir_drive:\n        paths_to_try.append((Path(run_dir_drive) / filename, \"RUN_ID (DRIVE)\"))\n    if fallback_dir_drive:\n        paths_to_try.append((Path(fallback_dir_drive) / filename, \"data/processed (DRIVE)\"))\n    \n    # Try each path\n    for path, source_name in paths_to_try:\n        if path.exists():\n            print(f\"  [LOAD] {filename} <- {source_name}\")\n            if use_pandas:\n                return pd.read_pickle(path)\n            return load_pickle(path)\n    \n    # None found\n    tried = \"\\n  \".join([f\"{src}: {p}\" for p, src in paths_to_try])\n    raise FileNotFoundError(f\"File not found: {filename}\\nTried:\\n  {tried}\")\n\ndef copy_file(src: Path, dst: Path) -> Path:\n    \"\"\"Copy file with metadata.\"\"\"\n    src, dst = Path(src), Path(dst)\n    ensure_dir(dst.parent)\n    shutil.copy2(src, dst)\n    return dst\n\n\ndef copy_tree(src_dir: Path, dst_dir: Path, ignore: Optional[Any] = None) -> Path:\n    \"\"\"Copy directory tree.\"\"\"\n    src_dir, dst_dir = Path(src_dir), Path(dst_dir)\n    ensure_dir(dst_dir.parent)\n    if dst_dir.exists():\n        shutil.rmtree(dst_dir)\n    shutil.copytree(src_dir, dst_dir, ignore=ignore)\n    return dst_dir\n\n\ndef run_dirs() -> Tuple[Path, Path]:\n    \"\"\"Return (plots_dir, reports_dir) for current run.\"\"\"\n    return Path(PLOTS_DIR), Path(REPORTS_DIR)\n\n\n# --- RUN PARAMETERS ---\nRUN_PARAMS: Dict[str, Any] = {\n    \"run_id\": RUN_ID,\n    \"random_state\": 42,  # Global random state for reproducibility\n    # Used by: Cell 5 (paths setup)\n    \"paths\": {\n        \"project_root_local\": str(PROJECT_ROOT),\n        \"project_root_drive\": str(DRIVE_PROJECT_ROOT),\n        \"run_dir_local\": str(LOCAL_RUN_DIR),\n        \"outputs_dir_local\": str(LOCAL_PATHS[\"outputs_dir\"]),\n        \"models_dir_local\": str(LOCAL_PATHS[\"models_dir\"]),\n        \"reports_dir_local\": str(LOCAL_PATHS[\"reports_dir\"]),\n        \"plots_dir_local\": str(LOCAL_PATHS[\"plots_dir\"]),\n        \"config_dir_local\": str(LOCAL_PATHS[\"config_dir\"]),\n        \"logs_dir_local\": str(LOCAL_PATHS[\"logs_dir\"]),\n        \"proc_dir_local\": str(LOCAL_PATHS[\"proc_dir\"]),\n        \"fs_dir_local\": str(LOCAL_PATHS[\"fs_dir\"]),\n        \"ms_dir_local\": str(LOCAL_PATHS[\"ms_dir\"]),\n        \"run_dir_drive\": str(DRIVE_RUN_DIR),\n        \"outputs_dir_drive\": str(DRIVE_PATHS[\"outputs_dir\"]),\n        \"models_dir_drive\": str(DRIVE_PATHS[\"models_dir\"]),\n        \"reports_dir_drive\": str(DRIVE_PATHS[\"reports_dir\"]),\n        \"plots_dir_drive\": str(DRIVE_PATHS[\"plots_dir\"]),\n        \"config_dir_drive\": str(DRIVE_PATHS[\"config_dir\"]),\n        \"logs_dir_drive\": str(DRIVE_PATHS[\"logs_dir\"]),\n        \"proc_dir_drive\": str(DRIVE_PATHS[\"proc_dir\"]),\n        \"fs_dir_drive\": str(DRIVE_PATHS[\"fs_dir\"]),\n        \"ms_dir_drive\": str(DRIVE_PATHS[\"ms_dir\"]),\n        # Project-level data directories (not run-specific)\n        \"data_raw_local\": str(DATA_DIRS_LOCAL[\"raw\"]),\n        \"data_interim_local\": str(DATA_DIRS_LOCAL[\"interim\"]),\n        \"data_processed_local\": str(DATA_DIRS_LOCAL[\"processed\"]),\n        \"data_raw_drive\": str(DATA_DIRS_DRIVE[\"raw\"]),\n        \"data_interim_drive\": str(DATA_DIRS_DRIVE[\"interim\"]),\n        \"data_processed_drive\": str(DATA_DIRS_DRIVE[\"processed\"]),\n    },\n    # Used by: Blocks 3,4,14,15,19,20 (data loading & split)\n    \"data\": {\n        \"target\": \"GOOGL_logret_t1\",\n        \"target_src_col\": \"GOOGL_logret_cc\",\n        \"target_col\": \"GOOGL_logret_t1\",\n        \"start_date\": \"2004-09-01\",\n        \"end_date\": \"2025-01-15\",  # Set fixed date for reproducibility; use None for today's date\n        \"limit_start_date\": \"2015-12-31\",\n        #  Exact dates (takes priority if provided)\n        \"train_end\": \"2020-12-31\",      # Last date for training\n        \"valid_start\": \"2021-01-01\",    # First date for validation\n        \"valid_end\": \"2022-12-31\",      # Last date for validation\n        \"test_start\": \"2023-01-01\",     # First date for test\n        \"test_end\": None,               # None = use all remaining data\n    },\n    # Used by: Blocks 7-13,18,19,20 (feature engineering)\n    \"features\": {\n        \"feature_set_name\": \"XGB-30\",\n        \"feature_selection_artifact\": \"selected_features_xgb.pkl\",\n        # Rolling windows\n        \"rolling_w_short\": 5,\n        \"rolling_w_long\": 21,\n        \"do_volume_rolling\": True,\n        # Cross-asset\n        \"cross_asset_base\": \"GOOGL\",\n        \"cross_asset_peers\": [\"SPY\", \"QQQ\", \"^IXIC\", \"XLK\"],\n        \"cross_asset_windows\": [5, 21],\n        # Regime\n        \"regime_base\": \"GOOGL\",\n        \"market_vol_ticker\": \"SPY\",\n        # Exclusions\n        \"exclude_raw_ohlc\": [\"^VIX\", \"^TNX\"],\n        # Crisis periods\n        \"covid_start\": \"2020-02-01\",\n        \"covid_end\": \"2023-05-05\",\n        \"crisis_2008_start\": \"2007-07-01\",\n        \"crisis_2008_end\": \"2009-09-01\",\n        # Numeric safety\n        \"eps\": 1e-12,\n    },\n    # XGBoost\n\n\n    # EU Break Close Flags\n    # Used by: Block 2B (EU break close flags)\n    \"eu_break_close\": {\n        \"enabled\": True,\n        \"eu_ticker\": \"^GDAXI\",\n        \"gap_days_threshold\": 2,\n        \"apply_to\": \"next_us_trading_day\",  # \"same_calendar_date\" or \"next_us_trading_day\"\n    },\n    # EDA\n    # Used by: Blocks 17,18,19 (EDA)\n    \"eda\": {\n        \"enabled\": True,\n        \"returns_bins\": 50,\n    },\n    # Sample weights\n    # Used by: Block 20 (sample weights)\n    \"weights\": {\n        \"c\": 1.0,\n        \"max_w\": 4.0,\n    },\n    # NN Feature Selection\n    # Used by: Block 21 (NN feature groups)\n    \"nn_feature_select\": {\n        \"n40\": 40,\n        \"n80\": 80,\n        \"per_group_40\": 4,\n        \"per_group_80\": 8,\n        \"corr_thr\": 0.95,\n        \"mi_n_neighbors\": 5,\n        \"mi_random_state\": 42,\n    },\n    # Used by: Block 23 (XGB feature selection)\n    \"xgb_fs\": {\n        \"spearman_thresh\": 0.90,\n        \"gain_cum_thresh\": 0.90,\n        \"min_features\": 15,\n        \"neg_sigma\": 1.0,\n        \"pos_sigma\": 0.5,\n        \"min_gain\": 0.0,\n        \"perm_repeats\": 20,\n        \"n_estimators\": 4000,\n        \"learning_rate\": 0.05,\n        \"max_depth\": 3,\n        \"min_child_weight\": 10,\n        \"gamma\": 0.5,\n        \"subsample\": 0.70,\n        \"colsample_bytree\": 0.70,\n        \"reg_alpha\": 1e-4,\n        \"reg_lambda\": 5.0,\n        \"max_delta_step\": 1,\n        \"early_stopping_rounds\": 80,\n        \"random_state\": 42,\n    },\n    # Used by: Blocks 24,25,26,28 (HPO & model training)\n    \"hpo\": {\n        \"n_estimators\": 4000,\n        \"early_stopping_rounds\": 80,\n        \"n_trials_stage1\": 160,\n        \"n_trials_stage2\": 80,\n        \"n_trials_stage2_lowlr\": 40,\n        \"print_every_stage1\": 20,\n        \"print_every_stage2\": 20,\n        \"tie_tol\": 1e-5,\n        \"random_state\": 42,\n        # Shared lookback for alignment with Neural Networks\n        \"lookback\": 15,\n        # XGBoost model settings\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"tree_method\": \"hist\",\n        # Date-based\n        \"valid_es_start\": \"2021-01-01\",\n        \"valid_es_end\": \"2021-12-31\",\n        \"valid_score_start\": \"2022-01-01\",\n        \"valid_score_end\": \"2022-12-31\",\n        # Sampling parameters for HPO\n        # Used by: Block 24 (HPO sampling)\n    \"sampling\": {\n            # Used by: Block 24 (HPO Stage 1)\n    \"broad\": {\n                \"max_depth\": [2, 7],\n                \"lr_low\": [0.003, 0.06],\n                \"lr_high\": [0.06, 0.12],\n                \"lr_high_prob\": 0.15,\n                \"min_child_weight_log\": [0.5, 20.0],\n                \"subsample\": [0.6, 1.0],\n                \"colsample_bytree\": [0.55, 1.0],\n                \"gamma\": [0.0, 3.0],\n                \"reg_alpha_exp\": [-9, -2],\n                \"reg_lambda_exp\": [-2, 1.3],\n                \"max_delta_step\": [0.0, 2.0],\n            },\n            # Used by: Block 24 (HPO Stage 2)\n    \"refine\": {\n                \"max_depth_delta\": [-1, 2],\n                \"max_depth_clip\": [2, 8],\n                \"lr_sigma\": 0.25,\n                \"lr_clip\": [0.002, 0.15],\n                \"min_child_weight_sigma\": 0.40,\n                \"min_child_weight_clip\": [0.3, 30.0],\n                \"subsample_sigma\": 0.06,\n                \"subsample_clip\": [0.5, 1.0],\n                \"colsample_sigma\": 0.06,\n                \"colsample_clip\": [0.5, 1.0],\n                \"gamma_sigma\": 0.30,\n                \"gamma_clip\": [0.0, 5.0],\n                \"reg_alpha_sigma\": 0.7,\n                \"reg_alpha_exp_clip\": [-10, 0],\n                \"reg_lambda_sigma\": 0.5,\n                \"reg_lambda_exp_clip\": [-3, 2],\n                \"max_delta_step_sigma\": 0.20,\n                \"max_delta_step_clip\": [0.0, 4.0],\n            },\n            # Used by: Block 24 (HPO Stage 2 Low-LR)\n    \"refine_low_lr\": {\n                \"lr_shift\": -0.8,\n                \"lr_clip\": [0.0015, 0.06],\n            },\n        },\n    },\n    # Used by: Blocks 25,27,29 (plotting)\n    \"plot\": {\n        \"n_plot\": 200,\n        \"figsize\": [13, 5],\n        \"dpi\": 150,\n    },\n    # Used by: Block 25 (SHAP analysis)\n    \"shap\": {\n        \"enabled\": True,\n        \"max_display\": 20,\n        \"figsize\": [10, 8],\n        \"plot_type_bar\": True,\n        \"plot_type_beeswarm\": True,\n        \"save_values\": True,\n    },\n    # Used by: Blocks 26,27 (LSTM model)\n        # --- LightGBM Feature Selection ---\n    \"lgb_fs\": {\n        \"spearman_thresh\": 0.90,\n        \"gain_cum_thresh\": 0.90,\n        \"min_features\": 15,\n        \"neg_sigma\": 1.0,\n        \"pos_sigma\": 0.5,\n        \"min_gain\": 0.0,\n        \"perm_repeats\": 20,\n        \"n_estimators\": 4000,\n        \"early_stopping_rounds\": 80,\n        \"learning_rate\": 0.05,\n        \"max_depth\": 3,\n        \"num_leaves\": 31,\n        \"min_child_samples\": 20,\n        \"subsample\": 0.70,\n        \"colsample_bytree\": 0.70,\n        \"reg_alpha\": 1e-4,\n        \"reg_lambda\": 5.0,\n        \"random_state\": 42,\n    },\n    # --- LightGBM HPO ---\n    \"lgb_hpo\": {\n        \"n_estimators\": 4000,\n        \"early_stopping_rounds\": 80,\n        \"n_trials_stage1\": 160,\n        \"n_trials_stage2\": 80,\n        \"n_trials_stage2_lowlr\": 40,\n        \"valid_es_start\": \"2021-01-01\",\n        \"valid_es_end\": \"2021-12-31\",\n        \"valid_score_start\": \"2022-01-01\",\n        \"valid_score_end\": \"2022-12-31\",\n        \"lookback\": 15,\n        \"random_state\": 42,\n    },\n    \"lstm\": {\n        \"lookback\": 15,\n        \"stride\": 1,\n        \"units_1\": 32,\n        \"units_2\": 16,\n        \"dense_units\": 16,\n        \"dropout\": 0.20,\n        \"learning_rate\": 5e-4,\n        \"clipnorm\": 1.0,\n        \"loss\": \"mse\",\n        \"dense_activation\": \"relu\",\n        \"output_activation\": \"linear\",\n        \"epochs\": 80,\n        \"batch_size\": 16,\n        \"patience\": 10,\n        \"random_state\": 42,\n        \"feature_sets\": [\"neural_40\", \"neural_80\", \"xgb_selected\"],\n    },\n    # Used by: Blocks 26,27 (GRU model)\n    \"gru\": {\n        \"lookback\": 15,\n        \"stride\": 1,\n        \"units_1\": 32,\n        \"units_2\": 16,\n        \"dense_units\": 16,\n        \"dropout\": 0.20,\n        \"learning_rate\": 5e-4,\n        \"clipnorm\": 1.0,\n        \"loss\": \"mse\",\n        \"dense_activation\": \"relu\",\n        \"output_activation\": \"linear\",\n        \"epochs\": 80,\n        \"batch_size\": 16,\n        \"patience\": 10,\n        \"random_state\": 42,\n        \"feature_sets\": [\"neural_40\", \"neural_80\", \"xgb_selected\"],\n    },\n    # Used by: Blocks 28,29 (Hybrid Sequential)\n    \"hybrid_seq\": {\n        \"lookback\": 15,\n        \"stride\": 1,\n        \"lstm_units\": 32,\n        \"gru_units\": 16,\n        \"dense_units\": 16,\n        \"dropout\": 0.20,\n        \"learning_rate\": 4e-4,\n        \"clipnorm\": 1.0,\n        \"loss\": \"mse\",\n        \"dense_activation\": \"relu\",\n        \"output_activation\": \"linear\",\n        \"epochs\": 90,\n        \"batch_size\": 16,\n        \"patience\": 12,\n        \"random_state\": 42,\n        \"feature_sets\": [\"neural_40\", \"neural_80\", \"xgb_selected\"],\n    },\n    # Used by: Blocks 28,29 (Hybrid Parallel)\n    \"hybrid_par\": {\n        \"lookback\": 15,\n        \"stride\": 1,\n        \"lstm_units\": 24,\n        \"gru_units\": 24,\n        \"dense_units\": 16,\n        \"dropout\": 0.20,\n        \"learning_rate\": 4e-4,\n        \"clipnorm\": 1.0,\n        \"loss\": \"mse\",\n        \"dense_activation\": \"relu\",\n        \"output_activation\": \"linear\",\n        \"epochs\": 90,\n        \"batch_size\": 16,\n        \"patience\": 12,\n        \"random_state\": 42,\n        \"feature_sets\": [\"neural_40\", \"neural_80\", \"xgb_selected\"],\n    },\n    # --- Ensemble ---\n    \"ensemble\": {\n        \"method\": \"weighted_average\",  # simple_average, weighted_average, stacking, rank_average\n        \"models\": [\"xgb\", \"lgb\", \"lstm\", \"gru\", \"hybrid_seq\", \"hybrid_par\"],\n        \"weights\": \"auto\",  # \"auto\" for inverse_wrmse, or dict\n        \"weight_method\": \"inverse_wrmse\",\n        \"meta_model\": \"ridge\",\n        \"meta_params\": {\n            \"alpha\": 1.0,\n            \"random_state\": 42,\n        },\n    },\n}\n\n# Save run params to BOTH local and drive\nsave_json(RUN_PARAMS, LOCAL_PATHS[\"config_dir\"] / \"run_params.json\")\nsave_text(\n    json.dumps(RUN_PARAMS, indent=2, ensure_ascii=False),\n    LOCAL_PATHS[\"config_dir\"] / \"run_params.txt\",\n)\nsave_json(RUN_PARAMS, DRIVE_PATHS[\"config_dir\"] / \"run_params.json\")\nsave_text(\n    json.dumps(RUN_PARAMS, indent=2, ensure_ascii=False),\n    DRIVE_PATHS[\"config_dir\"] / \"run_params.txt\",\n)\n\n\ndef save_run_outputs(\n    metrics: Dict[str, Any],\n    predictions_valid: Optional[\"pd.DataFrame\"] = None,\n    predictions_test: Optional[\"pd.DataFrame\"] = None,\n    extra_artifacts: Optional[Dict[str, Any]] = None,\n    model: Optional[Any] = None,\n    model_filename: str = \"model.json\",\n) -> None:\n    \"\"\"Save standard run outputs (metrics, predictions, model).\"\"\"\n    save_json(metrics, OUTPUTS_DIR / \"metrics.json\")\n    save_text(\n        \"\\n\".join([f\"{k}: {v}\" for k, v in metrics.items()]),\n        OUTPUTS_DIR / \"metrics.txt\",\n    )\n\n    if predictions_valid is not None:\n        predictions_valid.to_csv(OUTPUTS_DIR / \"predictions_valid.csv\", index=True)\n    if predictions_test is not None:\n        predictions_test.to_csv(OUTPUTS_DIR / \"predictions_test.csv\", index=True)\n\n    if extra_artifacts:\n        for name, obj in extra_artifacts.items():\n            save_pickle(obj, OUTPUTS_DIR / f\"{name}.pkl\")\n\n    if model is not None:\n        path = MODELS_DIR / model_filename\n        if hasattr(model, \"save_model\"):\n            model.save_model(str(path))\n        else:\n            save_pickle(model, MODELS_DIR / (Path(model_filename).stem + \".pkl\"))\n\n\n# --- Code Snapshot ---\nEXPORT_CODE = True\n\n\ndef snapshot_code(project_root: Path, local_run_dir: Path, drive_run_dir: Path) -> None:\n    \"\"\"Save current notebook to run directories for reproducibility.\"\"\"\n    \n    # Step 1: Save the notebook first (Colab-specific)\n    try:\n        from google.colab import _message\n        _message.blocking_request('save_notebook', {'save': True})\n        print(\"[SNAPSHOT] Notebook saved via Colab API\")\n    except Exception:\n        print(\"[SNAPSHOT] Could not auto-save notebook (not in Colab or already saved)\")\n    \n    # Step 2: Find the notebook file\n    notebook_path = None\n    search_paths = [\n        project_root / \"google_stock_ml_unified.ipynb\",\n        Path(\"/content/google_stock_ml_unified.ipynb\"),\n        Path(\"/content/my_project/google_stock_ml_unified.ipynb\"),\n    ]\n    \n    # Also search for any .ipynb files\n    for pattern in [\"/content/*.ipynb\", \"/content/my_project/*.ipynb\"]:\n        for p in glob(pattern):\n            if \"checkpoint\" not in p.lower():\n                search_paths.append(Path(p))\n    \n    for p in search_paths:\n        if p.exists():\n            notebook_path = p\n            break\n    \n    # Step 3: Copy to both locations\n    def _save_snapshot(dst_run_dir: Path, location: str):\n        export_dir = dst_run_dir / \"code_snapshot\"\n        export_dir.mkdir(parents=True, exist_ok=True)\n        \n        if notebook_path and notebook_path.exists():\n            dst = export_dir / notebook_path.name\n            copy_file(notebook_path, dst)\n            \n            # Save metadata\n            meta = {\n                \"run_id\": RUN_ID,\n                \"timestamp\": datetime.now().isoformat(),\n                \"source_path\": str(notebook_path),\n            }\n            save_json(meta, export_dir / \"snapshot_meta.json\")\n            print(f\"[SNAPSHOT] {location}: Saved {notebook_path.name}\")\n        else:\n            save_text(f\"Notebook not found. Searched: {[str(p) for p in search_paths]}\", \n                     export_dir / \"NO_SNAPSHOT.txt\")\n            print(f\"[SNAPSHOT] {location}: Notebook not found\")\n    \n    _save_snapshot(local_run_dir, \"LOCAL\")\n    _save_snapshot(drive_run_dir, \"DRIVE\")\n\nif EXPORT_CODE:\n    snapshot_code(PROJECT_ROOT, LOCAL_RUN_DIR, DRIVE_RUN_DIR)\n    save_text(\"Snapshot completed\", LOCAL_PATHS[\"logs_dir\"] / \"export_log.txt\")\n    save_text(\"Snapshot completed\", DRIVE_PATHS[\"logs_dir\"] / \"export_log.txt\")\n\nprint(\"[OK] BOOT + BLOCK 0 complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "d545dfea",
   "metadata": {},
   "source": [
    "## BLOCK 1 \u2014 ENV + IMPORTS (XGB + LSTM/GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916aaef8",
   "metadata": {},
   "outputs": [],
   "source": "\n# Colab installs (only if missing)\nif IN_COLAB:\n    try:\n        import yfinance  # noqa: F401\n        import pandas_datareader  # noqa: F401\n        import pandas_market_calendars  # noqa: F401\n        import scipy  # noqa: F401\n    except ImportError:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n             \"yfinance\", \"pandas_datareader\", \"pandas-market-calendars\", \"scipy\"],\n            check=True\n        )\n    try:\n        import lightgbm  # noqa: F401\n    except ImportError:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lightgbm\"],\n            check=True\n        )\n\nimport lightgbm as lgb\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch  # Required for EDA volatility plots\n\nimport re\nimport csv\n\nimport yfinance as yf\nfrom pandas_datareader import data as pdr\nimport pandas_market_calendars as mcal\n\nfrom scipy import stats\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import ParameterSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.feature_selection import mutual_info_regression\n\ntry:\n    from IPython.display import display\nexcept ImportError:\n    display = print\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\n# Random state from RUN_PARAMS\nRANDOM_STATE = int(RUN_PARAMS.get(\"random_state\", 42))\nnp.random.seed(RANDOM_STATE)\n\n# TensorFlow (optional)\nTF_AVAILABLE = False\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n\n    tf.random.set_seed(RANDOM_STATE)\n    TF_AVAILABLE = True\nexcept ImportError:\n    TF_AVAILABLE = False\n\n# NN config is defined per-model in RUN_PARAMS[\"lstm\"], [\"gru\"], [\"hybrid_seq\"], [\"hybrid_par\"]\n\nprint(\"[ENV] python:\", sys.version.split()[0])\nprint(\"[ENV] numpy:\", np.__version__)\nprint(\"[ENV] pandas:\", pd.__version__)\nprint(\"[ENV] yfinance:\", getattr(yf, \"__version__\", \"unknown\"))\nprint(\"[ENV] pandas_datareader:\", getattr(pdr, \"__version__\", \"unknown\"))\nprint(\"[ENV] pandas_market_calendars:\", getattr(mcal, \"__version__\", \"unknown\"))\nprint(\"[ENV] scipy:\", getattr(sys.modules.get(\"scipy\"), \"__version__\", \"unknown\"))\nprint(\"[ENV] xgboost:\", getattr(xgb, \"__version__\", \"unknown\"))\nprint(\"[ENV] tensorflow:\", \"OK\" if TF_AVAILABLE else \"NOT AVAILABLE\")\n\nprint(\"[ENV] RANDOM_STATE:\", RANDOM_STATE)\n\n# NN config is per-model (see RUN_PARAMS[\"lstm\"], [\"gru\"], etc.)\n\n\n# Global constants from config\nTARGET_T1 = str(RUN_PARAMS[\"data\"][\"target_col\"])\nEPS = float(RUN_PARAMS[\"features\"][\"eps\"])\nprint(\"[ENV] TARGET_T1:\", TARGET_T1)\nprint(\"[ENV] EPS:\", EPS)\n\nprint(\"[OK] BLOCK 1 complete.\")\n\n# -------------------------\n# Shared Metric Functions (used across all model blocks)\n# -------------------------\ndef _to_np(x):\n    \"\"\"Convert to numpy array.\"\"\"\n    return np.asarray(x, dtype=float)\n\n\ndef w_rmse(y_true, y_pred, w):\n    \"\"\"Weighted Root Mean Squared Error (Corrected).\"\"\"\n    y_true = _to_np(y_true)\n    y_pred = _to_np(y_pred)\n    w = _to_np(w)\n    mse_w = np.sum(w * (y_true - y_pred) ** 2) / (np.sum(w) + EPS)\n    return float(np.sqrt(mse_w))\n\n\ndef w_mae(y_true, y_pred, w):\n    \"\"\"Weighted Mean Absolute Error (Corrected).\"\"\"\n    y_true = _to_np(y_true)\n    y_pred = _to_np(y_pred)\n    w = _to_np(w)\n    mae_w = np.sum(w * np.abs(y_true - y_pred)) / (np.sum(w) + EPS)\n    return float(mae_w)\n\n\ndef dir_acc(y_true, y_pred):\n    \"\"\"Directional Accuracy.\"\"\"\n    y_true = _to_np(y_true)\n    y_pred = _to_np(y_pred)\n    return float(np.mean((y_true > 0) == (y_pred > 0)))\n\n\nprint(\"[ENV] Metric functions loaded: w_rmse, w_mae, dir_acc\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 2: Data Ingestion & Preprocessing\n",
    "\n",
    "**Download data, feature engineering, interim processing**\n",
    "\n",
    "**Blocks:** 2-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0df8e9",
   "metadata": {},
   "source": [
    "## BLOCK 2 \u2014 LOAD PRICES + EARNINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb33505",
   "metadata": {},
   "outputs": [],
   "source": "# Date range\nstart = RUN_PARAMS[\"data\"][\"start_date\"]\nend = RUN_PARAMS[\"data\"].get(\"end_date\") or datetime.now().strftime(\"%Y-%m-%d\")\nRUN_PARAMS[\"data\"][\"end_date\"] = end\n\n# --- 1. Define official calendar (NASDAQ) to avoid relying on Yahoo alone ---\nnyse = mcal.get_calendar('NASDAQ')\nvalid_days = nyse.valid_days(start_date=start, end_date=end)\nmaster_index = pd.Index(valid_days.tz_localize(None).normalize(), name=\"Date\")\n\n# Price tickers (includes GDAXI in main list)\nprice_tickers = [\n    \"GOOGL\", \"MSFT\", \"NVDA\",\n    \"^IXIC\", \"SPY\", \"QQQ\",\n    \"^VIX\", \"^TNX\",\n    \"XLK\", \"^GDAXI\"\n]\n\ndata_dict = {}\n\n# Download price data\nfor t in price_tickers:\n    df = yf.download(t, start=start, end=end, auto_adjust=True, progress=False)\n    if df is None or df.empty:\n        continue\n\n    df.columns = [f\"{t}_{c[0] if isinstance(c, tuple) else c}\" for c in df.columns]\n\n    if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:\n        df.index = df.index.tz_convert(None)\n\n    data_dict[t] = df\n\n# --- 2. Build Master Table aligned to official calendar ---\nif \"GOOGL\" not in data_dict:\n    raise ValueError(\"GOOGL data is missing. Cannot build master timeline.\")\n\n# Use master_index as base for all data\nprices_all = pd.DataFrame(index=master_index)\n\nfor t, df in data_dict.items():\n    # Left join to calendar ensures we don't miss official trading days\n    prices_all = prices_all.join(df, how=\"left\")\n\n# Forward fill prices only\nprice_cols = [c for c in prices_all.columns if any(s in c for s in ['_Open', '_High', '_Low', '_Close'])]\nprices_all[price_cols] = prices_all.sort_index()[price_cols].ffill()\n\n# Earnings data (GOOGL)\ntkr = yf.Ticker(\"GOOGL\")\nedf = tkr.get_earnings_dates(limit=100)\n\nearnings = pd.DataFrame(index=prices_all.index)\nearnings[\"is_earnings_day\"] = 0\n\n# Pre-create columns\nearnings[\"eps_surprise_pct_yahoo\"] = np.nan\nearnings[\"has_eps_surprise_yahoo\"] = 0\nearnings[\"eps_surprise_pct_calc\"] = np.nan\nearnings[\"has_eps_surprise_calc\"] = 0\n\nif edf is not None and len(edf) > 0:\n    edf = edf.copy()\n    idx = pd.to_datetime(edf.index)\n    if getattr(idx, \"tz\", None) is not None:\n        idx = idx.tz_convert(None)\n    idx = idx.normalize()\n\n    edf.index = idx\n    edf = edf[~edf.index.duplicated(keep=\"last\")].sort_index()\n\n    cols_lower = {c.lower(): c for c in edf.columns}\n\n    def pick_col(possible_names):\n        for name in possible_names:\n            key = name.lower()\n            if key in cols_lower:\n                return cols_lower[key]\n        return None\n\n    col_exp = pick_col([\"EPS Estimate\", \"eps estimate\", \"Eps Estimate\"])\n    col_act = pick_col([\"Reported EPS\", \"reported eps\", \"EPS Actual\", \"eps actual\"])\n    col_pct = pick_col([\"Surprise(%)\", \"surprise(%)\", \"Surprise (%)\", \"surprise (%)\"])\n\n    eps_daily = pd.DataFrame(index=edf.index)\n    eps_daily[\"eps_expected\"] = edf[col_exp] if col_exp else np.nan\n    eps_daily[\"eps_actual\"] = edf[col_act] if col_act else np.nan\n\n    if col_exp and col_act:\n        eps_surprise = eps_daily[\"eps_actual\"] - eps_daily[\"eps_expected\"]\n        denom = eps_daily[\"eps_expected\"].abs()\n        eps_daily[\"eps_surprise_pct_calc\"] = np.where(denom > 0, 100.0 * (eps_surprise / denom), np.nan)\n    else:\n        eps_daily[\"eps_surprise_pct_calc\"] = np.nan\n\n    eps_daily[\"eps_surprise_pct_yahoo\"] = edf[col_pct] if col_pct else np.nan\n    data_dict[\"EARNINGS_EPS_DEBUG\"] = eps_daily.copy()\n\n    # Align to official trading days in prices_all\n    eps_on_trading_days = eps_daily.reindex(prices_all.index)\n\n    earnings[\"is_earnings_day\"] = prices_all.index.isin(eps_daily.index).astype(\"int8\")\n    earnings[\"eps_surprise_pct_yahoo\"] = eps_on_trading_days[\"eps_surprise_pct_yahoo\"].values\n    earnings[\"eps_surprise_pct_calc\"] = eps_on_trading_days[\"eps_surprise_pct_calc\"].values\n    earnings[\"has_eps_surprise_yahoo\"] = earnings[\"eps_surprise_pct_yahoo\"].notna().astype(\"int8\")\n    earnings[\"has_eps_surprise_calc\"] = earnings[\"eps_surprise_pct_calc\"].notna().astype(\"int8\")\n\n# Merge prices + earnings\nfull_df = prices_all.join(earnings, how=\"left\")\n\n# Drop low-information volume columns\nDROP_VOLUME_COLS = [\"^VIX_Volume\", \"^TNX_Volume\"]\nfull_df = full_df.drop(columns=[c for c in DROP_VOLUME_COLS if c in full_df.columns])\n\n# --- Summary functions (unchanged) ---\ndef feature_info(df: pd.DataFrame) -> pd.DataFrame:\n    return (\n        pd.DataFrame({\n            \"feature\": df.columns,\n            \"dtype\": [df[c].dtype for c in df.columns],\n            \"non_null\": [int(df[c].notna().sum()) for c in df.columns],\n            \"null\": [int(df[c].isna().sum()) for c in df.columns],\n            \"null_pct\": [float(df[c].isna().mean() * 100.0) for c in df.columns],\n            \"unique\": [int(df[c].nunique(dropna=True)) for c in df.columns],\n        })\n        .sort_values([\"null_pct\", \"feature\"])\n        .reset_index(drop=True)\n    )\n\ndef frequency_summary(df: pd.DataFrame) -> pd.DataFrame:\n    rows = []\n    n = len(df)\n    for c in df.columns:\n        s = df[c]\n        n_nan = int(s.isna().sum())\n        n_nonnull = int(s.notna().sum())\n        if pd.api.types.is_numeric_dtype(s):\n            s_nn = s.dropna()\n            n_zero = int((s_nn == 0).sum())\n            n_nonzero = int((s_nn != 0).sum())\n            pct_nonzero_of_nonnull = (100.0 * n_nonzero / n_nonnull) if n_nonnull else np.nan\n        else:\n            n_zero = 0\n            n_nonzero = n_nonnull\n            pct_nonzero_of_nonnull = (100.0 * n_nonzero / n_nonnull) if n_nonnull else np.nan\n        rows.append({\n            \"feature\": c, \"dtype\": str(s.dtype), \"n_total\": n, \"n_nan\": n_nan,\n            \"n_nonnull\": n_nonnull, \"n_zero\": n_zero, \"n_nonzero\": n_nonzero,\n            \"pct_nan\": (100.0 * n_nan / n) if n else np.nan,\n            \"pct_nonzero\": (100.0 * n_nonzero / n) if n else np.nan,\n            \"pct_nonzero_of_nonnull\": pct_nonzero_of_nonnull,\n        })\n    return pd.DataFrame(rows).sort_values([\"pct_nan\", \"feature\"]).reset_index(drop=True)\n\nprint(\"\\n=== FEATURE INFO SUMMARY ===\")\nprint(feature_info(full_df).to_string(index=False))\n\nprint(\"\\n=== FEATURE FREQUENCY SUMMARY ===\")\nprint(frequency_summary(full_df).to_string(index=False))\n\nprint(\"[OK] BLOCK 2 complete. full_df shape:\", full_df.shape)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 2B \u2014 EU BREAK CLOSE FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =========================\n# EU \"GAP\" FLAGS (US Closed, EU Open)\n# =========================\ndef build_eu_info_gap_flags(\n    full_df: pd.DataFrame,\n    eu_ticker: str = \"^GDAXI\",\n    apply_to: str = \"next_us_trading_day\"\n) -> pd.DataFrame:\n    \"\"\"\n    Returns:\n        - EU_break_close_flag (int8): 1 if US was closed previously while EU was open.\n        - EU_break_close_up   (int8): 1 if EU cumulative return during US holiday was positive.\n        - EU_break_close_down (int8): 1 if EU cumulative return during US holiday was negative.\n    \"\"\"\n    # 1. Get EU data from data_dict downloaded in block 2\n    if eu_ticker not in data_dict:\n        raise ValueError(f\"{eu_ticker} missing from data_dict. Ensure Block 2 ran correctly.\")\n    \n    eu_data = data_dict[eu_ticker].copy()\n    eu_days = eu_data.index.normalize()\n    us_days = full_df.index.normalize()\n    \n    # 2. Identify gap days: Europe open, US closed\n    gap_days = eu_days.difference(us_days)\n    \n    # 3. Compute log returns (allows summing over consecutive holiday days)\n    close_col = f\"{eu_ticker}_Close\"\n    eu_log_ret = np.log(eu_data[close_col] / eu_data[close_col].shift(1))\n    \n    # 4. Create events table for gap days\n    eu_gap_events = pd.DataFrame(index=gap_days)\n    eu_gap_events[\"gap_return\"] = eu_log_ret.reindex(gap_days)\n    \n    # 5. Map to next US trading day\n    # searchsorted(side='left') finds first US index >= holiday day\n    pos = np.searchsorted(us_days, eu_gap_events.index, side=\"left\")\n    valid_mask = pos < len(us_days)\n    \n    eu_gap_events['target_us_date'] = pd.NaT  # NaT for datetime instead of np.nan\n    eu_gap_events.loc[valid_mask, 'target_us_date'] = us_days[pos[valid_mask]]\n    \n    # 6. Aggregate (for long holidays, sum all EU returns to US opening day)\n    agg_gap = eu_gap_events.dropna(subset=['target_us_date']).groupby('target_us_date')[\"gap_return\"].sum()\n    \n    # 7. Create output table in original format\n    out = pd.DataFrame(index=us_days)\n    out[\"EU_break_close_flag\"] = np.int8(0)\n    out[\"EU_break_close_up\"] = np.int8(0)\n    out[\"EU_break_close_down\"] = np.int8(0)\n    \n    out.loc[agg_gap.index, \"EU_break_close_flag\"] = 1\n    out.loc[agg_gap.index, \"EU_break_close_up\"] = (agg_gap > 0).astype(\"int8\")\n    out.loc[agg_gap.index, \"EU_break_close_down\"] = (agg_gap < 0).astype(\"int8\")\n    \n    return out\n\n# =========================\n# USAGE\n# =========================\nEU_CFG = RUN_PARAMS.get(\"eu_break_close\", {})\nEU_ENABLED = bool(EU_CFG.get(\"enabled\", True))\n\nif EU_ENABLED:\n    print(\"[INFO] Building EU break close flags...\")\n    \n    # Function now receives updated full_df\n    eu_flags = build_eu_info_gap_flags(\n        full_df=full_df,\n        eu_ticker=EU_CFG.get(\"eu_ticker\", \"^GDAXI\")\n    )\n\n    # Join - align to original full_df index\n    eu_flags.index = full_df.index\n    \n    # Remove existing columns if block runs again\n    cols_to_drop = [c for c in eu_flags.columns if c in full_df.columns]\n    if cols_to_drop:\n        full_df = full_df.drop(columns=cols_to_drop)\n        \n    full_df = full_df.join(eu_flags, how=\"left\")\n    \n    # Fill NaN with 0 for flag columns\n    for col in [\"EU_break_close_flag\", \"EU_break_close_up\", \"EU_break_close_down\"]:\n        if col in full_df.columns:\n            full_df[col] = full_df[col].fillna(0).astype(\"int8\")\n    \n    n_events = full_df[\"EU_break_close_flag\"].sum()\n    print(f\"[INFO] EU break close events: {n_events}\")\n    print(f\"[INFO] EU break close up: {full_df['EU_break_close_up'].sum()}\")\n    print(f\"[INFO] EU break close down: {full_df['EU_break_close_down'].sum()}\")\n    print(f\"[OK] BLOCK 2B complete. full_df shape: {full_df.shape}\")\nelse:\n    print(\"[SKIP] BLOCK 2B \u2014 EU break close disabled in config.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "9b615080",
   "metadata": {},
   "source": [
    "## BLOCK 3 \u2014 MACRO FEATURES (FRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2809eaa",
   "metadata": {},
   "outputs": [],
   "source": "\n# Preconditions\nassert \"full_df\" in globals(), \"[ERROR] full_df is not defined.\"\nassert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\nfull_df = full_df.sort_index()\n\n# Use RUN_PARAMS dates (set in Block 2)\nstart = pd.to_datetime(RUN_PARAMS[\"data\"][\"start_date\"])\nend = pd.to_datetime(RUN_PARAMS[\"data\"][\"end_date\"])\n\nprint(\"[INFO] full_df range:\", full_df.index.min(), \"->\", full_df.index.max(), \"| rows:\", len(full_df))\nprint(\"[INFO] FRED pull range:\", start, \"->\", end)\n\n# --- EPS surprise: flag + fill-0 ---\neps_pairs = [\n    (\"eps_surprise_pct_yahoo\", \"has_eps_surprise_yahoo\"),\n    (\"eps_surprise_pct_calc\", \"has_eps_surprise_calc\"),\n]\nfor val_col, flag_col in eps_pairs:\n    if val_col in full_df.columns and flag_col in full_df.columns:\n        mask_fill0 = (full_df[flag_col] == 0)\n        full_df.loc[mask_fill0, val_col] = full_df.loc[mask_fill0, val_col].fillna(0.0)\n\n        still_na_when_flag0 = int(full_df.loc[full_df[flag_col] == 0, val_col].isna().sum())\n        assert still_na_when_flag0 == 0, f\"[ERROR] {val_col} still has NaN where {flag_col}==0\"\n\nprint(\"[OK] EPS surprise handled: eps_surprise_pct_* filled with 0 where has_* == 0 (flags preserved).\")\n\n# --- Pull monthly series from FRED ---\ncpi = pdr.DataReader(\"CPIAUCSL\", \"fred\", start, end).rename(columns={\"CPIAUCSL\": \"CPI\"})\nrate = pdr.DataReader(\"FEDFUNDS\", \"fred\", start, end).rename(columns={\"FEDFUNDS\": \"FEDFUNDS\"})\n\ncpi.index = pd.to_datetime(cpi.index)\nrate.index = pd.to_datetime(rate.index)\n\n# Compute MONTHLY features (BEFORE daily ffill)\n\n# CPI features\ncpi[\"CPI_pct_mom\"] = cpi[\"CPI\"].pct_change(1, fill_method=None)\ncpi[\"CPI_accel_pct_mom\"] = cpi[\"CPI_pct_mom\"] - cpi[\"CPI_pct_mom\"].shift(1)\ncpi_feats_monthly = cpi[[\"CPI_pct_mom\", \"CPI_accel_pct_mom\"]].copy()\n\n# FEDFUNDS features\nrate[\"FEDFUNDS_delta_mom\"] = rate[\"FEDFUNDS\"].diff(1)\nrate[\"FEDFUNDS_changed\"] = (rate[\"FEDFUNDS_delta_mom\"].fillna(0) != 0).astype(\"int8\")\nrate[\"FEDFUNDS_level\"] = rate[\"FEDFUNDS\"].copy()\nrate_feats_monthly = rate[[\"FEDFUNDS_delta_mom\", \"FEDFUNDS_changed\", \"FEDFUNDS_level\"]].copy()\n\nprint(\"\\n[INFO] Monthly CPI feats head:\\n\", cpi_feats_monthly.head(6))\nprint(\"\\n[INFO] Monthly FEDFUNDS feats head:\\n\", rate_feats_monthly.head(6))\n\n# Upsample FEATURES to daily and forward-fill (calendar daily)\ncpi_feats_daily = cpi_feats_monthly.resample(\"D\").ffill()\nrate_feats_daily = rate_feats_monthly.resample(\"D\").ffill()\nmacro_daily = pd.concat([cpi_feats_daily, rate_feats_daily], axis=1)\n\n# Align macro index to full_df (exact same trading dates)\nmacro_daily.index = pd.to_datetime(macro_daily.index)\nmacro_daily.index.name = full_df.index.name\nmacro_aligned = macro_daily.reindex(full_df.index)\n\n# Missingness flags BEFORE any fill (macro only)\nFLAG_SUFFIX = \"_is_missing\"\nmacro_numeric_to_fill0 = [\"CPI_pct_mom\", \"CPI_accel_pct_mom\", \"FEDFUNDS_delta_mom\"]\n\nfor col in macro_numeric_to_fill0:\n    if col in macro_aligned.columns:\n        macro_aligned[f\"{col}{FLAG_SUFFIX}\"] = macro_aligned[col].isna().astype(\"int8\")\n\n# Fill only macro gaps (no global full_df ffill)\nmacro_cols = macro_aligned.columns.tolist()\nmacro_aligned[macro_cols] = macro_aligned[macro_cols].ffill()\n\n# After ffill, leading NaNs may remain. Fill with 0 for selected numeric cols.\nfor col in macro_numeric_to_fill0:\n    if col in macro_aligned.columns:\n        macro_aligned[col] = macro_aligned[col].fillna(0.0)\n\n# Enforce FEDFUNDS_changed to stay binary int8 after reindex/ffill\nFLAG_COL = \"FEDFUNDS_changed\"\nif FLAG_COL in macro_aligned.columns:\n    macro_aligned[FLAG_COL] = (\n        macro_aligned[FLAG_COL]\n        .fillna(0)\n        .clip(0, 1)\n        .astype(\"int8\")\n    )\n\n# Release-day flags (on trading-day index)\nif \"CPI_pct_mom\" in macro_aligned.columns:\n    cpi_series = macro_aligned[\"CPI_pct_mom\"].astype(\"float64\")\n    macro_aligned[\"CPI_release_day\"] = (cpi_series.notna() & cpi_series.ne(cpi_series.shift(1))).astype(\"int8\")\n\nif \"FEDFUNDS_level\" in macro_aligned.columns:\n    ff_series = macro_aligned[\"FEDFUNDS_level\"].astype(\"float64\")\n    macro_aligned[\"FEDFUNDS_release_day\"] = (ff_series.notna() & ff_series.ne(ff_series.shift(1))).astype(\"int8\")\n\n# Update macro_cols after adding flags\nmacro_cols = macro_aligned.columns.tolist()\n\n# Remove existing macro columns from full_df to avoid duplicates\nexisting_macro_cols = [c for c in macro_cols if c in full_df.columns]\nif existing_macro_cols:\n    print(f\"[INFO] Removing {len(existing_macro_cols)} existing macro cols from full_df before merge\")\n    full_df = full_df.drop(columns=existing_macro_cols)\n\n# Merge into full_df\nfull_df_merged = pd.concat([full_df, macro_aligned], axis=1)\n\n# Enforce missingness flags + release flags to int8\nfor col in [c for c in full_df_merged.columns if c.endswith(FLAG_SUFFIX)]:\n    full_df_merged[col] = full_df_merged[col].fillna(0).clip(0, 1).astype(\"int8\")\n\nfor col in [\"CPI_release_day\", \"FEDFUNDS_release_day\"]:\n    if col in full_df_merged.columns:\n        full_df_merged[col] = full_df_merged[col].fillna(0).clip(0, 1).astype(\"int8\")\n\nif FLAG_COL in full_df_merged.columns:\n    full_df_merged[FLAG_COL] = (\n        full_df_merged[FLAG_COL]\n        .fillna(0)\n        .clip(0, 1)\n        .astype(\"int8\")\n    )\n\n# --- Diagnostics (detailed) ---\nprint(\"\\n[CHECK] Macro columns (including flags + release):\", macro_cols)\nprint(\"\\n[CHECK] Macro head (aligned to trading dates):\\n\", full_df_merged[macro_cols].head(15))\nprint(\"\\n[CHECK] Macro NaNs count:\\n\", full_df_merged[macro_cols].isna().sum())\n\nif FLAG_COL in full_df_merged.columns:\n    print(\"\\n[CHECK] FEDFUNDS_changed dtype:\", full_df_merged[FLAG_COL].dtype)\n    print(\"\\n[CHECK] FEDFUNDS_changed value counts:\\n\", full_df_merged[FLAG_COL].value_counts(dropna=False))\n\n# Release-day quick counts\nfor col in [\"CPI_release_day\", \"FEDFUNDS_release_day\"]:\n    if col in full_df_merged.columns:\n        print(f\"\\n[CHECK] {col} value counts:\\n\", full_df_merged[col].value_counts(dropna=False))\n\n# EPS diagnostics (post-fill)\nfor val_col, flag_col in eps_pairs:\n    if val_col in full_df_merged.columns and flag_col in full_df_merged.columns:\n        na_total = int(full_df_merged[val_col].isna().sum())\n        na_flag0 = int(full_df_merged.loc[full_df_merged[flag_col] == 0, val_col].isna().sum())\n        print(f\"\\n[CHECK] EPS {val_col}: na_total={na_total} | na_when_{flag_col}==0 => {na_flag0}\")\n\nprint(\"\\n[CHECK] full_df_merged info():\")\nprint(full_df_merged.info())\n\n# Continue downstream\nfull_df = full_df_merged\n\nprint(\"[OK] BLOCK 3 complete. full_df shape:\", full_df.shape)"
  },
  {
   "cell_type": "markdown",
   "id": "a30169d8",
   "metadata": {},
   "source": [
    "## BLOCK 4 \u2014 TIME FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49101e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_before = set(full_df.columns)\n",
    "\n",
    "# Ensure clean datetime index\n",
    "full_df.index = pd.to_datetime(full_df.index)\n",
    "full_df.index.name = \"Date\"\n",
    "full_df = full_df.sort_index()\n",
    "idx = full_df.index\n",
    "\n",
    "# Discrete time features\n",
    "full_df[\"day_of_week\"] = idx.weekday.astype(\"int8\")   # 0=Mon ... 6=Sun\n",
    "full_df[\"month\"] = idx.month.astype(\"int8\")           # 1..12\n",
    "full_df[\"quarter\"] = idx.quarter.astype(\"int8\")       # 1..4\n",
    "\n",
    "# Quarter binary dummies\n",
    "q = full_df[\"quarter\"].astype(\"int8\")\n",
    "full_df[\"is_q1\"] = (q == 1).astype(\"int8\")\n",
    "full_df[\"is_q2\"] = (q == 2).astype(\"int8\")\n",
    "full_df[\"is_q3\"] = (q == 3).astype(\"int8\")\n",
    "full_df[\"is_q4\"] = (q == 4).astype(\"int8\")\n",
    "\n",
    "# Cyclical features\n",
    "weekday = idx.weekday.astype(int)\n",
    "period_week = 5\n",
    "full_df[\"weekday_sin_5\"] = np.sin(2 * np.pi * (weekday % period_week) / period_week)\n",
    "full_df[\"weekday_cos_5\"] = np.cos(2 * np.pi * (weekday % period_week) / period_week)\n",
    "\n",
    "day_of_year = idx.dayofyear.astype(int)  # 1..365/366\n",
    "year_len = np.where(idx.is_leap_year, 366, 365)\n",
    "full_df[\"day_of_year_sin\"] = np.sin(2 * np.pi * (day_of_year - 1) / year_len)\n",
    "full_df[\"day_of_year_cos\"] = np.cos(2 * np.pi * (day_of_year - 1) / year_len)\n",
    "\n",
    "# Report newly added columns\n",
    "cols_after = set(full_df.columns)\n",
    "new_cols = sorted(cols_after - cols_before)\n",
    "\n",
    "print(\"New time features added to full_df:\")\n",
    "for c in new_cols:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "preview_cols = [\n",
    "    \"day_of_week\", \"month\", \"quarter\", \"is_q1\", \"is_q2\", \"is_q3\", \"is_q4\",\n",
    "    \"weekday_sin_5\", \"weekday_cos_5\", \"day_of_year_sin\", \"day_of_year_cos\"\n",
    "]\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(full_df[preview_cols].head(10))\n",
    "\n",
    "print(\"\\nNaN check (time features):\")\n",
    "print(full_df[preview_cols].isna().sum())\n",
    "\n",
    "print(\"[OK] BLOCK 4 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fc6cf",
   "metadata": {},
   "source": [
    "## BLOCK 5 \u2014 CONTROL CHECKS (leakage/time integrity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"[CONTROL] Starting control checks for leakage/time integrity...\")\n",
    "\n",
    "# Basic index integrity\n",
    "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a pandas DataFrame.\"\n",
    "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "\n",
    "full_df = full_df.sort_index()\n",
    "full_df.index.name = full_df.index.name or \"Date\"\n",
    "\n",
    "assert full_df.index.is_monotonic_increasing, \"[ERROR] Index is not monotonic increasing after sort.\"\n",
    "assert full_df.index.is_unique, \"[ERROR] Index has duplicate timestamps.\"\n",
    "\n",
    "print(\n",
    "    f\"[OK] Index: DatetimeIndex | sorted | unique | range={full_df.index.min()} -> {full_df.index.max()} | rows={len(full_df)}\"\n",
    ")\n",
    "\n",
    "# Column sanity / duplicates\n",
    "cols = list(full_df.columns)\n",
    "dup_cols = pd.Index(cols)[pd.Index(cols).duplicated()].tolist()\n",
    "assert len(dup_cols) == 0, f\"[ERROR] Duplicate column names detected: {dup_cols}\"\n",
    "print(f\"[OK] Columns: {len(cols)} total | no duplicate column names\")\n",
    "\n",
    "# Leakage guard by naming conventions\n",
    "t1_cols = [c for c in cols if re.search(r\"(_t1\\b|_t\\+1\\b|t_plus_1\\b)\", c)]\n",
    "assert len(t1_cols) == 0, f\"[LEAKAGE ERROR] Found t+1 style columns in full_df: {t1_cols[:20]} (showing up to 20)\"\n",
    "\n",
    "suspicious_patterns = [\n",
    "    r\"\\bshift\\(\\s*-1\\s*\\)\", r\"\\blead\\b\", r\"\\bforward\\b\", r\"\\bfwd\\b\",\n",
    "    r\"\\bnext_day\\b\", r\"\\btomorrow\\b\", r\"\\bt\\+1\\b\"\n",
    "]\n",
    "sus_cols = [c for c in cols if any(re.search(p, c.lower()) for p in suspicious_patterns)]\n",
    "if len(sus_cols) > 0:\n",
    "    print(f\"[WARN] Suspicious potential lead columns by NAME: {sus_cols[:25]} (showing up to 25)\")\n",
    "else:\n",
    "    print(\"[OK] No suspicious lead/shift(-1) patterns found in column names\")\n",
    "\n",
    "print(\"[OK] Leakage guard (name-based) passed: no *_t1 columns\")\n",
    "\n",
    "# Missingness report\n",
    "earnings_cols = [\n",
    "    c for c in cols\n",
    "    if c.startswith(\"eps_surprise\")\n",
    "    or c.startswith(\"has_eps_surprise\")\n",
    "    or c == \"is_earnings_day\"\n",
    "]\n",
    "macro_cols = [c for c in cols if c.startswith(\"CPI_\") or c.startswith(\"FEDFUNDS_\")]\n",
    "\n",
    "focus_cols = [c for c in (earnings_cols + macro_cols) if c in cols]\n",
    "\n",
    "print(\"\\n[CONTROL] Missingness report (focus: Earnings + Macro)\")\n",
    "if len(focus_cols) == 0:\n",
    "    print(\"[INFO] No focus columns found (earnings/macro) \u2014 skipping focus missingness table.\")\n",
    "else:\n",
    "    miss = full_df[focus_cols].isna().sum().sort_values(ascending=False)\n",
    "    miss_pct = (miss / len(full_df) * 100).round(2)\n",
    "    miss_tbl = pd.DataFrame({\"na_count\": miss, \"na_pct\": miss_pct})\n",
    "    print(miss_tbl)\n",
    "\n",
    "na_any = full_df.isna().sum()\n",
    "top_na = na_any[na_any > 0].sort_values(ascending=False).head(15)\n",
    "print(\"\\n[CONTROL] Global NaNs (top 15 cols with NA)\")\n",
    "if len(top_na) == 0:\n",
    "    print(\"[OK] No NaNs in full_df.\")\n",
    "else:\n",
    "    top_na_pct = (top_na / len(full_df) * 100).round(2)\n",
    "    print(pd.DataFrame({\"na_count\": top_na, \"na_pct\": top_na_pct}))\n",
    "\n",
    "# Event-like columns tags (governance lists)\n",
    "EVENT_COLS_EARNINGS = earnings_cols\n",
    "EVENT_COLS_MACRO = macro_cols\n",
    "EVENT_COLS_ALL = sorted(set(EVENT_COLS_EARNINGS + EVENT_COLS_MACRO))\n",
    "\n",
    "print(\"\\n[CONTROL] Event-like column tagging:\")\n",
    "print(f\"  - Earnings cols: {len(EVENT_COLS_EARNINGS)}\")\n",
    "print(f\"  - Macro cols   : {len(EVENT_COLS_MACRO)}\")\n",
    "print(f\"  - Total event  : {len(EVENT_COLS_ALL)}\")\n",
    "\n",
    "flag_checks = [c for c in [\"is_earnings_day\", \"has_eps_surprise_yahoo\", \"has_eps_surprise_calc\", \"FEDFUNDS_changed\"] if c in cols]\n",
    "if len(flag_checks) > 0:\n",
    "    print(\"\\n[CONTROL] Flag dtype + value counts (quick):\")\n",
    "    for c in flag_checks:\n",
    "        vc = full_df[c].value_counts(dropna=False)\n",
    "        print(f\"  - {c}: dtype={full_df[c].dtype} | values={vc.to_dict()}\")\n",
    "\n",
    "# Save governance artifacts\n",
    "governance = {\n",
    "    \"event_cols_earnings\": EVENT_COLS_EARNINGS,\n",
    "    \"event_cols_macro\": EVENT_COLS_MACRO,\n",
    "    \"event_cols_all\": EVENT_COLS_ALL,\n",
    "    \"focus_cols_missingness\": focus_cols,\n",
    "    \"flag_checks\": flag_checks,\n",
    "}\n",
    "\n",
    "print(\"\\n[CONTROL] DONE. No features created. No missingness fixed. Leakage name-guards applied.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"[OK] BLOCK 5 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014e591",
   "metadata": {},
   "source": [
    "## BLOCK 6 \u2014 RAW TRANSFORMATIONS (NO ROLLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps = float(RUN_PARAMS[\"features\"][\"eps\"])  # numeric safety for logs/divisions\n",
    "\n",
    "# Preconditions\n",
    "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
    "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "\n",
    "\n",
    "# Identify OHLCV ticker prefixes\n",
    "def has_cols(prefix: str, required: list) -> bool:\n",
    "    \"\"\"Check if all required columns exist for a ticker prefix.\"\"\"\n",
    "    return all(f\"{prefix}_{c}\" in full_df.columns for c in required)\n",
    "\n",
    "\n",
    "required_ohlc = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "required_ohlcv = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "prefixes = sorted({c.rsplit(\"_\", 1)[0] for c in full_df.columns if c.endswith(\"_Close\")})\n",
    "\n",
    "# EXCLUDE market indicators from OHLC-style transforms\n",
    "EXCLUDE_RAW_OHLC = set(RUN_PARAMS[\"features\"][\"exclude_raw_ohlc\"])\n",
    "\n",
    "ohlc_prefixes = [p for p in prefixes if has_cols(p, required_ohlc) and p not in EXCLUDE_RAW_OHLC]\n",
    "ohlcv_prefixes = [p for p in prefixes if has_cols(p, required_ohlcv) and p not in EXCLUDE_RAW_OHLC]\n",
    "\n",
    "print(f\"[INFO] Found {len(ohlc_prefixes)} OHLC tickers (excluded {sorted(EXCLUDE_RAW_OHLC)}): {ohlc_prefixes}\")\n",
    "print(f\"[INFO] Found {len(ohlcv_prefixes)} OHLCV tickers (with Volume, excluded {sorted(EXCLUDE_RAW_OHLC)}): {ohlcv_prefixes}\")\n",
    "\n",
    "# Raw transforms per ticker (NO rolling)\n",
    "new_cols = {}\n",
    "\n",
    "for p in ohlc_prefixes:\n",
    "    o = full_df[f\"{p}_Open\"].astype(\"float64\")\n",
    "    h = full_df[f\"{p}_High\"].astype(\"float64\")\n",
    "    l = full_df[f\"{p}_Low\"].astype(\"float64\")\n",
    "    c = full_df[f\"{p}_Close\"].astype(\"float64\")\n",
    "\n",
    "    # log returns\n",
    "    new_cols[f\"{p}_logret_cc\"] = np.log((c + eps) / (c.shift(1) + eps))\n",
    "    new_cols[f\"{p}_logret_oc\"] = np.log((c + eps) / (o + eps))\n",
    "    new_cols[f\"{p}_logret_gap_co\"] = np.log((o + eps) / (c.shift(1) + eps))\n",
    "\n",
    "    # abs variants\n",
    "    new_cols[f\"{p}_abs_logret_cc\"] = new_cols[f\"{p}_logret_cc\"].abs()\n",
    "    new_cols[f\"{p}_abs_logret_oc\"] = new_cols[f\"{p}_logret_oc\"].abs()\n",
    "    new_cols[f\"{p}_abs_logret_gap_co\"] = new_cols[f\"{p}_logret_gap_co\"].abs()\n",
    "\n",
    "    # intraday range\n",
    "    new_cols[f\"{p}_log_hl\"] = np.log((h + eps) / (l + eps))\n",
    "\n",
    "    # close position within High-Low range\n",
    "    denom_hl = (h - l).replace(0.0, np.nan)\n",
    "    close_pos = (c - l) / denom_hl\n",
    "    new_cols[f\"{p}_close_pos_hl\"] = close_pos\n",
    "    new_cols[f\"{p}_close_pos_hl_centered\"] = close_pos - 0.5\n",
    "\n",
    "    # lags for logret_cc\n",
    "    for lag in [1, 5, 21]:\n",
    "        new_cols[f\"{p}_logret_cc_lag{lag}\"] = new_cols[f\"{p}_logret_cc\"].shift(lag)\n",
    "\n",
    "# Volume features (only where Volume exists)\n",
    "for p in ohlcv_prefixes:\n",
    "    v = full_df[f\"{p}_Volume\"].astype(\"float64\")\n",
    "    c = full_df[f\"{p}_Close\"].astype(\"float64\")\n",
    "\n",
    "    new_cols[f\"{p}_log_vol\"] = np.log(v + 1.0)\n",
    "    new_cols[f\"{p}_log_vol_chg_1d\"] = new_cols[f\"{p}_log_vol\"] - new_cols[f\"{p}_log_vol\"].shift(1)\n",
    "\n",
    "    dollar_vol = (c * v).astype(\"float64\")\n",
    "    new_cols[f\"{p}_log_dollar_vol\"] = np.log(dollar_vol + 1.0)\n",
    "\n",
    "# Attach to full_df (remove existing to avoid duplicates on re-run)\n",
    "new_df = pd.DataFrame(new_cols, index=full_df.index)\n",
    "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
    "if existing_new_cols:\n",
    "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
    "    full_df = full_df.drop(columns=existing_new_cols)\n",
    "full_df = pd.concat([full_df, new_df], axis=1)\n",
    "\n",
    "# Diagnostics\n",
    "cols_after = set(full_df.columns)\n",
    "added = sorted(cols_after - cols_before)\n",
    "\n",
    "print(f\"\\n[OK] BLOCK 6 added {len(added)} raw feature columns.\")\n",
    "print(\"[INFO] Sample of added columns (first 40):\")\n",
    "for c in added[:40]:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "nan_counts = full_df[added].isna().sum().sort_values(ascending=False).head(15)\n",
    "print(\"\\n[CHECK] NaNs in NEW raw features (top 15):\")\n",
    "print(nan_counts)\n",
    "\n",
    "print(\"[OK] BLOCK 6 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c218968",
   "metadata": {},
   "source": [
    "## BLOCK 7 \u2014 ROLLING STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "# Windows from RUN_PARAMS\n",
    "W_SHORT = int(RUN_PARAMS[\"features\"][\"rolling_w_short\"])\n",
    "W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
    "DO_VOLUME_ROLLING = bool(RUN_PARAMS[\"features\"][\"do_volume_rolling\"])\n",
    "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "\n",
    "# Determine tickers that have raw series\n",
    "tickers = sorted({\n",
    "    c.replace(\"_logret_cc\", \"\")\n",
    "    for c in full_df.columns\n",
    "    if c.endswith(\"_logret_cc\") and not c.endswith(\"_abs_logret_cc\")\n",
    "})\n",
    "print(f\"[INFO] Rolling stats tickers detected from *_logret_cc (excluding *_abs_*): {tickers}\")\n",
    "\n",
    "new_cols = {}\n",
    "\n",
    "# Rolling stats for returns / abs returns / HL\n",
    "for p in tickers:\n",
    "    col_r = f\"{p}_logret_cc\"\n",
    "    col_ar = f\"{p}_abs_logret_cc\"\n",
    "    col_hl = f\"{p}_log_hl\"\n",
    "\n",
    "    if col_r not in full_df.columns:\n",
    "        continue\n",
    "\n",
    "    r = full_df[col_r].astype(\"float64\")\n",
    "\n",
    "    # rolling mean/std of logret_cc\n",
    "    r_m_s = r.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
    "    r_s_s = r.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
    "\n",
    "    r_m_l = r.rolling(W_LONG, min_periods=W_LONG).mean()\n",
    "    r_s_l = r.rolling(W_LONG, min_periods=W_LONG).std()\n",
    "\n",
    "    new_cols[f\"{p}_logret_cc_mean_{W_SHORT}\"] = r_m_s\n",
    "    new_cols[f\"{p}_logret_cc_std_{W_SHORT}\"] = r_s_s\n",
    "    new_cols[f\"{p}_logret_cc_mean_{W_LONG}\"] = r_m_l\n",
    "    new_cols[f\"{p}_logret_cc_std_{W_LONG}\"] = r_s_l\n",
    "\n",
    "    # z-score vs long window\n",
    "    new_cols[f\"{p}_logret_cc_z_{W_LONG}\"] = (r - r_m_l) / (r_s_l + eps)\n",
    "\n",
    "    # abs_logret_cc mean/std\n",
    "    if col_ar in full_df.columns:\n",
    "        ar = full_df[col_ar].astype(\"float64\")\n",
    "        new_cols[f\"{p}_abs_logret_cc_mean_{W_SHORT}\"] = ar.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
    "        new_cols[f\"{p}_abs_logret_cc_std_{W_SHORT}\"] = ar.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
    "        new_cols[f\"{p}_abs_logret_cc_mean_{W_LONG}\"] = ar.rolling(W_LONG, min_periods=W_LONG).mean()\n",
    "        new_cols[f\"{p}_abs_logret_cc_std_{W_LONG}\"] = ar.rolling(W_LONG, min_periods=W_LONG).std()\n",
    "\n",
    "    # HL rolling mean/std + z-score\n",
    "    if col_hl in full_df.columns:\n",
    "        hl = full_df[col_hl].astype(\"float64\")\n",
    "        hl_m_s = hl.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
    "        hl_s_s = hl.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
    "\n",
    "        hl_m_l = hl.rolling(W_LONG, min_periods=W_LONG).mean()\n",
    "        hl_s_l = hl.rolling(W_LONG, min_periods=W_LONG).std()\n",
    "\n",
    "        new_cols[f\"{p}_log_hl_mean_{W_SHORT}\"] = hl_m_s\n",
    "        new_cols[f\"{p}_log_hl_std_{W_SHORT}\"] = hl_s_s\n",
    "        new_cols[f\"{p}_log_hl_mean_{W_LONG}\"] = hl_m_l\n",
    "        new_cols[f\"{p}_log_hl_std_{W_LONG}\"] = hl_s_l\n",
    "        new_cols[f\"{p}_log_hl_z_{W_LONG}\"] = (hl - hl_m_l) / (hl_s_l + eps)\n",
    "\n",
    "# Optional: Volume rolling stats\n",
    "if DO_VOLUME_ROLLING:\n",
    "    for p in tickers:\n",
    "        col_lv = f\"{p}_log_vol\"\n",
    "        if col_lv not in full_df.columns:\n",
    "            continue\n",
    "\n",
    "        lv = full_df[col_lv].astype(\"float64\")\n",
    "\n",
    "        lv_m_s = lv.rolling(W_SHORT, min_periods=W_SHORT).mean()\n",
    "        lv_s_s = lv.rolling(W_SHORT, min_periods=W_SHORT).std()\n",
    "\n",
    "        lv_m_l = lv.rolling(W_LONG, min_periods=W_LONG).mean()\n",
    "        lv_s_l = lv.rolling(W_LONG, min_periods=W_LONG).std()\n",
    "\n",
    "        new_cols[f\"{p}_log_vol_mean_{W_SHORT}\"] = lv_m_s\n",
    "        new_cols[f\"{p}_log_vol_std_{W_SHORT}\"] = lv_s_s\n",
    "        new_cols[f\"{p}_log_vol_mean_{W_LONG}\"] = lv_m_l\n",
    "        new_cols[f\"{p}_log_vol_std_{W_LONG}\"] = lv_s_l\n",
    "        new_cols[f\"{p}_log_vol_z_{W_LONG}\"] = (lv - lv_m_l) / (lv_s_l + eps)\n",
    "\n",
    "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
    "roll_df = pd.DataFrame(new_cols, index=full_df.index)\n",
    "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
    "if existing_new_cols:\n",
    "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
    "    full_df = full_df.drop(columns=existing_new_cols)\n",
    "full_df = pd.concat([full_df, roll_df], axis=1)\n",
    "\n",
    "cols_after = set(full_df.columns)\n",
    "added = sorted(cols_after - cols_before)\n",
    "\n",
    "print(\n",
    "    f\"\\n[OK] BLOCK 7 added {len(added)} rolling-stat columns \"\n",
    "    f\"(W_SHORT={W_SHORT}, W_LONG={W_LONG}, volume_rolling={DO_VOLUME_ROLLING}).\"\n",
    ")\n",
    "print(\"[INFO] Sample added cols (first 40):\")\n",
    "for c in added[:40]:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(15)\n",
    "print(\"\\n[CHECK] NaNs in NEW rolling features (top 15):\")\n",
    "print(nan_top)\n",
    "\n",
    "print(\"[OK] BLOCK 7 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9341c74",
   "metadata": {},
   "source": [
    "## BLOCK 8 \u2014 CROSS-ASSET RELATIONSHIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "BASE = str(RUN_PARAMS[\"features\"][\"cross_asset_base\"])\n",
    "PEERS = list(RUN_PARAMS[\"features\"][\"cross_asset_peers\"])\n",
    "WINDOWS = list(RUN_PARAMS[\"features\"][\"cross_asset_windows\"])\n",
    "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
    "\n",
    "# Preconditions\n",
    "base_col = f\"{BASE}_logret_cc\"\n",
    "assert base_col in full_df.columns, f\"[ERROR] Missing base return column: {base_col}\"\n",
    "\n",
    "for p in PEERS:\n",
    "    col = f\"{p}_logret_cc\"\n",
    "    assert col in full_df.columns, f\"[ERROR] Missing peer return column: {col}\"\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "\n",
    "# Rolling correlation & beta\n",
    "new_cols = {}\n",
    "r_base = full_df[base_col].astype(\"float64\")\n",
    "\n",
    "for p in PEERS:\n",
    "    r_peer = full_df[f\"{p}_logret_cc\"].astype(\"float64\")\n",
    "\n",
    "    for w in WINDOWS:\n",
    "        w = int(w)\n",
    "        new_cols[f\"{BASE}_corr_{p}_{w}\"] = r_base.rolling(w, min_periods=w).corr(r_peer)\n",
    "\n",
    "        cov = r_base.rolling(w, min_periods=w).cov(r_peer)\n",
    "        var = r_peer.rolling(w, min_periods=w).var()\n",
    "        new_cols[f\"{BASE}_beta_{p}_{w}\"] = cov / (var + eps)\n",
    "\n",
    "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
    "cross_df = pd.DataFrame(new_cols, index=full_df.index)\n",
    "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
    "if existing_new_cols:\n",
    "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
    "    full_df = full_df.drop(columns=existing_new_cols)\n",
    "full_df = pd.concat([full_df, cross_df], axis=1)\n",
    "\n",
    "added = sorted(set(full_df.columns) - cols_before)\n",
    "\n",
    "print(f\"\\n[OK] BLOCK 8 added {len(added)} cross-asset rolling columns (BASE={BASE}, windows={WINDOWS}).\")\n",
    "print(\"[INFO] Added columns:\")\n",
    "for c in added:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\n[CHECK] NaNs in NEW cross-asset features (top 10):\")\n",
    "print(nan_top)\n",
    "\n",
    "print(\"[OK] BLOCK 8 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f7c58",
   "metadata": {},
   "source": [
    "## BLOCK 9 \u2014 REGIME & INTERACTION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "BASE = str(RUN_PARAMS[\"features\"][\"regime_base\"])\n",
    "W_SHORT = int(RUN_PARAMS[\"features\"][\"rolling_w_short\"])\n",
    "W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
    "PEERS = list(RUN_PARAMS[\"features\"][\"cross_asset_peers\"])\n",
    "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "new_cols = {}\n",
    "\n",
    "# Preconditions for needed rolling columns\n",
    "req_cols = [\n",
    "    f\"{BASE}_logret_cc_std_{W_SHORT}\",\n",
    "    f\"{BASE}_logret_cc_std_{W_LONG}\",\n",
    "    f\"{BASE}_logret_cc_z_{W_LONG}\",\n",
    "    f\"{BASE}_abs_logret_cc\",\n",
    "    f\"{BASE}_abs_logret_cc_mean_{W_LONG}\",\n",
    "    f\"{BASE}_abs_logret_cc_std_{W_LONG}\",\n",
    "    f\"{BASE}_close_pos_hl\",\n",
    "    f\"{BASE}_log_hl_z_{W_LONG}\",\n",
    "    f\"{BASE}_logret_cc_mean_{W_LONG}\",\n",
    "]\n",
    "missing_req = [c for c in req_cols if c not in full_df.columns]\n",
    "assert len(missing_req) == 0, f\"[ERROR] Missing required columns for regime layer: {missing_req}\"\n",
    "\n",
    "# Volatility / Regime (BASE)\n",
    "std_s = full_df[f\"{BASE}_logret_cc_std_{W_SHORT}\"].astype(\"float64\")\n",
    "std_l = full_df[f\"{BASE}_logret_cc_std_{W_LONG}\"].astype(\"float64\")\n",
    "\n",
    "new_cols[f\"{BASE}_vol_ratio_{W_SHORT}_{W_LONG}\"] = std_s / (std_l + eps)\n",
    "new_cols[f\"{BASE}_vol_diff_{W_SHORT}_{W_LONG}\"] = std_s - std_l\n",
    "\n",
    "z_ret = full_df[f\"{BASE}_logret_cc_z_{W_LONG}\"].astype(\"float64\")\n",
    "new_cols[f\"{BASE}_vol_regime_score\"] = z_ret * (std_s / (std_l + eps))\n",
    "\n",
    "# Price Structure (BASE)\n",
    "abs_ret = full_df[f\"{BASE}_abs_logret_cc\"].astype(\"float64\")\n",
    "abs_ret_mean_l = full_df[f\"{BASE}_abs_logret_cc_mean_{W_LONG}\"].astype(\"float64\")\n",
    "abs_ret_std_l = full_df[f\"{BASE}_abs_logret_cc_std_{W_LONG}\"].astype(\"float64\")\n",
    "abs_ret_z = (abs_ret - abs_ret_mean_l) / (abs_ret_std_l + eps)\n",
    "\n",
    "new_cols[f\"{BASE}_price_struct_1\"] = (\n",
    "    full_df[f\"{BASE}_close_pos_hl\"].astype(\"float64\") * abs_ret_z\n",
    ")\n",
    "\n",
    "new_cols[f\"{BASE}_price_struct_2\"] = (\n",
    "    full_df[f\"{BASE}_log_hl_z_{W_LONG}\"].astype(\"float64\") *\n",
    "    (std_s / (std_l + eps))\n",
    ")\n",
    "\n",
    "# Market Context (BASE vs peers)\n",
    "for p in PEERS:\n",
    "    beta_col = f\"{BASE}_beta_{p}_{W_LONG}\"\n",
    "    corr_col = f\"{BASE}_corr_{p}_{W_LONG}\"\n",
    "\n",
    "    if beta_col in full_df.columns:\n",
    "        new_cols[f\"{BASE}_ctx_beta_vol_{p}\"] = full_df[beta_col].astype(\"float64\") * std_l\n",
    "\n",
    "    if corr_col in full_df.columns:\n",
    "        new_cols[f\"{BASE}_ctx_corr_volratio_{p}\"] = (\n",
    "            full_df[corr_col].astype(\"float64\") * (std_s / (std_l + eps))\n",
    "        )\n",
    "\n",
    "# Macro context (event-aware)\n",
    "trend_l = full_df[f\"{BASE}_logret_cc_mean_{W_LONG}\"].astype(\"float64\")\n",
    "\n",
    "# VIX level \u00d7 volatility (daily series)\n",
    "if \"^VIX_Close\" in full_df.columns:\n",
    "    new_cols[f\"{BASE}_vix_vol_interact\"] = full_df[\"^VIX_Close\"].astype(\"float64\") * std_l\n",
    "\n",
    "# FEDFUNDS delta \u00d7 trend, ONLY on release day\n",
    "if \"FEDFUNDS_delta_mom\" in full_df.columns and \"FEDFUNDS_release_day\" in full_df.columns:\n",
    "    new_cols[f\"{BASE}_fedfunds_trend_interact\"] = (\n",
    "        full_df[\"FEDFUNDS_delta_mom\"].astype(\"float64\") *\n",
    "        full_df[\"FEDFUNDS_release_day\"].astype(\"float64\") *\n",
    "        trend_l\n",
    "    )\n",
    "\n",
    "# CPI pct_mom \u00d7 trend, ONLY on release day\n",
    "if \"CPI_pct_mom\" in full_df.columns and \"CPI_release_day\" in full_df.columns:\n",
    "    new_cols[f\"{BASE}_cpi_trend_interact\"] = (\n",
    "        full_df[\"CPI_pct_mom\"].astype(\"float64\") *\n",
    "        full_df[\"CPI_release_day\"].astype(\"float64\") *\n",
    "        trend_l\n",
    "    )\n",
    "\n",
    "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
    "inter_df = pd.DataFrame(new_cols, index=full_df.index)\n",
    "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
    "if existing_new_cols:\n",
    "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
    "    full_df = full_df.drop(columns=existing_new_cols)\n",
    "full_df = pd.concat([full_df, inter_df], axis=1)\n",
    "\n",
    "added = sorted(set(full_df.columns) - cols_before)\n",
    "\n",
    "print(f\"\\n[OK] BLOCK 9 added {len(added)} regime/interaction columns (macro now event-aware).\")\n",
    "print(\"[INFO] Added columns:\")\n",
    "for c in added:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\n[CHECK] NaNs in NEW Block-9 features (top 10):\")\n",
    "print(nan_top)\n",
    "\n",
    "print(\"[OK] BLOCK 9 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b6149",
   "metadata": {},
   "source": [
    "## BLOCK 10 \u2014 VIX/TNX SPECIAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = full_df.sort_index()\n",
    "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "new_cols = {}\n",
    "\n",
    "\n",
    "# Preconditions\n",
    "def _need_cols(cols):\n",
    "    \"\"\"Check required columns exist.\"\"\"\n",
    "    missing = [c for c in cols if c not in full_df.columns]\n",
    "    assert len(missing) == 0, f\"[ERROR] Missing required columns: {missing}\"\n",
    "\n",
    "\n",
    "_need_cols([\n",
    "    \"^VIX_Open\", \"^VIX_High\", \"^VIX_Low\", \"^VIX_Close\",\n",
    "    \"^TNX_Open\", \"^TNX_High\", \"^TNX_Low\", \"^TNX_Close\"\n",
    "])\n",
    "\n",
    "\n",
    "def _log_ratio(num, den):\n",
    "    \"\"\"Safe log ratio.\"\"\"\n",
    "    return np.log((num + eps) / (den + eps))\n",
    "\n",
    "\n",
    "# VIX features\n",
    "vix_o = full_df[\"^VIX_Open\"].astype(\"float64\")\n",
    "vix_h = full_df[\"^VIX_High\"].astype(\"float64\")\n",
    "vix_l = full_df[\"^VIX_Low\"].astype(\"float64\")\n",
    "vix_c = full_df[\"^VIX_Close\"].astype(\"float64\")\n",
    "\n",
    "new_cols[\"VIX_log_level\"] = np.log(vix_c + eps)\n",
    "new_cols[\"VIX_delta_1d\"] = vix_c.diff(1)\n",
    "new_cols[\"VIX_abs_delta_1d\"] = new_cols[\"VIX_delta_1d\"].abs()\n",
    "new_cols[\"VIX_log_hl\"] = _log_ratio(vix_h, vix_l)\n",
    "new_cols[\"VIX_range_frac\"] = (vix_h - vix_l) / (vix_c.abs() + eps)\n",
    "new_cols[\"VIX_gap_oc\"] = _log_ratio(vix_c, vix_o)\n",
    "\n",
    "for lag in [1, 5, 21]:\n",
    "    new_cols[f\"VIX_delta_1d_lag{lag}\"] = new_cols[\"VIX_delta_1d\"].shift(lag)\n",
    "    new_cols[f\"VIX_abs_delta_1d_lag{lag}\"] = new_cols[\"VIX_abs_delta_1d\"].shift(lag)\n",
    "\n",
    "# TNX features\n",
    "tnx_o = full_df[\"^TNX_Open\"].astype(\"float64\")\n",
    "tnx_h = full_df[\"^TNX_High\"].astype(\"float64\")\n",
    "tnx_l = full_df[\"^TNX_Low\"].astype(\"float64\")\n",
    "tnx_c = full_df[\"^TNX_Close\"].astype(\"float64\")\n",
    "\n",
    "new_cols[\"TNX_level\"] = tnx_c\n",
    "new_cols[\"TNX_delta_1d\"] = tnx_c.diff(1)\n",
    "new_cols[\"TNX_abs_delta_1d\"] = new_cols[\"TNX_delta_1d\"].abs()\n",
    "new_cols[\"TNX_delta_5d\"] = tnx_c.diff(5)\n",
    "new_cols[\"TNX_log_hl\"] = _log_ratio(tnx_h, tnx_l)\n",
    "new_cols[\"TNX_range\"] = (tnx_h - tnx_l)\n",
    "new_cols[\"TNX_gap_oc\"] = _log_ratio(tnx_c, tnx_o)\n",
    "\n",
    "for lag in [1, 5, 21]:\n",
    "    new_cols[f\"TNX_delta_1d_lag{lag}\"] = new_cols[\"TNX_delta_1d\"].shift(lag)\n",
    "    new_cols[f\"TNX_abs_delta_1d_lag{lag}\"] = new_cols[\"TNX_abs_delta_1d\"].shift(lag)\n",
    "\n",
    "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
    "vix_tnx_df = pd.DataFrame(new_cols, index=full_df.index)\n",
    "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
    "if existing_new_cols:\n",
    "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
    "    full_df = full_df.drop(columns=existing_new_cols)\n",
    "full_df = pd.concat([full_df, vix_tnx_df], axis=1)\n",
    "\n",
    "added = sorted(set(full_df.columns) - cols_before)\n",
    "\n",
    "print(f\"\\n[OK] BLOCK 10 added {len(added)} special VIX/TNX columns.\")\n",
    "print(\"[INFO] Added columns:\")\n",
    "for c in added:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(12)\n",
    "print(\"\\n[CHECK] NaNs in NEW VIX/TNX features (top 12):\")\n",
    "print(nan_top)\n",
    "\n",
    "print(\"[OK] BLOCK 10 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4e057",
   "metadata": {},
   "source": [
    "## BLOCK 11 \u2014 EVENTS & SPARSE SIGNALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = full_df.sort_index()\n",
    "eps = float(RUN_PARAMS[\"features\"][\"eps\"])\n",
    "\n",
    "# Get params from RUN_PARAMS\n",
    "BASE = str(RUN_PARAMS[\"features\"][\"regime_base\"])\n",
    "W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
    "MARKET_VOL_TICKER = str(RUN_PARAMS[\"features\"][\"market_vol_ticker\"])\n",
    "\n",
    "BASE_VOL_COL = f\"{BASE}_logret_cc_std_{W_LONG}\"\n",
    "MARKET_VOL_COL = f\"{MARKET_VOL_TICKER}_logret_cc_std_{W_LONG}\"\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "new_cols = {}\n",
    "\n",
    "# Preconditions\n",
    "req = [\n",
    "    \"is_earnings_day\",\n",
    "    \"eps_surprise_pct_yahoo\", \"has_eps_surprise_yahoo\",\n",
    "    \"eps_surprise_pct_calc\", \"has_eps_surprise_calc\",\n",
    "    \"CPI_pct_mom\", \"CPI_release_day\",\n",
    "    \"FEDFUNDS_delta_mom\", \"FEDFUNDS_release_day\",\n",
    "]\n",
    "missing = [c for c in req if c not in full_df.columns]\n",
    "assert len(missing) == 0, f\"[ERROR] Missing required columns: {missing}\"\n",
    "assert BASE_VOL_COL in full_df.columns, f\"[ERROR] missing {BASE_VOL_COL}\"\n",
    "assert MARKET_VOL_COL in full_df.columns, f\"[ERROR] missing {MARKET_VOL_COL}\"\n",
    "\n",
    "\n",
    "# Helper: event-based previous release (lag1/lag2) mapped to daily index\n",
    "def prev_event_value_to_daily(\n",
    "    df: pd.DataFrame,\n",
    "    value_col: str,\n",
    "    release_flag_col: str,\n",
    "    lag_k: int = 1,\n",
    "    fill_before_first: float = 0.0,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Returns a daily series where each day carries the value from the previous (lag_k) RELEASE EVENT.\"\"\"\n",
    "    events = df.loc[df[release_flag_col] == 1, value_col].astype(\"float64\").copy()\n",
    "    shifted = events.shift(lag_k)\n",
    "\n",
    "    out = pd.Series(index=df.index, dtype=\"float64\")\n",
    "    out.loc[shifted.index] = shifted.values\n",
    "    out = out.ffill().fillna(fill_before_first)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Earnings: EPS surprise lags by EARNINGS EVENTS (lag1/lag2 = last/prev earnings)\n",
    "def eps_event_lags_to_daily(df: pd.DataFrame, val_col: str, flag_col: str, lag_k: int) -> pd.Series:\n",
    "    \"\"\"Event-based lag for EPS: values only when has_eps_surprise_* == 1, shift by EVENTS.\"\"\"\n",
    "    event_series = df.loc[df[flag_col] == 1, val_col].astype(\"float64\").copy()\n",
    "    shifted = event_series.shift(lag_k)\n",
    "\n",
    "    out = pd.Series(index=df.index, dtype=\"float64\")\n",
    "    out.loc[shifted.index] = shifted.values\n",
    "    out = out.ffill().fillna(0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "for src in [\"yahoo\", \"calc\"]:\n",
    "    val_col = f\"eps_surprise_pct_{src}\"\n",
    "    flag_col = f\"has_eps_surprise_{src}\"\n",
    "    new_cols[f\"eps_surprise_{src}_lag1\"] = eps_event_lags_to_daily(full_df, val_col, flag_col, lag_k=1)\n",
    "    new_cols[f\"eps_surprise_{src}_lag2\"] = eps_event_lags_to_daily(full_df, val_col, flag_col, lag_k=2)\n",
    "\n",
    "# post-earnings day 1..5 dummies\n",
    "earn = full_df[\"is_earnings_day\"].astype(\"int8\")\n",
    "for k in range(1, 6):\n",
    "    new_cols[f\"post_earnings_day_{k}\"] = earn.shift(k).fillna(0).astype(\"int8\")\n",
    "\n",
    "# EPS flag \u00d7 vol\n",
    "vol_base = full_df[BASE_VOL_COL].astype(\"float64\")\n",
    "new_cols[\"eps_flag_yahoo_x_vol\"] = full_df[\"has_eps_surprise_yahoo\"].astype(\"float64\") * vol_base\n",
    "new_cols[\"eps_flag_calc_x_vol\"] = full_df[\"has_eps_surprise_calc\"].astype(\"float64\") * vol_base\n",
    "\n",
    "# Macro: CPI (impulse + previous release)\n",
    "cpi_release = full_df[\"CPI_release_day\"].astype(\"int8\")\n",
    "cpi_val = full_df[\"CPI_pct_mom\"].astype(\"float64\")\n",
    "\n",
    "# impulse only on release day\n",
    "new_cols[\"CPI_impulse\"] = cpi_val * cpi_release.astype(\"float64\")\n",
    "\n",
    "# previous release value carried forward (event-lag1)\n",
    "new_cols[\"CPI_prev_release\"] = prev_event_value_to_daily(\n",
    "    full_df, value_col=\"CPI_pct_mom\", release_flag_col=\"CPI_release_day\", lag_k=1, fill_before_first=0.0\n",
    ")\n",
    "\n",
    "# change vs previous release, only meaningful on release days\n",
    "new_cols[\"CPI_change_prev_release\"] = (cpi_val - new_cols[\"CPI_prev_release\"]) * cpi_release.astype(\"float64\")\n",
    "\n",
    "# Macro: FEDFUNDS (impulse + previous decision delta)\n",
    "ff_release = full_df[\"FEDFUNDS_release_day\"].astype(\"int8\")\n",
    "ff_delta = full_df[\"FEDFUNDS_delta_mom\"].astype(\"float64\")\n",
    "\n",
    "new_cols[\"FEDFUNDS_impulse\"] = ff_delta * ff_release.astype(\"float64\")\n",
    "\n",
    "new_cols[\"FEDFUNDS_prev_delta\"] = prev_event_value_to_daily(\n",
    "    full_df, value_col=\"FEDFUNDS_delta_mom\", release_flag_col=\"FEDFUNDS_release_day\", lag_k=1, fill_before_first=0.0\n",
    ")\n",
    "\n",
    "# release-day \u00d7 market vol\n",
    "mkt_vol = full_df[MARKET_VOL_COL].astype(\"float64\")\n",
    "new_cols[\"FEDFUNDS_release_day_x_mkt_vol\"] = ff_release.astype(\"float64\") * mkt_vol\n",
    "\n",
    "# Attach + diagnostics (remove existing to avoid duplicates on re-run)\n",
    "blk11_df = pd.DataFrame(new_cols, index=full_df.index)\n",
    "existing_new_cols = [c for c in new_cols.keys() if c in full_df.columns]\n",
    "if existing_new_cols:\n",
    "    print(f\"[INFO] Removing {len(existing_new_cols)} existing cols from full_df before merge\")\n",
    "    full_df = full_df.drop(columns=existing_new_cols)\n",
    "full_df = pd.concat([full_df, blk11_df], axis=1)\n",
    "\n",
    "added = sorted(set(full_df.columns) - cols_before)\n",
    "\n",
    "print(f\"\\n[OK] BLOCK 11 added {len(added)} event/sparse-signal columns.\")\n",
    "print(\"[INFO] Added columns:\")\n",
    "for c in added:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "nan_top = full_df[added].isna().sum().sort_values(ascending=False).head(12)\n",
    "print(\"\\n[CHECK] NaNs in NEW Block-11 features (top 12):\")\n",
    "print(nan_top)\n",
    "\n",
    "# Sanity: impulse should be non-zero only on release days\n",
    "for col_imp, flag in [(\"CPI_impulse\", \"CPI_release_day\"), (\"FEDFUNDS_impulse\", \"FEDFUNDS_release_day\")]:\n",
    "    if col_imp in full_df.columns and flag in full_df.columns:\n",
    "        nz_days = int((full_df[col_imp].abs() > 0).sum())\n",
    "        flag_days = int(full_df[flag].sum())\n",
    "        print(f\"\\n[CHECK] {col_imp}: nonzero_days={nz_days} | {flag} count={flag_days}\")\n",
    "\n",
    "print(\"[OK] BLOCK 11 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e7273",
   "metadata": {},
   "source": [
    "## BLOCK 12 \u2014 CRISIS PERIOD FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fd6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "cols_before = set(full_df.columns)\n",
    "\n",
    "# Date windows from RUN_PARAMS\n",
    "covid_start = pd.Timestamp(RUN_PARAMS[\"features\"][\"covid_start\"])\n",
    "covid_end = pd.Timestamp(RUN_PARAMS[\"features\"][\"covid_end\"])\n",
    "\n",
    "crisis_2008_start = pd.Timestamp(RUN_PARAMS[\"features\"][\"crisis_2008_start\"])\n",
    "crisis_2008_end = pd.Timestamp(RUN_PARAMS[\"features\"][\"crisis_2008_end\"])\n",
    "\n",
    "# Build flags (overwrite-safe)\n",
    "full_df[\"covid_period\"] = ((full_df.index >= covid_start) & (full_df.index <= covid_end)).astype(\"int8\")\n",
    "full_df[\"pre_covid\"] = (full_df.index < covid_start).astype(\"int8\")\n",
    "full_df[\"post_covid\"] = (full_df.index > covid_end).astype(\"int8\")\n",
    "\n",
    "full_df[\"crisis_2008\"] = ((full_df.index >= crisis_2008_start) & (full_df.index <= crisis_2008_end)).astype(\"int8\")\n",
    "full_df[\"pre_crisis_2008\"] = (full_df.index < crisis_2008_start).astype(\"int8\")\n",
    "full_df[\"post_crisis_2008\"] = (full_df.index > crisis_2008_end).astype(\"int8\")\n",
    "\n",
    "# Sanity checks: mutually exclusive within each regime triad (pre / in / post)\n",
    "bad_covid = int(((full_df[\"pre_covid\"] + full_df[\"covid_period\"] + full_df[\"post_covid\"]) != 1).sum())\n",
    "bad_2008 = int(((full_df[\"pre_crisis_2008\"] + full_df[\"crisis_2008\"] + full_df[\"post_crisis_2008\"]) != 1).sum())\n",
    "assert bad_covid == 0, \"[ERROR] COVID flags are not mutually exclusive.\"\n",
    "assert bad_2008 == 0, \"[ERROR] Crisis flags are not mutually exclusive.\"\n",
    "\n",
    "cols_after = set(full_df.columns)\n",
    "added = sorted(cols_after - cols_before)\n",
    "\n",
    "counts = {\n",
    "    \"pre_covid\": int(full_df[\"pre_covid\"].sum()),\n",
    "    \"covid_period\": int(full_df[\"covid_period\"].sum()),\n",
    "    \"post_covid\": int(full_df[\"post_covid\"].sum()),\n",
    "    \"pre_crisis_2008\": int(full_df[\"pre_crisis_2008\"].sum()),\n",
    "    \"crisis_2008\": int(full_df[\"crisis_2008\"].sum()),\n",
    "    \"post_crisis_2008\": int(full_df[\"post_crisis_2008\"].sum()),\n",
    "}\n",
    "\n",
    "print(f\"[OK] BLOCK 12 crisis flags added/updated. Newly added cols: {len(added)}\")\n",
    "print(\"[INFO] Counts:\", counts)\n",
    "\n",
    "print(\"[OK] BLOCK 12 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2553698",
   "metadata": {},
   "source": [
    "## BLOCK 13 \u2014 DEFINE TARGET (NEXT-DAY LOG RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "SRC_COL = str(RUN_PARAMS[\"data\"][\"target_src_col\"])\n",
    "\n",
    "assert SRC_COL in full_df.columns, f\"[ERROR] Missing {SRC_COL}.\"\n",
    "\n",
    "# Deterministic overwrite on rerun\n",
    "if TARGET_T1 in full_df.columns:\n",
    "    print(f\"[WARN] {TARGET_T1} already exists \u2014 overwriting it deterministically (shift(-1) of {SRC_COL}).\")\n",
    "\n",
    "full_df[TARGET_T1] = full_df[SRC_COL].shift(-1).astype(\"float64\")\n",
    "\n",
    "# Sanity: expect exactly 1 NaN at the end\n",
    "n_nan = int(full_df[TARGET_T1].isna().sum())\n",
    "print(\"[INFO] Target NaNs:\", n_nan, \"out of\", len(full_df))\n",
    "assert n_nan == 1, f\"[ERROR] Expected exactly 1 NaN in {TARGET_T1} (last row). Found {n_nan}.\"\n",
    "\n",
    "print(\"[OK] Target defined:\", TARGET_T1)\n",
    "print(\"[INFO] Target tail preview (before drop):\")\n",
    "print(full_df[[TARGET_T1]].tail(3))\n",
    "\n",
    "# Drop last row (NaN target)\n",
    "before = len(full_df)\n",
    "full_df = full_df.dropna(subset=[TARGET_T1]).copy()\n",
    "after = len(full_df)\n",
    "\n",
    "print(f\"[OK] Dropped last row with NaN target: {before} -> {after}\")\n",
    "print(\"[INFO] New index range:\", full_df.index.min(), \"->\", full_df.index.max())\n",
    "print(\"[INFO] Target tail preview (after drop):\")\n",
    "print(full_df[[TARGET_T1]].tail(3))\n",
    "\n",
    "# Persist target metadata into RUN_PARAMS\n",
    "RUN_PARAMS.setdefault(\"data\", {})\n",
    "RUN_PARAMS[\"data\"][\"target_src_col\"] = SRC_COL\n",
    "RUN_PARAMS[\"data\"][\"target_col\"] = TARGET_T1\n",
    "\n",
    "# Save target metadata\n",
    "meta = {\n",
    "    \"target_src_col\": SRC_COL,\n",
    "    \"target_col\": TARGET_T1,\n",
    "    \"rows_before_drop\": int(before),\n",
    "    \"rows_after_drop\": int(after),\n",
    "    \"min_date\": str(full_df.index.min()),\n",
    "    \"max_date\": str(full_df.index.max()),\n",
    "}\n",
    "save_json(meta, OUTPUTS_DIR / \"target_meta.json\")\n",
    "save_json(meta, DRIVE_PATHS[\"outputs_dir\"] / \"target_meta.json\")\n",
    "\n",
    "print(\"[OK] BLOCK 13 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513f150",
   "metadata": {},
   "source": [
    "## BLOCK 14 \u2014 DROP DUPLICATE FEATURE COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdf2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
    "\n",
    "dup_mask = full_df.columns.duplicated(keep=\"last\")\n",
    "dup_cols = full_df.columns[dup_mask].tolist()\n",
    "\n",
    "if len(dup_cols) == 0:\n",
    "    print(\"[OK] No duplicate column names found. Nothing to drop.\")\n",
    "else:\n",
    "    before_shape = full_df.shape\n",
    "    full_df = full_df.loc[:, ~dup_mask].copy()\n",
    "    after_shape = full_df.shape\n",
    "\n",
    "    print(f\"[WARN] Dropped {len(dup_cols)} duplicate columns (keep_last).\")\n",
    "    print(\"[INFO] Shape:\", before_shape, \"->\", after_shape)\n",
    "    print(\"[INFO] Example dropped duplicates (first 30):\", dup_cols[:30])\n",
    "\n",
    "print(\"[OK] BLOCK 14 complete. full_df shape:\", full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d31044",
   "metadata": {},
   "source": [
    "## BLOCK 15 \u2014 LIMIT PERIOD + SAVE SNAPSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2937331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "full_df = full_df.sort_index()\n",
    "\n",
    "# Limit start date from RUN_PARAMS\n",
    "limit_start = str(RUN_PARAMS[\"data\"][\"limit_start_date\"])\n",
    "full_df = full_df.loc[limit_start:].copy()\n",
    "\n",
    "# Save to interim\n",
    "INTERIM_DIR_LOCAL = DATA_DIRS_LOCAL[\"interim\"]\n",
    "INTERIM_DIR_DRIVE = DATA_DIRS_DRIVE[\"interim\"]\n",
    "\n",
    "out_local = INTERIM_DIR_LOCAL / \"full_df.pkl\"\n",
    "out_drive = INTERIM_DIR_DRIVE / \"full_df.pkl\"\n",
    "\n",
    "full_df.to_pickle(out_local)\n",
    "copy_file(out_local, out_drive)\n",
    "\n",
    "print(\"[OK] Saved full_df snapshot to:\")\n",
    "print(\"  - local:\", out_local)\n",
    "print(\"  - drive:\", out_drive)\n",
    "print(\"[INFO] full_df rows:\", len(full_df), \"| range:\", full_df.index.min(), \"->\", full_df.index.max())\n",
    "\n",
    "print(\"[OK] BLOCK 15 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984abc2",
   "metadata": {},
   "source": [
    "## BLOCK 16 \u2014 FULL COMPLETENESS SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b459f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_full_info_with_missing(df: pd.DataFrame, title: str = \"DATAFRAME INFO\") -> pd.DataFrame:\n",
    "    \"\"\"Print full column-wise completeness table.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[{title}]\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "    print(f\"Total rows: {n_rows:,}\")\n",
    "    print(f\"Total cols: {n_cols:,}\")\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(f\"Index range: {df.index.min()} -> {df.index.max()}\")\n",
    "\n",
    "    missing = df.isna().sum()\n",
    "    non_null = n_rows - missing\n",
    "    dtypes = df.dtypes.astype(str)\n",
    "\n",
    "    summary = (\n",
    "        pd.DataFrame({\n",
    "            \"dtype\": dtypes,\n",
    "            \"non_null\": non_null,\n",
    "            \"missing\": missing,\n",
    "            \"missing_%\": (missing / max(n_rows, 1) * 100).round(3),\n",
    "        })\n",
    "        .sort_values([\"missing\", \"missing_%\"], ascending=False)\n",
    "    )\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\", None,\n",
    "        \"display.max_columns\", None,\n",
    "        \"display.width\", 220,\n",
    "        \"display.max_colwidth\", 60\n",
    "    ):\n",
    "        print(\"\\n[Column-wise completeness]\")\n",
    "        print(summary)\n",
    "\n",
    "    print(\"\\n[Top 20 columns by missing]\")\n",
    "    print(summary.head(20))\n",
    "\n",
    "    total_missing = int(missing.sum())\n",
    "    total_cells = int(n_rows * n_cols)\n",
    "    print(\"\\n[Overall missing]\")\n",
    "    print(\n",
    "        f\"Total missing cells: {total_missing:,} / {total_cells:,} \"\n",
    "        f\"({(total_missing / max(total_cells, 1) * 100):.4f}%)\"\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "summary = print_full_info_with_missing(full_df, title=\"full_df (after feature blocks + time limit)\")\n",
    "\n",
    "# Save artifacts\n",
    "FEATURE_LIST_DIR_LOCAL = OUTPUTS_DIR / \"feature_lists\"\n",
    "FEATURE_LIST_DIR_DRIVE = DRIVE_PATHS[\"outputs_dir\"] / \"feature_lists\"\n",
    "ensure_dir(FEATURE_LIST_DIR_LOCAL)\n",
    "ensure_dir(FEATURE_LIST_DIR_DRIVE)\n",
    "\n",
    "feature_list = list(full_df.columns)\n",
    "\n",
    "# Feature lists\n",
    "features_txt_local = FEATURE_LIST_DIR_LOCAL / \"feature_list_all_columns.txt\"\n",
    "features_pkl_local = FEATURE_LIST_DIR_LOCAL / \"feature_list_all_columns.pkl\"\n",
    "features_csv_local = FEATURE_LIST_DIR_LOCAL / \"feature_list_all_columns.csv\"\n",
    "\n",
    "features_txt_local.write_text(\"\\n\".join(feature_list), encoding=\"utf-8\")\n",
    "save_pickle(feature_list, features_pkl_local)\n",
    "pd.DataFrame({\"feature\": feature_list}).to_csv(features_csv_local, index=False)\n",
    "\n",
    "copy_file(features_txt_local, FEATURE_LIST_DIR_DRIVE / features_txt_local.name)\n",
    "copy_file(features_pkl_local, FEATURE_LIST_DIR_DRIVE / features_pkl_local.name)\n",
    "copy_file(features_csv_local, FEATURE_LIST_DIR_DRIVE / features_csv_local.name)\n",
    "\n",
    "# Missingness summary\n",
    "missing_csv_local = FEATURE_LIST_DIR_LOCAL / \"missing_summary_all_columns.csv\"\n",
    "summary.reset_index(names=\"feature\").to_csv(missing_csv_local, index=False)\n",
    "copy_file(missing_csv_local, FEATURE_LIST_DIR_DRIVE / missing_csv_local.name)\n",
    "\n",
    "print(\"\\n[OK] Saved feature list + missing summary to:\")\n",
    "print(\"  - local :\", FEATURE_LIST_DIR_LOCAL)\n",
    "print(\"  - drive :\", FEATURE_LIST_DIR_DRIVE)\n",
    "\n",
    "print(\"[OK] BLOCK 16 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Data visualization and analysis**\n",
    "\n",
    "**Blocks:** 17-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c9844",
   "metadata": {},
   "source": [
    "## BLOCK 17 \u2014 EDA RETURNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if EDA is enabled\n",
    "if not RUN_PARAMS[\"eda\"][\"enabled\"]:\n",
    "    print(\"[SKIP] BLOCK 17 \u2014 EDA disabled in RUN_PARAMS.\")\n",
    "else:\n",
    "    # Guardrails\n",
    "    assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
    "    assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "\n",
    "    full_df = full_df.sort_index()\n",
    "\n",
    "    # EDA dirs inside RUN folders\n",
    "    EDA_PLOTS_LOCAL = LOCAL_PATHS[\"plots_dir\"] / \"eda_returns\"\n",
    "    EDA_REPS_LOCAL = LOCAL_PATHS[\"reports_dir\"] / \"eda_returns\"\n",
    "    EDA_PLOTS_DRIVE = DRIVE_PATHS[\"plots_dir\"] / \"eda_returns\"\n",
    "    EDA_REPS_DRIVE = DRIVE_PATHS[\"reports_dir\"] / \"eda_returns\"\n",
    "\n",
    "    for p in [EDA_PLOTS_LOCAL, EDA_REPS_LOCAL, EDA_PLOTS_DRIVE, EDA_REPS_DRIVE]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    STATS_FILE_LOCAL = EDA_REPS_LOCAL / \"eda_returns_stats.csv\"\n",
    "    STATS_FILE_DRIVE = EDA_REPS_DRIVE / \"eda_returns_stats.csv\"\n",
    "\n",
    "    # Get bins from RUN_PARAMS\n",
    "    EDA_BINS = int(RUN_PARAMS[\"eda\"][\"returns_bins\"])\n",
    "\n",
    "    def safe_name(s: str) -> str:\n",
    "        \"\"\"Convert ticker name to safe filename.\"\"\"\n",
    "        s = s.replace(\"^\", \"\")\n",
    "        return re.sub(r\"[^A-Za-z0-9\\-_]+\", \"_\", s)\n",
    "\n",
    "    def analyze_returns(series: pd.Series, ticker: str, bins: int = EDA_BINS) -> None:\n",
    "        \"\"\"Analyze returns distribution for a ticker.\"\"\"\n",
    "        data = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "        if data.empty:\n",
    "            print(f\"[WARN] {ticker}: empty after dropna(). Skipping.\")\n",
    "            return\n",
    "\n",
    "        fn = safe_name(ticker)\n",
    "\n",
    "        # Histogram\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(data.values, bins=bins)\n",
    "        plt.title(f\"Histogram of {ticker} Daily Log Returns\")\n",
    "        plt.xlabel(\"Log return\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_PLOTS_LOCAL / f\"{fn}_hist.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # Boxplot\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.boxplot(data.values, vert=False)\n",
    "        plt.title(f\"Boxplot of {ticker} Daily Log Returns\")\n",
    "        plt.xlabel(\"Log return\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_PLOTS_LOCAL / f\"{fn}_box.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # Stats CSV (append)\n",
    "        skew_val = float(stats.skew(data.values, bias=False))\n",
    "        kurt_val = float(stats.kurtosis(data.values, fisher=False, bias=False))\n",
    "\n",
    "        file_exists = STATS_FILE_LOCAL.exists()\n",
    "        with open(STATS_FILE_LOCAL, mode=\"a\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            if not file_exists:\n",
    "                w.writerow([\"run_id\", \"ticker\", \"col\", \"n\", \"skewness\", \"kurtosis_pearson\"])\n",
    "            w.writerow([RUN_ID, ticker, series.name, int(len(data)), skew_val, kurt_val])\n",
    "\n",
    "        print(f\"[OK] {ticker}: saved hist+box; skew={skew_val:.6f}, kurt={kurt_val:.6f}\")\n",
    "\n",
    "    # AUTO: find all *_logret_cc columns (exclude abs)\n",
    "    ret_cols = sorted([\n",
    "        c for c in full_df.columns\n",
    "        if c.endswith(\"_logret_cc\") and not c.endswith(\"_abs_logret_cc\")\n",
    "    ])\n",
    "    assert len(ret_cols) > 0, \"[ERROR] No *_logret_cc columns found.\"\n",
    "\n",
    "    print(f\"[INFO] Found {len(ret_cols)} return columns.\")\n",
    "\n",
    "    for col in ret_cols:\n",
    "        ticker = col.replace(\"_logret_cc\", \"\")\n",
    "        analyze_returns(full_df[col], ticker=ticker)\n",
    "\n",
    "    # Mirror plots + stats to DRIVE\n",
    "    for img in EDA_PLOTS_LOCAL.glob(\"*.png\"):\n",
    "        copy_file(img, EDA_PLOTS_DRIVE / img.name)\n",
    "\n",
    "    if STATS_FILE_LOCAL.exists():\n",
    "        copy_file(STATS_FILE_LOCAL, STATS_FILE_DRIVE)\n",
    "\n",
    "    print(\"\\n[OK] EDA Returns complete.\")\n",
    "    print(\"[INFO] LOCAL plots :\", EDA_PLOTS_LOCAL)\n",
    "    print(\"[INFO] DRIVE plots :\", EDA_PLOTS_DRIVE)\n",
    "    print(\"[INFO] LOCAL report:\", STATS_FILE_LOCAL)\n",
    "    print(\"[INFO] DRIVE report:\", STATS_FILE_DRIVE)\n",
    "\n",
    "print(\"[OK] BLOCK 17 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3736c",
   "metadata": {},
   "source": [
    "## BLOCK 18 \u2014 EDA VOLATILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878eb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if EDA is enabled\n",
    "if not RUN_PARAMS[\"eda\"][\"enabled\"]:\n",
    "    print(\"[SKIP] BLOCK 18 \u2014 EDA disabled in RUN_PARAMS.\")\n",
    "else:\n",
    "    # Guardrails\n",
    "    assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
    "    assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "    full_df = full_df.sort_index()\n",
    "\n",
    "    # EDA dirs inside RUN folders\n",
    "    EDA_PLOTS_LOCAL = LOCAL_PATHS[\"plots_dir\"] / \"eda_volatility\"\n",
    "    EDA_PLOTS_DRIVE = DRIVE_PATHS[\"plots_dir\"] / \"eda_volatility\"\n",
    "    EDA_PLOTS_LOCAL.mkdir(parents=True, exist_ok=True)\n",
    "    EDA_PLOTS_DRIVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _safe_fname(s: str) -> str:\n",
    "        \"\"\"Convert column name to safe filename.\"\"\"\n",
    "        s = s.replace(\"^\", \"\")\n",
    "        return re.sub(r\"[^A-Za-z0-9\\-_]+\", \"_\", s)\n",
    "\n",
    "    def plot_volatility_with_crisis_periods(\n",
    "        df: pd.DataFrame,\n",
    "        vol_col: str,\n",
    "        covid_col: str = \"covid_period\",\n",
    "        crisis_col: str = \"crisis_2008\",\n",
    "    ) -> None:\n",
    "        \"\"\"Plot volatility with COVID and 2008 crisis shading.\"\"\"\n",
    "        missing = [c for c in [vol_col, covid_col, crisis_col] if c not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"[SKIP] {vol_col}: missing columns {missing}\")\n",
    "            return\n",
    "\n",
    "        tmp = df[[vol_col, covid_col, crisis_col]].dropna(subset=[vol_col]).copy()\n",
    "        if tmp.empty:\n",
    "            print(f\"[SKIP] {vol_col}: empty after dropping NaN vol.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        (line_handle,) = plt.plot(tmp.index, tmp[vol_col].astype(\"float64\"), label=vol_col)\n",
    "\n",
    "        def shade_period(mask: pd.Series, label: str) -> Patch:\n",
    "            mask = mask.astype(bool).values\n",
    "            idx = tmp.index\n",
    "\n",
    "            in_seg = False\n",
    "            start = None\n",
    "            for d, flag in zip(idx, mask):\n",
    "                if flag and not in_seg:\n",
    "                    in_seg = True\n",
    "                    start = d\n",
    "                elif (not flag) and in_seg:\n",
    "                    plt.axvspan(start, d, alpha=0.2)\n",
    "                    in_seg = False\n",
    "\n",
    "            if in_seg:\n",
    "                plt.axvspan(start, idx[-1], alpha=0.2)\n",
    "\n",
    "            return Patch(alpha=0.2, label=label)\n",
    "\n",
    "        covid_patch = shade_period(tmp[covid_col] == 1, label=\"COVID period\")\n",
    "        crisis_patch = shade_period(tmp[crisis_col] == 1, label=\"2008 crisis\")\n",
    "\n",
    "        plt.title(f\"{vol_col} with COVID and 2008 crisis shading\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Volatility (std)\")\n",
    "        plt.legend(handles=[line_handle, covid_patch, crisis_patch], loc=\"upper right\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        save_path = EDA_PLOTS_LOCAL / f\"{_safe_fname(vol_col)}_with_covid_and_2008.png\"\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"[INFO] Saved (LOCAL): {save_path}\")\n",
    "\n",
    "        # Mirror to drive\n",
    "        copy_file(save_path, EDA_PLOTS_DRIVE / save_path.name)\n",
    "\n",
    "    # Run for all return-vol columns (*_logret_cc_std_21)\n",
    "    W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
    "    vol_cols = sorted([c for c in full_df.columns if c.endswith(f\"_logret_cc_std_{W_LONG}\")])\n",
    "\n",
    "    print(f\"[INFO] LOCAL EDA plots dir: {EDA_PLOTS_LOCAL}\")\n",
    "    print(f\"[INFO] DRIVE EDA plots dir: {EDA_PLOTS_DRIVE}\")\n",
    "    print(f\"[INFO] Found {len(vol_cols)} *_logret_cc_std_{W_LONG} columns.\")\n",
    "\n",
    "    if len(vol_cols) == 0:\n",
    "        print(f\"[WARN] No *_logret_cc_std_{W_LONG} columns found. Skipping volatility EDA.\")\n",
    "    else:\n",
    "        for col in vol_cols:\n",
    "            plot_volatility_with_crisis_periods(\n",
    "                full_df,\n",
    "                vol_col=col,\n",
    "                covid_col=\"covid_period\",\n",
    "                crisis_col=\"crisis_2008\",\n",
    "            )\n",
    "\n",
    "    print(\"\\n[OK] Volatility EDA complete.\")\n",
    "\n",
    "print(\"[OK] BLOCK 18 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caba4e4",
   "metadata": {},
   "source": [
    "## BLOCK 19 \u2014 EDA CATEGORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28044911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if EDA is enabled\n",
    "if not RUN_PARAMS[\"eda\"][\"enabled\"]:\n",
    "    print(\"[SKIP] BLOCK 19 \u2014 EDA disabled in RUN_PARAMS.\")\n",
    "else:\n",
    "    # Guardrails\n",
    "    assert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\n",
    "    assert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\n",
    "    full_df = full_df.sort_index()\n",
    "\n",
    "    # EDA dirs inside RUN folders\n",
    "    EDA_PLOTS_LOCAL = LOCAL_PATHS[\"plots_dir\"] / \"eda_categorical\"\n",
    "    EDA_REPS_LOCAL = LOCAL_PATHS[\"reports_dir\"] / \"eda_categorical\"\n",
    "    EDA_PLOTS_DRIVE = DRIVE_PATHS[\"plots_dir\"] / \"eda_categorical\"\n",
    "    EDA_REPS_DRIVE = DRIVE_PATHS[\"reports_dir\"] / \"eda_categorical\"\n",
    "\n",
    "    for p in [EDA_PLOTS_LOCAL, EDA_REPS_LOCAL, EDA_PLOTS_DRIVE, EDA_REPS_DRIVE]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _safe_fname(s: str) -> str:\n",
    "        \"\"\"Convert column name to safe filename.\"\"\"\n",
    "        s = s.replace(\"^\", \"\")\n",
    "        return re.sub(r\"[^A-Za-z0-9\\-_]+\", \"_\", s)\n",
    "\n",
    "    # Categorical/binary columns for EDA\n",
    "    categorical_cols = [\n",
    "        \"is_earnings_day\",\n",
    "        \"has_eps_surprise_yahoo\",\n",
    "        \"has_eps_surprise_calc\",\n",
    "        \"post_earnings_day_1\", \"post_earnings_day_2\", \"post_earnings_day_3\",\n",
    "        \"post_earnings_day_4\", \"post_earnings_day_5\",\n",
    "        \"CPI_release_day\",\n",
    "        \"FEDFUNDS_release_day\",\n",
    "        \"FEDFUNDS_changed\",\n",
    "        \"CPI_pct_mom_is_missing\",\n",
    "        \"CPI_accel_pct_mom_is_missing\",\n",
    "        \"FEDFUNDS_delta_mom_is_missing\",\n",
    "        \"is_q1\", \"is_q2\", \"is_q3\", \"is_q4\",\n",
    "        \"covid_period\",\n",
    "        \"crisis_2008\",\n",
    "        \"pre_covid\",\n",
    "        \"post_covid\",\n",
    "        \"pre_crisis_2008\",\n",
    "        \"post_crisis_2008\",\n",
    "    ]\n",
    "    categorical_cols = [c for c in categorical_cols if c in full_df.columns]\n",
    "\n",
    "    print(\"[INFO] Categorical/binary columns used for EDA:\")\n",
    "    print(categorical_cols)\n",
    "\n",
    "    # Save value counts to CSV\n",
    "    cat_counts_file_local = EDA_REPS_LOCAL / \"categorical_value_counts.csv\"\n",
    "    cat_counts_file_drive = EDA_REPS_DRIVE / \"categorical_value_counts.csv\"\n",
    "\n",
    "    with open(cat_counts_file_local, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"run_id\", \"variable\", \"value\", \"count\"])\n",
    "        for col in categorical_cols:\n",
    "            vc = full_df[col].value_counts(dropna=False)\n",
    "            for val, count in vc.items():\n",
    "                writer.writerow([RUN_ID, col, val, int(count)])\n",
    "\n",
    "    copy_file(cat_counts_file_local, cat_counts_file_drive)\n",
    "\n",
    "    print(\"[INFO] Saved categorical value counts to:\")\n",
    "    print(\"  - local:\", cat_counts_file_local)\n",
    "    print(\"  - drive:\", cat_counts_file_drive)\n",
    "\n",
    "    # Bar plots for each categorical column\n",
    "    for col in categorical_cols:\n",
    "        vc = full_df[col].value_counts(dropna=False).sort_index()\n",
    "        if vc.empty:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar([str(x) for x in vc.index], vc.values)\n",
    "        plt.title(f\"Frequency of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        fname = f\"{_safe_fname(col)}_bar.png\"\n",
    "        out_local = EDA_PLOTS_LOCAL / fname\n",
    "        out_drive = EDA_PLOTS_DRIVE / fname\n",
    "\n",
    "        plt.savefig(out_local, dpi=150)\n",
    "        plt.close()\n",
    "        copy_file(out_local, out_drive)\n",
    "\n",
    "    # Boxplots: numeric targets by categorical\n",
    "    BASE = str(RUN_PARAMS[\"features\"][\"regime_base\"])\n",
    "    W_LONG = int(RUN_PARAMS[\"features\"][\"rolling_w_long\"])\n",
    "    TARGET_COL = str(RUN_PARAMS[\"data\"][\"target_col\"])\n",
    "\n",
    "    numeric_targets = [f\"{BASE}_logret_cc\", f\"{BASE}_logret_cc_std_{W_LONG}\"]\n",
    "    if TARGET_COL in full_df.columns:\n",
    "        numeric_targets.append(TARGET_COL)\n",
    "    numeric_targets = [c for c in numeric_targets if c in full_df.columns]\n",
    "\n",
    "    print(\"[INFO] Numeric targets used for boxplots:\", numeric_targets)\n",
    "\n",
    "    for target in numeric_targets:\n",
    "        for col in categorical_cols:\n",
    "            if full_df[col].nunique(dropna=True) < 2:\n",
    "                continue\n",
    "\n",
    "            tmp = full_df[[col, target]].dropna()\n",
    "            if tmp.empty:\n",
    "                continue\n",
    "\n",
    "            groups, labels = [], []\n",
    "            for k in sorted(tmp[col].unique()):\n",
    "                groups.append(tmp.loc[tmp[col] == k, target].astype(\"float64\").values)\n",
    "                labels.append(str(k))\n",
    "\n",
    "            if len(groups) < 2:\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.boxplot(groups, labels=labels, showfliers=False)\n",
    "            plt.title(f\"{target} by {col}\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(target)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            fname = f\"{_safe_fname(target)}_by_{_safe_fname(col)}_box.png\"\n",
    "            out_local = EDA_PLOTS_LOCAL / fname\n",
    "            out_drive = EDA_PLOTS_DRIVE / fname\n",
    "\n",
    "            plt.savefig(out_local, dpi=150)\n",
    "            plt.close()\n",
    "            copy_file(out_local, out_drive)\n",
    "\n",
    "    # Cram\u00e9r's V heatmap\n",
    "    def cramers_v(x: pd.Series, y: pd.Series) -> float:\n",
    "        \"\"\"Calculate Cram\u00e9r's V statistic for two categorical variables.\"\"\"\n",
    "        xy = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "        if xy.empty:\n",
    "            return np.nan\n",
    "\n",
    "        confusion = pd.crosstab(xy[\"x\"], xy[\"y\"])\n",
    "        if confusion.shape[0] < 2 or confusion.shape[1] < 2:\n",
    "            return 0.0\n",
    "\n",
    "        chi2 = stats.chi2_contingency(confusion, correction=False)[0]\n",
    "        n = confusion.to_numpy().sum()\n",
    "        r, k = confusion.shape\n",
    "        denom = n * (min(r, k) - 1)\n",
    "        return 0.0 if denom <= 0 else float(np.sqrt(chi2 / denom))\n",
    "\n",
    "    n_cat = len(categorical_cols)\n",
    "    if n_cat >= 2:\n",
    "        mat = np.zeros((n_cat, n_cat), dtype=float)\n",
    "        for i, c1 in enumerate(categorical_cols):\n",
    "            for j, c2 in enumerate(categorical_cols):\n",
    "                mat[i, j] = 1.0 if i == j else cramers_v(full_df[c1], full_df[c2])\n",
    "\n",
    "        plt.figure(figsize=(max(10, n_cat * 0.6), max(8, n_cat * 0.6)))\n",
    "        im = plt.imshow(mat, vmin=0.0, vmax=1.0)\n",
    "        plt.title(\"Cram\u00e9r's V Heatmap \u2014 Categorical/Binary Variables\")\n",
    "        plt.xticks(range(n_cat), categorical_cols, rotation=90)\n",
    "        plt.yticks(range(n_cat), categorical_cols)\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        heatmap_local = EDA_PLOTS_LOCAL / \"categorical_cramersV_heatmap.png\"\n",
    "        heatmap_drive = EDA_PLOTS_DRIVE / \"categorical_cramersV_heatmap.png\"\n",
    "        plt.savefig(heatmap_local, dpi=150)\n",
    "        plt.close()\n",
    "        copy_file(heatmap_local, heatmap_drive)\n",
    "\n",
    "        print(\"[INFO] Saved Cram\u00e9r's V heatmap to:\")\n",
    "        print(\"  - local:\", heatmap_local)\n",
    "        print(\"  - drive:\", heatmap_drive)\n",
    "    else:\n",
    "        print(\"[INFO] Skipped Cram\u00e9r's V heatmap: need at least 2 categorical columns.\")\n",
    "\n",
    "    print(\"\\n[OK] Categorical/Binary EDA complete.\")\n",
    "    print(\"[INFO] LOCAL plots  :\", EDA_PLOTS_LOCAL)\n",
    "    print(\"[INFO] DRIVE plots  :\", EDA_PLOTS_DRIVE)\n",
    "    print(\"[INFO] LOCAL reports:\", EDA_REPS_LOCAL)\n",
    "    print(\"[INFO] DRIVE reports:\", EDA_REPS_DRIVE)\n",
    "\n",
    "print(\"[OK] BLOCK 19 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 4: Train/Valid/Test Split & NN Features\n",
    "\n",
    "**Data splitting and neural network feature preparation**\n",
    "\n",
    "**Blocks:** 20-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703c068",
   "metadata": {},
   "source": [
    "## BLOCK 20 \u2014 SPLIT + WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# Guardrails\nassert isinstance(full_df, pd.DataFrame), \"[ERROR] full_df must be a DataFrame.\"\nassert isinstance(full_df.index, pd.DatetimeIndex), \"[ERROR] full_df.index must be a DatetimeIndex.\"\nfull_df = full_df.sort_index()\n\n# Target column check\nif TARGET_T1 not in full_df.columns:\n    raise KeyError(f\"[ERROR] TARGET='{TARGET_T1}' not found in full_df. Run target-definition block first.\")\n\n# All features except target\nfeature_cols_t1 = [c for c in full_df.columns if c != TARGET_T1]\n\n# Model df (features + target)\nmodel_df = full_df.loc[:, feature_cols_t1 + [TARGET_T1]].copy()\nbefore = len(model_df)\nmodel_df = model_df.dropna(subset=[TARGET_T1]).copy()\nafter = len(model_df)\n\nif len(model_df) == 0:\n    raise ValueError(\"[ERROR] model_df became empty after dropping NaN target rows.\")\n\nprint(\"[INFO] model_df range:\", model_df.index.min(), \"->\", model_df.index.max(), \"| rows:\", len(model_df))\nprint(\"[INFO] Dropped rows due to missing target:\", before - after)\nprint(\"[INFO] #Features (ALL):\", len(feature_cols_t1))\n\n# Time split from RUN_PARAMS (date-based)\ndata_cfg = RUN_PARAMS[\"data\"]\n\ntrain_end_date = data_cfg[\"train_end\"]\nvalid_start_date = data_cfg[\"valid_start\"]\nvalid_end_date = data_cfg[\"valid_end\"]\ntest_start_date = data_cfg[\"test_start\"]\ntest_end_date = data_cfg.get(\"test_end\")\n\nprint(\"[INFO] Data range:\", model_df.index.min().date(), \"->\", model_df.index.max().date())\n\ntrain_end = pd.Timestamp(train_end_date)\nvalid_start = pd.Timestamp(valid_start_date)\nvalid_end = pd.Timestamp(valid_end_date)\ntest_start = pd.Timestamp(test_start_date)\ntest_end = pd.Timestamp(test_end_date) if test_end_date else model_df.index.max()\n\nmask_train = model_df.index <= train_end\nmask_valid = (model_df.index >= valid_start) & (model_df.index <= valid_end)\nmask_test = (model_df.index >= test_start) & (model_df.index <= test_end)\n\nsplit_info = f\"train: <= {train_end.date()} | valid: {valid_start.date()} - {valid_end.date()} | test: {test_start.date()} - {test_end.date()}\"\n\ntrain_df = model_df.loc[mask_train].copy()\nvalid_df = model_df.loc[mask_valid].copy()\ntest_df = model_df.loc[mask_test].copy()\n\nif len(train_df) == 0:\n    raise ValueError(\"[ERROR] train_df is empty. Check split configuration and data coverage.\")\n\nn_total = len(model_df)\npct_train = (len(train_df) / n_total * 100.0) if n_total else 0.0\npct_valid = (len(valid_df) / n_total * 100.0) if n_total else 0.0\npct_test = (len(test_df) / n_total * 100.0) if n_total else 0.0\n\nprint(\"[INFO] Split sizes:\",\n      f\"TRAIN={len(train_df):,} ({pct_train:.2f}%) | \"\n      f\"VALID={len(valid_df):,} ({pct_valid:.2f}%) | \"\n      f\"TEST={len(test_df):,} ({pct_test:.2f}%)\")\nprint(f\"[INFO] {split_info}\")\n\n# Weights per split: time * |y| (normalize mean=1 per split)\n_eps = float(RUN_PARAMS[\"features\"][\"eps\"])\nc = float(RUN_PARAMS[\"weights\"][\"c\"])\nmax_w = float(RUN_PARAMS[\"weights\"][\"max_w\"])\n\n\ndef _build_split_weights(df: pd.DataFrame, target_col: str, split_name: str):\n    \"\"\"Build sample weights for a split: time * |y| normalized to mean=1.\"\"\"\n    n = len(df)\n    if n == 0:\n        return np.array([], dtype=float), dict(\n            split=split_name, n=0, time_min=np.nan, time_max=np.nan, time_mean=np.nan,\n            y_min=np.nan, y_max=np.nan, y_med_abs=np.nan, y_cap_rate=np.nan,\n            w_min=np.nan, w_max=np.nan, w_mean=np.nan\n        )\n\n    w_time = np.linspace(1.0, 2.0, n, dtype=float)\n\n    y_vals = df[target_col].astype(float).to_numpy()\n    abs_y = np.abs(y_vals)\n    med_abs = float(np.median(abs_y)) if n else 0.0\n\n    ratio = abs_y / (med_abs + _eps)\n    w_y = 1.0 + c * np.sqrt(ratio)\n    w_y = np.clip(w_y, 1.0, max_w)\n    cap_rate = float(np.mean(w_y >= max_w))\n\n    w = w_time * w_y\n    w = w / (w.mean() + _eps)\n\n    stats_dict = dict(\n        split=split_name, n=n,\n        time_min=float(w_time.min()), time_max=float(w_time.max()), time_mean=float(w_time.mean()),\n        y_min=float(w_y.min()), y_max=float(w_y.max()), y_med_abs=float(med_abs), y_cap_rate=float(cap_rate),\n        w_min=float(w.min()), w_max=float(w.max()), w_mean=float(w.mean())\n    )\n    return w, stats_dict\n\n\nw_train, st_train = _build_split_weights(train_df, TARGET_T1, \"TRAIN\")\nw_valid, st_valid = _build_split_weights(valid_df, TARGET_T1, \"VALID\")\nw_test, st_test = _build_split_weights(test_df, TARGET_T1, \"TEST\")\n\nprint(\"\\n[INFO] y-weight cap rate (==max_w):\")\nprint(f\"  TRAIN: {st_train['y_cap_rate']:.4f} | c={c} max_w={max_w}\")\nprint(f\"  VALID: {st_valid['y_cap_rate']:.4f} | c={c} max_w={max_w}\" if len(valid_df) else \"  VALID: empty\")\nprint(f\"  TEST : {st_test['y_cap_rate']:.4f} | c={c} max_w={max_w}\" if len(test_df) else \"  TEST : empty\")\n\nprint(\"\\n[INFO] sample_weight diagnostics (per-split mean=1):\")\nfor st in [st_train, st_valid, st_test]:\n    if st[\"n\"] == 0:\n        print(f\"  {st['split']}: empty\")\n        continue\n    print(\n        f\"  {st['split']}: \"\n        f\"time[{st['time_min']:.3f}->{st['time_max']:.3f}] \"\n        f\"y[{st['y_min']:.3f}->{st['y_max']:.3f}] (median_abs_y={st['y_med_abs']:.6g}) \"\n        f\"w[{st['w_min']:.3f}->{st['w_max']:.3f}] (mean={st['w_mean']:.3f})\"\n    )\n\ntrain_df[\"sample_weight\"] = w_train\nvalid_df[\"sample_weight\"] = w_valid\ntest_df[\"sample_weight\"] = w_test\n\n# Final X/y matrices (source-of-truth = X_train_t1.columns)\nX_train_t1 = train_df.loc[:, feature_cols_t1].copy()\ny_train_t1 = train_df.loc[:, TARGET_T1].copy()\n\nX_valid_t1 = valid_df.loc[:, feature_cols_t1].copy()\ny_valid_t1 = valid_df.loc[:, TARGET_T1].copy()\n\nX_test_t1 = test_df.loc[:, feature_cols_t1].copy()\ny_test_t1 = test_df.loc[:, TARGET_T1].copy()\n\nfeature_cols_t1 = list(X_train_t1.columns)\n\nassert len(X_train_t1) == len(w_train), \"[ERROR] w_train length mismatch\"\nassert len(X_valid_t1) == len(w_valid), \"[ERROR] w_valid length mismatch\"\nassert len(X_test_t1) == len(w_test), \"[ERROR] w_test length mismatch\"\n\nprint(\"\\n[INFO] Shapes:\", X_train_t1.shape, X_valid_t1.shape, X_test_t1.shape)\nprint(\"[OK] feature_cols_t1 set from X_train_t1.columns | #Features=\", len(feature_cols_t1))\n\n# Save XGBoost splits (project data/processed local + drive)\nLOCAL_PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\nDRIVE_PROC_DATA_DIR = DATA_DIRS_DRIVE[\"processed\"]\n\n# Features (X) for XGBoost\nX_train_t1.to_pickle(LOCAL_PROC_DATA_DIR / \"X_train_xgb.pkl\")\nX_valid_t1.to_pickle(LOCAL_PROC_DATA_DIR / \"X_valid_xgb.pkl\")\nX_test_t1.to_pickle(LOCAL_PROC_DATA_DIR / \"X_test_xgb.pkl\")\n\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_train_xgb.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_xgb.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_valid_xgb.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_xgb.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_test_xgb.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_xgb.pkl\")\n\n# Target (y) - shared across all models\nsave_pickle(y_train_t1, LOCAL_PROC_DATA_DIR / \"y_train.pkl\")\nsave_pickle(y_valid_t1, LOCAL_PROC_DATA_DIR / \"y_valid.pkl\")\nsave_pickle(y_test_t1, LOCAL_PROC_DATA_DIR / \"y_test.pkl\")\n\ncopy_file(LOCAL_PROC_DATA_DIR / \"y_train.pkl\", DRIVE_PROC_DATA_DIR / \"y_train.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"y_valid.pkl\", DRIVE_PROC_DATA_DIR / \"y_valid.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"y_test.pkl\", DRIVE_PROC_DATA_DIR / \"y_test.pkl\")\n\n# Sample weights - shared across all models\nsave_pickle(w_train, LOCAL_PROC_DATA_DIR / \"weights_train.pkl\")\nsave_pickle(w_valid, LOCAL_PROC_DATA_DIR / \"weights_valid.pkl\")\nsave_pickle(w_test, LOCAL_PROC_DATA_DIR / \"weights_test.pkl\")\n\ncopy_file(LOCAL_PROC_DATA_DIR / \"weights_train.pkl\", DRIVE_PROC_DATA_DIR / \"weights_train.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"weights_valid.pkl\", DRIVE_PROC_DATA_DIR / \"weights_valid.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"weights_test.pkl\", DRIVE_PROC_DATA_DIR / \"weights_test.pkl\")\n\nprint(\"\\n[INFO] Saved XGBoost splits to:\", LOCAL_PROC_DATA_DIR)\nprint(\"  - X_train/valid/test_xgb.pkl\")\nprint(\"  - y_train/valid/test.pkl\")\nprint(\"  - weights_train/valid/test.pkl\")\n\nprint(\"[OK] BLOCK 20 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7b238",
   "metadata": {},
   "source": [
    "## BLOCK 21 \u2014 NN FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8face",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preconditions\n",
    "assert \"X_train_t1\" in globals(), \"[ERROR] X_train_t1 missing. Run BLOCK 20 first.\"\n",
    "assert \"y_train_t1\" in globals(), \"[ERROR] y_train_t1 missing. Run BLOCK 20 first.\"\n",
    "\n",
    "FS_OUT = ensure_dir(Path(FS_DIR))\n",
    "\n",
    "# Params from RUN_PARAMS\n",
    "NN_CFG = RUN_PARAMS[\"nn_feature_select\"]\n",
    "N40 = int(NN_CFG[\"n40\"])\n",
    "N80 = int(NN_CFG[\"n80\"])\n",
    "per_group_40 = int(NN_CFG[\"per_group_40\"])\n",
    "per_group_80 = int(NN_CFG[\"per_group_80\"])\n",
    "corr_thr = float(NN_CFG[\"corr_thr\"])\n",
    "mi_neighbors = int(NN_CFG[\"mi_n_neighbors\"])\n",
    "mi_random_state = int(NN_CFG[\"mi_random_state\"])\n",
    "\n",
    "\n",
    "# Helpers\n",
    "def _dedup_preserve_order(seq):\n",
    "    \"\"\"Remove duplicates while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if isinstance(x, str) and x and (x not in seen):\n",
    "            out.append(x)\n",
    "            seen.add(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _feature_group(col: str) -> str:\n",
    "    \"\"\"Extract feature group from column name.\"\"\"\n",
    "    if not isinstance(col, str) or not col:\n",
    "        return \"OTHER\"\n",
    "    if col.startswith(\"^\"):\n",
    "        return col.split(\"_\", 1)[0]\n",
    "    return col.split(\"_\", 1)[0]\n",
    "\n",
    "\n",
    "def _safe_spearman_corr(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Spearman correlation on numeric DataFrame.\"\"\"\n",
    "    return df.corr(method=\"spearman\")\n",
    "\n",
    "\n",
    "def _select_with_decorrelation(ranked_cols, X_train_num, k, corr_threshold):\n",
    "    \"\"\"Select top-k features with de-correlation.\"\"\"\n",
    "    ranked_cols = [c for c in ranked_cols if c in X_train_num.columns]\n",
    "    if k <= 0:\n",
    "        return []\n",
    "\n",
    "    selected = []\n",
    "    if len(ranked_cols) == 0:\n",
    "        return selected\n",
    "\n",
    "    # Precompute correlation only once for speed\n",
    "    sub = X_train_num.loc[:, ranked_cols]\n",
    "    corr = _safe_spearman_corr(sub).abs()\n",
    "\n",
    "    for c in ranked_cols:\n",
    "        if len(selected) == 0:\n",
    "            selected.append(c)\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        too_close = False\n",
    "        for s in selected:\n",
    "            val = corr.at[c, s]\n",
    "            if pd.notna(val) and float(val) >= corr_threshold:\n",
    "                too_close = True\n",
    "                break\n",
    "        if not too_close:\n",
    "            selected.append(c)\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "# Numeric matrix (TRAIN only) + cleaning\n",
    "X = X_train_t1.copy()\n",
    "y = y_train_t1.astype(float).to_numpy()\n",
    "\n",
    "# Keep numeric dtypes only\n",
    "X = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Drop constant columns\n",
    "nunique = X.nunique(dropna=False)\n",
    "const_cols = nunique[nunique <= 1].index.tolist()\n",
    "if const_cols:\n",
    "    X = X.drop(columns=const_cols)\n",
    "    print(f\"[INFO] Dropped {len(const_cols)} constant columns\")\n",
    "\n",
    "# Replace inf -> nan, then drop cols that have any non-finite\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "bad_cols = [c for c in X.columns if not np.isfinite(X[c].to_numpy(dtype=float)).all()]\n",
    "if bad_cols:\n",
    "    X = X.drop(columns=bad_cols)\n",
    "    print(f\"[INFO] Dropped {len(bad_cols)} columns with non-finite values\")\n",
    "\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"[ERROR] No usable numeric features left after cleaning on TRAIN.\")\n",
    "\n",
    "print(f\"[INFO] Features after cleaning: {X.shape[1]}\")\n",
    "\n",
    "# Mutual Information ranking (TRAIN only)\n",
    "print(\"[INFO] Computing Mutual Information (this may take a minute)...\")\n",
    "mi = mutual_info_regression(\n",
    "    X.to_numpy(dtype=float),\n",
    "    y,\n",
    "    n_neighbors=mi_neighbors,\n",
    "    random_state=mi_random_state,\n",
    ")\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Group-aware shortlist\n",
    "groups = {}\n",
    "for col in mi_series.index:\n",
    "    g = _feature_group(col)\n",
    "    groups.setdefault(g, []).append(col)\n",
    "\n",
    "\n",
    "def _build_group_seed(per_group):\n",
    "    \"\"\"Build seed list with per_group features from each group.\"\"\"\n",
    "    seed = []\n",
    "    for g, cols in groups.items():\n",
    "        take = cols[:per_group]\n",
    "        seed.extend(take)\n",
    "    return _dedup_preserve_order(seed)\n",
    "\n",
    "\n",
    "seed40 = _build_group_seed(per_group_40)\n",
    "seed80 = _build_group_seed(per_group_80)\n",
    "\n",
    "# Global ranked list\n",
    "ranked_all = mi_series.index.tolist()\n",
    "\n",
    "# Final pick with de-correlation\n",
    "rank40 = _dedup_preserve_order(seed40 + ranked_all)\n",
    "rank80 = _dedup_preserve_order(seed80 + ranked_all)\n",
    "\n",
    "neural_40 = _select_with_decorrelation(rank40, X, N40, corr_thr)\n",
    "neural_80 = _select_with_decorrelation(rank80, X, N80, corr_thr)\n",
    "\n",
    "if len(neural_40) == 0 or len(neural_80) == 0:\n",
    "    raise ValueError(\"[ERROR] NN feature selection produced empty sets.\")\n",
    "\n",
    "# Ensure 40 \u2286 80 if possible\n",
    "if not set(neural_40).issubset(set(neural_80)):\n",
    "    base = _dedup_preserve_order(neural_40 + neural_80 + ranked_all)\n",
    "    neural_80 = _select_with_decorrelation(base, X, N80, corr_thr)\n",
    "\n",
    "# Save\n",
    "p40 = FS_OUT / \"neural_feature_cols_40_bygroup.pkl\"\n",
    "p80 = FS_OUT / \"neural_feature_cols_80_bygroup.pkl\"\n",
    "save_pickle(neural_40, p40)\n",
    "save_pickle(neural_80, p80)\n",
    "\n",
    "# Also save to DRIVE\n",
    "FS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"fs_dir\"]))\n",
    "copy_file(p40, FS_OUT_DRIVE / p40.name)\n",
    "copy_file(p80, FS_OUT_DRIVE / p80.name)\n",
    "\n",
    "print(\"[OK] NN feature lists saved:\")\n",
    "print(\"  -\", p40, \"| n=\", len(neural_40))\n",
    "print(\"  -\", p80, \"| n=\", len(neural_80))\n",
    "\n",
    "# Diagnostics\n",
    "print(\"[INFO] Top-10 MI features (TRAIN):\", mi_series.head(10).index.tolist())\n",
    "print(\"[INFO] Groups covered in NEURAL40:\", sorted({_feature_group(c) for c in neural_40}))\n",
    "print(\"[INFO] Groups covered in NEURAL80:\", sorted({_feature_group(c) for c in neural_80}))\n",
    "\n",
    "print(\"[OK] BLOCK 21 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101cc5f",
   "metadata": {},
   "source": [
    "## BLOCK 22 \u2014 NEURAL FEATURE PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34278f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# Preconditions\nassert \"X_train_t1\" in globals(), \"[ERROR] X_train_t1 missing. Run BLOCK 20 first.\"\nassert \"X_valid_t1\" in globals(), \"[ERROR] X_valid_t1 missing. Run BLOCK 20 first.\"\nassert \"X_test_t1\" in globals(), \"[ERROR] X_test_t1 missing. Run BLOCK 20 first.\"\n\nFS_OUT = ensure_dir(Path(FS_DIR))\nFS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"fs_dir\"]))\n\n# Load neural feature lists from BLOCK 21 (with fallback)\nFS_DIR_LOCAL = Path(LOCAL_PATHS[\"fs_dir\"])\nFS_DIR_DRIVE = Path(DRIVE_PATHS[\"fs_dir\"])\nFALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\nFALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n\nneural_cols_40 = load_with_fallback(\n    \"neural_feature_cols_40_bygroup.pkl\",\n    FS_DIR_LOCAL, FALLBACK_PROC_LOCAL, FS_DIR_DRIVE, FALLBACK_PROC_DRIVE\n)\nneural_cols_80 = load_with_fallback(\n    \"neural_feature_cols_80_bygroup.pkl\",\n    FS_DIR_LOCAL, FALLBACK_PROC_LOCAL, FS_DIR_DRIVE, FALLBACK_PROC_DRIVE\n)\n\n# Clean and dedupe\nneural_cols_40 = list(dict.fromkeys([c for c in neural_cols_40 if isinstance(c, str) and c.strip()]))\nneural_cols_80 = list(dict.fromkeys([c for c in neural_cols_80 if isinstance(c, str) and c.strip()]))\n\n# Filter to columns that exist in X_train_t1\ntrain_cols = set(X_train_t1.columns)\nneural_cols_40 = [c for c in neural_cols_40 if c in train_cols]\nneural_cols_80 = [c for c in neural_cols_80 if c in train_cols]\n\nif len(neural_cols_40) == 0:\n    raise ValueError(\"[ERROR] neural_cols_40 became empty after filtering to X_train_t1.columns.\")\nif len(neural_cols_80) == 0:\n    raise ValueError(\"[ERROR] neural_cols_80 became empty after filtering to X_train_t1.columns.\")\n\n# Verify columns exist in valid/test splits\nvalid_cols = set(X_valid_t1.columns)\ntest_cols = set(X_test_t1.columns)\n\nmissing_valid_40 = sorted(set(neural_cols_40) - valid_cols)\nmissing_test_40 = sorted(set(neural_cols_40) - test_cols)\nmissing_valid_80 = sorted(set(neural_cols_80) - valid_cols)\nmissing_test_80 = sorted(set(neural_cols_80) - test_cols)\n\n\ndef _report_missing(tag, miss):\n    \"\"\"Report missing columns.\"\"\"\n    if miss:\n        print(f\"[ERROR] Missing in {tag}: {len(miss)} columns (showing up to 20): {miss[:20]}\")\n        return True\n    return False\n\n\nerr = False\nerr |= _report_missing(\"X_valid_t1 (neural_40)\", missing_valid_40)\nerr |= _report_missing(\"X_test_t1  (neural_40)\", missing_test_40)\nerr |= _report_missing(\"X_valid_t1 (neural_80)\", missing_valid_80)\nerr |= _report_missing(\"X_test_t1  (neural_80)\", missing_test_80)\n\nif err:\n    raise KeyError(\"[ERROR] Inconsistent columns across splits for neural feature sets.\")\n\n# Build neural feature matrices\nX_train_neural_40 = X_train_t1.loc[:, neural_cols_40].copy()\nX_valid_neural_40 = X_valid_t1.loc[:, neural_cols_40].copy()\nX_test_neural_40 = X_test_t1.loc[:, neural_cols_40].copy()\n\nX_train_neural_80 = X_train_t1.loc[:, neural_cols_80].copy()\nX_valid_neural_80 = X_valid_t1.loc[:, neural_cols_80].copy()\nX_test_neural_80 = X_test_t1.loc[:, neural_cols_80].copy()\n\nprint(\"\\n[OK] Neural feature matrices created:\")\nprint(f\"  - 40: TRAIN={X_train_neural_40.shape} | VALID={X_valid_neural_40.shape} | TEST={X_test_neural_40.shape}\")\nprint(f\"  - 80: TRAIN={X_train_neural_80.shape} | VALID={X_valid_neural_80.shape} | TEST={X_test_neural_80.shape}\")\n\n# Save resolved neural feature lists to FS_DIR\np40_res = FS_OUT / \"neural_feature_cols_40_bygroup_resolved.pkl\"\np80_res = FS_OUT / \"neural_feature_cols_80_bygroup_resolved.pkl\"\nsave_pickle(neural_cols_40, p40_res)\nsave_pickle(neural_cols_80, p80_res)\n\ncopy_file(p40_res, FS_OUT_DRIVE / p40_res.name)\ncopy_file(p80_res, FS_OUT_DRIVE / p80_res.name)\n\n# Save Neural 40 splits to data/processed\nLOCAL_PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\nDRIVE_PROC_DATA_DIR = DATA_DIRS_DRIVE[\"processed\"]\n\nX_train_neural_40.to_pickle(LOCAL_PROC_DATA_DIR / \"X_train_neural_40.pkl\")\nX_valid_neural_40.to_pickle(LOCAL_PROC_DATA_DIR / \"X_valid_neural_40.pkl\")\nX_test_neural_40.to_pickle(LOCAL_PROC_DATA_DIR / \"X_test_neural_40.pkl\")\n\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_train_neural_40.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_neural_40.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_valid_neural_40.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_neural_40.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_test_neural_40.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_neural_40.pkl\")\n\n# Save Neural 80 splits to data/processed\nX_train_neural_80.to_pickle(LOCAL_PROC_DATA_DIR / \"X_train_neural_80.pkl\")\nX_valid_neural_80.to_pickle(LOCAL_PROC_DATA_DIR / \"X_valid_neural_80.pkl\")\nX_test_neural_80.to_pickle(LOCAL_PROC_DATA_DIR / \"X_test_neural_80.pkl\")\n\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_train_neural_80.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_neural_80.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_valid_neural_80.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_neural_80.pkl\")\ncopy_file(LOCAL_PROC_DATA_DIR / \"X_test_neural_80.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_neural_80.pkl\")\n\nprint(\"\\n[OK] Saved Neural splits to:\", LOCAL_PROC_DATA_DIR)\nprint(\"  - X_train/valid/test_neural_40.pkl\")\nprint(\"  - X_train/valid/test_neural_80.pkl\")\n\nprint(\"[OK] BLOCK 22 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 5: Feature Selection\n",
    "\n",
    "**XGBoost feature selection with mutual information**\n",
    "\n",
    "**Block:** 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42ed7a",
   "metadata": {},
   "source": [
    "## BLOCK 23 \u2014 XGB FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d97bd",
   "metadata": {},
   "outputs": [],
   "source": "\n# Config from RUN_PARAMS\nXGB_FS_CFG = RUN_PARAMS[\"xgb_fs\"]\nMISSING_SUFFIX = \"_is_missing\"\n\n# Input/Output directories\nPROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\nFS_OUT_LOCAL = ensure_dir(Path(FS_DIR))\nFS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"fs_dir\"]))\n\n# -------------------------\n# Load splits with fallback (RUN_ID -> data/processed -> DRIVE)\n# -------------------------\nRUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\nFALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\nRUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\nFALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n\nprint(\"[INFO] Loading splits with fallback...\")\n\nX_train_t1 = load_with_fallback(\"X_train_xgb.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\nX_valid_t1 = load_with_fallback(\"X_valid_xgb.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\nX_test_t1 = load_with_fallback(\"X_test_xgb.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n\ny_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\ny_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n\nw_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\nw_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n\nprint(f\"[INFO] Loaded: X_train={X_train_t1.shape} | X_valid={X_valid_t1.shape} | X_test={X_test_t1.shape}\")\n\nprint(\"[INFO] XGB Feature Selection output dirs:\")\nprint(\"  - LOCAL:\", FS_OUT_LOCAL)\nprint(\"  - DRIVE:\", FS_OUT_DRIVE)\n\n\n# Metric functions defined in Cell 5 (using w_rmse instead of weighted_rmse)\n\n\n# -------------------------\n# 1) Identify missingness flags (for dependency closure)\n# -------------------------\nall_train_cols = list(X_train_t1.columns)\nmissing_flags = [c for c in all_train_cols if c.endswith(MISSING_SUFFIX)]\nmissing_flag_set = set(missing_flags)\n\nbase_to_missing = {}\nfor flag in missing_flags:\n    base = flag[:-len(MISSING_SUFFIX)]\n    if base in missing_flag_set:\n        continue\n    if base in all_train_cols:\n        base_to_missing[base] = flag\n\nprint(f\"[INFO] Found missingness flags: {len(missing_flags)}\")\n\n# -------------------------\n# 2) Build matrices for FS (exclude flags)\n# -------------------------\nnonflag_cols = [c for c in all_train_cols if c not in missing_flag_set and c != TARGET_T1 and c != \"sample_weight\"]\n\nXtr = X_train_t1.loc[:, nonflag_cols].copy()\nXva = X_valid_t1.loc[:, nonflag_cols].copy()\n\nytr = y_train_t1.astype(float).copy()\nyva = y_valid_t1.astype(float).copy()\n\nwtr = _to_np(w_train)\nwva = _to_np(w_valid)\n\n# -------------------------\n# 3) Drop constant TRAIN columns\n# -------------------------\nnunique = Xtr.nunique(dropna=False)\nconst_cols = nunique[nunique <= 1].index.tolist()\nif const_cols:\n    print(f\"[INFO] Dropping constant TRAIN columns: {len(const_cols)}\")\n    Xtr = Xtr.drop(columns=const_cols)\n    Xva = Xva.drop(columns=const_cols, errors=\"ignore\")\n\n# -------------------------\n# 4) Spearman filter\n# -------------------------\nSPEARMAN_THRESH = float(XGB_FS_CFG[\"spearman_thresh\"])\n\ncorr_ff = Xtr.corr(method=\"spearman\").abs().fillna(0.0)\ncols = list(corr_ff.columns)\n\nfeat_to_y = Xtr.apply(lambda s: s.corr(ytr, method=\"spearman\"))\nfeat_to_y_abs = feat_to_y.abs().fillna(0.0)\n\npairs = []\nfor i in range(len(cols)):\n    for j in range(i + 1, len(cols)):\n        cval = float(corr_ff.iat[i, j])\n        if cval > SPEARMAN_THRESH:\n            pairs.append((cols[i], cols[j], cval))\npairs.sort(key=lambda t: t[2], reverse=True)\n\nactive = set(cols)\n\nfor a, b, cval in pairs:\n    if (a in active) and (b in active):\n        ay = float(feat_to_y_abs.get(a, 0.0))\n        by = float(feat_to_y_abs.get(b, 0.0))\n        drop = a if ay < by else b\n        active.remove(drop)\n\nkept_cols = [c for c in cols if c in active]\nprint(f\"[INFO] Spearman filter: start={len(cols)} | kept={len(kept_cols)} | thresh={SPEARMAN_THRESH}\")\n\nX_train_fs = Xtr.loc[:, kept_cols].copy()\nX_valid_fs = Xva.loc[:, kept_cols].copy()\n\n# -------------------------\n# 5) XGBoost FS model (GAIN)\n# -------------------------\nxgb_params = dict(\n    n_estimators=int(XGB_FS_CFG[\"n_estimators\"]),\n    learning_rate=float(XGB_FS_CFG[\"learning_rate\"]),\n    max_depth=int(XGB_FS_CFG[\"max_depth\"]),\n    min_child_weight=float(XGB_FS_CFG[\"min_child_weight\"]),\n    gamma=float(XGB_FS_CFG[\"gamma\"]),\n    subsample=float(XGB_FS_CFG[\"subsample\"]),\n    colsample_bytree=float(XGB_FS_CFG[\"colsample_bytree\"]),\n    reg_alpha=float(XGB_FS_CFG[\"reg_alpha\"]),\n    reg_lambda=float(XGB_FS_CFG[\"reg_lambda\"]),\n    max_delta_step=float(XGB_FS_CFG[\"max_delta_step\"]),\n    objective=\"reg:squarederror\",\n    eval_metric=\"rmse\",\n    tree_method=\"hist\",\n    random_state=int(XGB_FS_CFG[\"random_state\"]),\n    n_jobs=-1,\n    early_stopping_rounds=int(XGB_FS_CFG[\"early_stopping_rounds\"]),\n)\n\nprint(\"[INFO] Training XGBoost for feature selection...\")\nmodel_fs = xgb.XGBRegressor(**xgb_params)\nmodel_fs.fit(\n    X_train_fs, ytr.loc[X_train_fs.index],\n    sample_weight=wtr,\n    eval_set=[(X_valid_fs, yva.loc[X_valid_fs.index])],\n    sample_weight_eval_set=[wva],\n    verbose=False\n)\n\nbest_iter = getattr(model_fs, \"best_iteration\", None)\nbest_score = getattr(model_fs, \"best_score\", None)\nprint(f\"[INFO] XGB train done. best_iteration={best_iter} | best_score(valid_rmse)={best_score}\")\n\nbooster = model_fs.get_booster()\nscore_gain = booster.get_score(importance_type=\"gain\")\n\nimp_gain = pd.DataFrame({\"feature\": kept_cols})\nimp_gain[\"gain\"] = imp_gain[\"feature\"].map(score_gain).fillna(0.0).astype(float)\nimp_gain = imp_gain.sort_values(\"gain\", ascending=False).reset_index(drop=True)\n\ngain_sum = float(imp_gain[\"gain\"].sum())\nimp_gain[\"gain_frac\"] = imp_gain[\"gain\"] / (gain_sum + EPS)\nimp_gain[\"cum_gain\"] = imp_gain[\"gain_frac\"].cumsum()\n\n# -------------------------\n# 6) Permutation importance on VALID\n# -------------------------\ndef neg_weighted_rmse_scorer(estimator, X, y):\n    p = estimator.predict(X)\n    return -w_rmse(y, p, wva)\n\n\nPERM_REPEATS = int(XGB_FS_CFG[\"perm_repeats\"])\n\nprint(f\"[INFO] Computing permutation importance (repeats={PERM_REPEATS})...\")\nperm = permutation_importance(\n    model_fs,\n    X_valid_fs,\n    yva.loc[X_valid_fs.index],\n    scoring=neg_weighted_rmse_scorer,\n    n_repeats=PERM_REPEATS,\n    random_state=int(XGB_FS_CFG[\"random_state\"]),\n    n_jobs=-1\n)\n\nperm_df = pd.DataFrame({\n    \"feature\": X_valid_fs.columns,\n    \"perm_importance_mean\": perm.importances_mean,\n    \"perm_importance_std\": perm.importances_std\n}).reset_index(drop=True)\n\nperm_df = perm_df.merge(\n    imp_gain.loc[:, [\"feature\", \"gain\", \"gain_frac\", \"cum_gain\"]],\n    on=\"feature\",\n    how=\"left\"\n)\n\n# -------------------------\n# 7) Selection policy + fallback ladder\n# -------------------------\nGAIN_CUM_THRESH = float(XGB_FS_CFG[\"gain_cum_thresh\"])\nMIN_FEATURES = int(XGB_FS_CFG[\"min_features\"])\nNEG_SIGMA = float(XGB_FS_CFG[\"neg_sigma\"])\nPOS_SIGMA = float(XGB_FS_CFG[\"pos_sigma\"])\nMIN_GAIN = float(XGB_FS_CFG[\"min_gain\"])\n\nperm_df[\"perm_strongly_negative\"] = (\n    (perm_df[\"perm_importance_mean\"] < 0) &\n    (np.abs(perm_df[\"perm_importance_mean\"]) > (NEG_SIGMA * (perm_df[\"perm_importance_std\"] + EPS)))\n)\n\nperm_df[\"perm_confident_positive\"] = (\n    (perm_df[\"perm_importance_mean\"] > 0) &\n    (perm_df[\"perm_importance_mean\"] > (POS_SIGMA * (perm_df[\"perm_importance_std\"] + EPS)))\n)\n\nstrong_neg_set = set(perm_df.loc[perm_df[\"perm_strongly_negative\"], \"feature\"].tolist())\n\ngain_candidates = imp_gain.loc[\n    (imp_gain[\"cum_gain\"] <= GAIN_CUM_THRESH) | (imp_gain.index < MIN_FEATURES),\n    \"feature\"\n].tolist()\nprint(f\"[INFO] Candidates by GAIN: cum<={GAIN_CUM_THRESH} with min {MIN_FEATURES} => {len(gain_candidates)}\")\n\nperm_map = perm_df.set_index(\"feature\")[[\"perm_confident_positive\", \"perm_importance_mean\", \"gain\"]]\n\nselected = []\nfor f in gain_candidates:\n    if f in strong_neg_set:\n        continue\n    g = float(perm_map.loc[f, \"gain\"]) if f in perm_map.index else 0.0\n    if g <= MIN_GAIN:\n        continue\n    is_pos = bool(perm_map.loc[f, \"perm_confident_positive\"]) if f in perm_map.index else False\n    if not is_pos:\n        continue\n    selected.append(f)\n\nprint(f\"[INFO] Selected after strict filters: {len(selected)}\")\n\n# Fallback 1: relax to perm_mean > 0\nif len(selected) < MIN_FEATURES:\n    print(\"[WARN] Too few features; relaxing to perm_mean > 0.\")\n    selected = []\n    for f in gain_candidates:\n        if f in strong_neg_set:\n            continue\n        g = float(perm_map.loc[f, \"gain\"]) if f in perm_map.index else 0.0\n        if g <= MIN_GAIN:\n            continue\n        pm = float(perm_map.loc[f, \"perm_importance_mean\"]) if f in perm_map.index else -1.0\n        if pm <= 0:\n            continue\n        selected.append(f)\n    print(f\"[INFO] Selected after relaxed filter: {len(selected)}\")\n\n# Fallback 2: gain candidates excluding strong-neg\nif len(selected) < MIN_FEATURES:\n    print(\"[WARN] Still too few; final fallback to GAIN candidates excluding strongly-negative.\")\n    selected = []\n    for f in gain_candidates:\n        if f in strong_neg_set:\n            continue\n        g = float(perm_map.loc[f, \"gain\"]) if f in perm_map.index else 0.0\n        if g <= MIN_GAIN:\n            continue\n        selected.append(f)\n    print(f\"[INFO] Selected after final fallback: {len(selected)}\")\n\n# Fallback 3: GUARANTEE min_features - take top by XGB GAIN\nif len(selected) < MIN_FEATURES:\n    print(f\"[WARN] Final fallback: taking top {MIN_FEATURES} features by XGB GAIN.\")\n    # Sort gain_candidates by XGB gain (highest first)\n    top_by_gain = imp_gain.head(MIN_FEATURES)[\"feature\"].tolist()\n    selected = top_by_gain\n    print(f\"[INFO] Selected after GAIN-only fallback: {len(selected)}\")\n\n# -------------------------\n# 8) Dependency closure: add X_is_missing if exists\n# -------------------------\nselected_set = set(selected)\nflags_added = []\n\nfor base, flag in base_to_missing.items():\n    if (base in selected_set) and (flag in X_train_t1.columns) and (flag not in selected_set):\n        selected.append(flag)\n        selected_set.add(flag)\n        flags_added.append(flag)\n\nprint(f\"[INFO] Missingness flags added: {len(flags_added)}\")\n\nselected_final = list(dict.fromkeys(selected))\nprint(f\"[INFO] FINAL selected features: {len(selected_final)}\")\n\n# -------------------------\n# 9) Save outputs (LOCAL + DRIVE)\n# -------------------------\n# Selected features\nsel_txt = FS_OUT_LOCAL / \"selected_features_xgb.txt\"\nsel_pkl = FS_OUT_LOCAL / \"selected_features_xgb.pkl\"\nsel_csv = FS_OUT_LOCAL / \"selected_features_xgb.csv\"\n\nsel_txt.write_text(\"\\n\".join(selected_final), encoding=\"utf-8\")\nsave_pickle(selected_final, sel_pkl)\npd.DataFrame({\"feature\": selected_final}).to_csv(sel_csv, index=False)\n\ncopy_file(sel_txt, FS_OUT_DRIVE / sel_txt.name)\ncopy_file(sel_pkl, FS_OUT_DRIVE / sel_pkl.name)\ncopy_file(sel_csv, FS_OUT_DRIVE / sel_csv.name)\n\n# Importance tables\nimp_gain_csv = FS_OUT_LOCAL / \"feature_importance_gain.csv\"\nperm_csv = FS_OUT_LOCAL / \"feature_importance_permutation_valid.csv\"\n\nimp_gain.to_csv(imp_gain_csv, index=False)\nperm_df.sort_values(\"perm_importance_mean\", ascending=False).to_csv(perm_csv, index=False)\n\ncopy_file(imp_gain_csv, FS_OUT_DRIVE / imp_gain_csv.name)\ncopy_file(perm_csv, FS_OUT_DRIVE / perm_csv.name)\n\n# Create and save filtered matrices (xgb_selected) to data/processed/\nDRIVE_PROC_DATA_DIR = DATA_DIRS_DRIVE[\"processed\"]\n\nX_train_xgb_selected = X_train_t1.loc[:, selected_final].copy()\nX_valid_xgb_selected = X_valid_t1.loc[:, selected_final].copy()\nX_test_xgb_selected = X_test_t1.loc[:, selected_final].copy()\n\nX_train_xgb_selected.to_pickle(PROC_DATA_DIR / \"X_train_xgb_selected.pkl\")\nX_valid_xgb_selected.to_pickle(PROC_DATA_DIR / \"X_valid_xgb_selected.pkl\")\nX_test_xgb_selected.to_pickle(PROC_DATA_DIR / \"X_test_xgb_selected.pkl\")\n\ncopy_file(PROC_DATA_DIR / \"X_train_xgb_selected.pkl\", DRIVE_PROC_DATA_DIR / \"X_train_xgb_selected.pkl\")\ncopy_file(PROC_DATA_DIR / \"X_valid_xgb_selected.pkl\", DRIVE_PROC_DATA_DIR / \"X_valid_xgb_selected.pkl\")\ncopy_file(PROC_DATA_DIR / \"X_test_xgb_selected.pkl\", DRIVE_PROC_DATA_DIR / \"X_test_xgb_selected.pkl\")\n\nprint(\"\\n[OK] Saved XGB Feature Selection artifacts:\")\nprint(\"  -\", sel_txt.name)\nprint(\"  -\", sel_pkl.name)\nprint(\"  -\", sel_csv.name)\nprint(\"  -\", imp_gain_csv.name)\nprint(\"  -\", perm_csv.name)\nprint(f\"[OK] Saved to {PROC_DATA_DIR}:\")\nprint(f\"  - X_train_xgb_selected.pkl ({X_train_xgb_selected.shape})\")\nprint(f\"  - X_valid_xgb_selected.pkl ({X_valid_xgb_selected.shape})\")\nprint(f\"  - X_test_xgb_selected.pkl ({X_test_xgb_selected.shape})\")\n\nprint(\"[OK] BLOCK 23 complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 6: XGBoost Hyperparameter Optimization\n",
    "\n",
    "**3-stage HPO: Broad \u2192 Refine \u2192 Low-LR**\n",
    "\n",
    "**Block:** 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884fe67",
   "metadata": {},
   "source": [
    "## BLOCK 24 \u2014 XGB HYPERPARAMETER OPTIMIZATION (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config\n",
    "HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
    "\n",
    "# Load selected features from BLOCK 23 (with fallback)\n",
    "FS_DIR_LOCAL = Path(LOCAL_PATHS[\"fs_dir\"])\n",
    "FS_DIR_DRIVE = Path(DRIVE_PATHS[\"fs_dir\"])\n",
    "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
    "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
    "\n",
    "selected_features = load_with_fallback(\n",
    "    \"selected_features_xgb.pkl\",\n",
    "    FS_DIR_LOCAL,           # 1. runs/RUN_ID/feature_selection (LOCAL)\n",
    "    FALLBACK_PROC_LOCAL,    # 2. data/processed (LOCAL)\n",
    "    FS_DIR_DRIVE,           # 3. runs/RUN_ID/feature_selection (DRIVE)\n",
    "    FALLBACK_PROC_DRIVE     # 4. data/processed (DRIVE)\n",
    ")\n",
    "print(f\"[INFO] Loaded {len(selected_features)} selected features\")\n",
    "# Directories\n",
    "\n",
    "# --- Fallback Logic: Try RUN_ID first, then data/processed ---\n",
    "def get_data_path(filename, run_local, fallback_local, run_drive=None, fallback_drive=None):\n",
    "    \"\"\"Return first existing path from: RUN_LOCAL -> FALLBACK_LOCAL -> RUN_DRIVE -> FALLBACK_DRIVE.\"\"\"\n",
    "    paths = [\n",
    "        (Path(run_local) / filename, \"RUN_ID (LOCAL)\"),\n",
    "        (Path(fallback_local) / filename, \"data/processed (LOCAL)\"),\n",
    "    ]\n",
    "    if run_drive:\n",
    "        paths.append((Path(run_drive) / filename, \"RUN_ID (DRIVE)\"))\n",
    "    if fallback_drive:\n",
    "        paths.append((Path(fallback_drive) / filename, \"data/processed (DRIVE)\"))\n",
    "    \n",
    "    for path, src in paths:\n",
    "        if path.exists():\n",
    "            print(f\"  [LOAD] {filename} <- {src}\")\n",
    "            return path\n",
    "    \n",
    "    raise FileNotFoundError(f\"{filename} not found in any location\")\n",
    "\n",
    "RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
    "FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
    "RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
    "FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
    "print(\"[INFO] Data loading with fallback enabled\")\n",
    "\n",
    "MS_OUT_LOCAL = ensure_dir(Path(MS_DIR))\n",
    "MS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"ms_dir\"]))\n",
    "\n",
    "print(\"[INFO] HPO output dirs:\")\n",
    "print(\"  - LOCAL:\", MS_OUT_LOCAL)\n",
    "print(\"  - DRIVE:\", MS_OUT_DRIVE)\n",
    "\n",
    "# -------------------------\n",
    "# Load splits from data/processed/ (TRAIN + VALID only)\n",
    "# -------------------------\n",
    "print(\"[INFO] Loading splits with fallback logic...\")\n",
    "\n",
    "X_train_sel = pd.read_pickle(get_data_path(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
    "X_valid_sel = pd.read_pickle(get_data_path(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
    "\n",
    "y_train_t1 = load_pickle(get_data_path(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
    "y_valid_t1 = load_pickle(get_data_path(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
    "\n",
    "w_train = load_pickle(get_data_path(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
    "w_valid = load_pickle(get_data_path(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE))\n",
    "\n",
    "print(f\"[INFO] Loaded: X_train_sel={X_train_sel.shape} | X_valid_sel={X_valid_sel.shape}\")\n",
    "\n",
    "y_train = y_train_t1.astype(float).copy()\n",
    "y_valid = y_valid_t1.astype(float).copy()\n",
    "\n",
    "w_train_arr = np.asarray(w_train, dtype=float)\n",
    "w_valid_arr = np.asarray(w_valid, dtype=float)\n",
    "\n",
    "print(\"[INFO] Shapes (selected):\", X_train_sel.shape, X_valid_sel.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Align VALID data with Neural Networks (skip first lookback-1 rows)\n",
    "# -------------------------\n",
    "LOOKBACK = int(HPO_CFG[\"lookback\"])\n",
    "SKIP_ROWS = LOOKBACK - 1\n",
    "\n",
    "X_valid_sel = X_valid_sel.iloc[SKIP_ROWS:]\n",
    "y_valid = y_valid.iloc[SKIP_ROWS:]\n",
    "w_valid_arr = w_valid_arr[SKIP_ROWS:]\n",
    "\n",
    "print(f\"[INFO] Aligned VALID with lookback={LOOKBACK}: skipped first {SKIP_ROWS} rows\")\n",
    "print(f\"[INFO] VALID shape after alignment: {X_valid_sel.shape}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VALID split for ES vs SCORE (date-based)\n",
    "# -------------------------\n",
    "VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
    "VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
    "VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
    "VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
    "\n",
    "\n",
    "def split_valid_for_es_and_score(Xv: pd.DataFrame, yv: pd.Series, wv: np.ndarray,\n",
    "                                  valid_es_start: str, valid_es_end: str,\n",
    "                                  valid_score_start: str, valid_score_end: str):\n",
    "    \"\"\"Split validation into ES (early stopping) and SCORE (model selection) sets.\"\"\"\n",
    "    if not isinstance(Xv.index, pd.DatetimeIndex):\n",
    "        return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
    "\n",
    "    es_start = pd.Timestamp(valid_es_start)\n",
    "    es_end = pd.Timestamp(valid_es_end)\n",
    "    sc_start = pd.Timestamp(valid_score_start)\n",
    "    sc_end = pd.Timestamp(valid_score_end)\n",
    "    \n",
    "    mask_es = (Xv.index >= es_start) & (Xv.index <= es_end)\n",
    "    mask_sc = (Xv.index >= sc_start) & (Xv.index <= sc_end)\n",
    "\n",
    "    X_es = Xv.loc[mask_es]\n",
    "    y_es = yv.loc[mask_es]\n",
    "    X_sc = Xv.loc[mask_sc]\n",
    "    y_sc = yv.loc[mask_sc]\n",
    "\n",
    "    wv_s = pd.Series(wv, index=Xv.index)\n",
    "    w_es = wv_s.loc[mask_es].to_numpy(dtype=float)\n",
    "    w_sc = wv_s.loc[mask_sc].to_numpy(dtype=float)\n",
    "\n",
    "    mode_str = f\"VALID_ES={valid_es_start}:{valid_es_end} / VALID_SCORE={valid_score_start}:{valid_score_end}\"\n",
    "    \n",
    "    if len(X_es) > 0 and len(X_sc) > 0:\n",
    "        return (X_es, y_es, w_es), (X_sc, y_sc, w_sc), mode_str\n",
    "\n",
    "    return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n",
    "\n",
    "\n",
    "(X_valid_es, y_valid_es, w_valid_es), (X_valid_sc, y_valid_sc, w_valid_sc), valid_mode = split_valid_for_es_and_score(\n",
    "    X_valid_sel, y_valid, w_valid_arr,\n",
    "    VALID_ES_START, VALID_ES_END, VALID_SCORE_START, VALID_SCORE_END\n",
    ")\n",
    "\n",
    "print(\"[INFO] VALID mode:\", valid_mode)\n",
    "print(\"[INFO] VALID_ES shape:\", X_valid_es.shape, \"| VALID_SCORE shape:\", X_valid_sc.shape)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Weighted metrics\n",
    "# -------------------------\n",
    "# Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Search configuration from RUN_PARAMS\n",
    "# -------------------------\n",
    "RANDOM_SEED = int(HPO_CFG[\"random_state\"])\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "N_ESTIMATORS = int(HPO_CFG[\"n_estimators\"])\n",
    "EARLY_STOP = int(HPO_CFG[\"early_stopping_rounds\"])\n",
    "\n",
    "N_TRIALS_STAGE1 = int(HPO_CFG[\"n_trials_stage1\"])\n",
    "N_TRIALS_STAGE2 = int(HPO_CFG[\"n_trials_stage2\"])\n",
    "N_TRIALS_STAGE2_LOWLR = int(HPO_CFG[\"n_trials_stage2_lowlr\"])\n",
    "\n",
    "PRINT_EVERY_STAGE1 = int(HPO_CFG[\"print_every_stage1\"])\n",
    "PRINT_EVERY_STAGE2 = int(HPO_CFG[\"print_every_stage2\"])\n",
    "\n",
    "TIE_TOL = float(HPO_CFG[\"tie_tol\"])\n",
    "\n",
    "BASE_MODEL_CFG = dict(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    objective=HPO_CFG[\"objective\"],\n",
    "    eval_metric=HPO_CFG[\"eval_metric\"],\n",
    "    tree_method=HPO_CFG[\"tree_method\"],\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0,\n",
    "    early_stopping_rounds=EARLY_STOP,\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Parameter sampling (from RUN_PARAMS[\"hpo\"][\"sampling\"])\n",
    "# -------------------------\n",
    "SAMP_CFG = HPO_CFG[\"sampling\"]\n",
    "BROAD_CFG = SAMP_CFG[\"broad\"]\n",
    "REFINE_CFG = SAMP_CFG[\"refine\"]\n",
    "LOWLR_CFG = SAMP_CFG[\"refine_low_lr\"]\n",
    "\n",
    "\n",
    "def _clip(v, lo, hi) -> float:\n",
    "    return float(min(max(float(v), lo), hi))\n",
    "\n",
    "\n",
    "def _log_uniform(rng, lo_exp, hi_exp) -> float:\n",
    "    return float(10 ** rng.uniform(lo_exp, hi_exp))\n",
    "\n",
    "\n",
    "def sample_broad(rng, broad_cfg):\n",
    "    \"\"\"Broad sampling biased toward lower learning_rate for stability.\"\"\"\n",
    "    lr_high_prob = broad_cfg[\"lr_high_prob\"]\n",
    "    if rng.random() < (1 - lr_high_prob):\n",
    "        lr_lo, lr_hi = broad_cfg[\"lr_low\"]\n",
    "        lr = float(np.exp(rng.uniform(np.log(lr_lo), np.log(lr_hi))))\n",
    "    else:\n",
    "        lr_lo, lr_hi = broad_cfg[\"lr_high\"]\n",
    "        lr = float(rng.uniform(lr_lo, lr_hi))\n",
    "\n",
    "    md_lo, md_hi = broad_cfg[\"max_depth\"]\n",
    "    mcw_lo, mcw_hi = broad_cfg[\"min_child_weight_log\"]\n",
    "    ss_lo, ss_hi = broad_cfg[\"subsample\"]\n",
    "    cs_lo, cs_hi = broad_cfg[\"colsample_bytree\"]\n",
    "    gm_lo, gm_hi = broad_cfg[\"gamma\"]\n",
    "    ra_lo, ra_hi = broad_cfg[\"reg_alpha_exp\"]\n",
    "    rl_lo, rl_hi = broad_cfg[\"reg_lambda_exp\"]\n",
    "    mds_lo, mds_hi = broad_cfg[\"max_delta_step\"]\n",
    "\n",
    "    return {\n",
    "        \"max_depth\": int(rng.integers(md_lo, md_hi)),\n",
    "        \"learning_rate\": lr,\n",
    "        \"min_child_weight\": float(np.exp(rng.uniform(np.log(mcw_lo), np.log(mcw_hi)))),\n",
    "        \"subsample\": float(rng.uniform(ss_lo, ss_hi)),\n",
    "        \"colsample_bytree\": float(rng.uniform(cs_lo, cs_hi)),\n",
    "        \"gamma\": float(rng.uniform(gm_lo, gm_hi)),\n",
    "        \"reg_alpha\": _log_uniform(rng, ra_lo, ra_hi),\n",
    "        \"reg_lambda\": _log_uniform(rng, rl_lo, rl_hi),\n",
    "        \"max_delta_step\": float(rng.uniform(mds_lo, mds_hi)),\n",
    "    }\n",
    "\n",
    "\n",
    "def _log_jitter(rng, v, sigma=0.6, lo_exp=-12, hi_exp=2):\n",
    "    v = float(max(v, 1e-12))\n",
    "    logv = np.log10(v) + rng.normal(0.0, sigma)\n",
    "    logv = float(np.clip(logv, lo_exp, hi_exp))\n",
    "    return float(10 ** logv)\n",
    "\n",
    "\n",
    "def sample_refine(rng, best, refine_cfg):\n",
    "    \"\"\"Refine around best parameters.\"\"\"\n",
    "    md_delta = refine_cfg[\"max_depth_delta\"]\n",
    "    md_clip = refine_cfg[\"max_depth_clip\"]\n",
    "    lr_sigma = refine_cfg[\"lr_sigma\"]\n",
    "    lr_clip = refine_cfg[\"lr_clip\"]\n",
    "    mcw_sigma = refine_cfg[\"min_child_weight_sigma\"]\n",
    "    mcw_clip = refine_cfg[\"min_child_weight_clip\"]\n",
    "    ss_sigma = refine_cfg[\"subsample_sigma\"]\n",
    "    ss_clip = refine_cfg[\"subsample_clip\"]\n",
    "    cs_sigma = refine_cfg[\"colsample_sigma\"]\n",
    "    cs_clip = refine_cfg[\"colsample_clip\"]\n",
    "    gm_sigma = refine_cfg[\"gamma_sigma\"]\n",
    "    gm_clip = refine_cfg[\"gamma_clip\"]\n",
    "    ra_sigma = refine_cfg[\"reg_alpha_sigma\"]\n",
    "    ra_clip = refine_cfg[\"reg_alpha_exp_clip\"]\n",
    "    rl_sigma = refine_cfg[\"reg_lambda_sigma\"]\n",
    "    rl_clip = refine_cfg[\"reg_lambda_exp_clip\"]\n",
    "    mds_sigma = refine_cfg[\"max_delta_step_sigma\"]\n",
    "    mds_clip = refine_cfg[\"max_delta_step_clip\"]\n",
    "\n",
    "    return {\n",
    "        \"max_depth\": int(np.clip(int(best[\"max_depth\"] + rng.integers(md_delta[0], md_delta[1])), md_clip[0], md_clip[1])),\n",
    "        \"learning_rate\": _clip(best[\"learning_rate\"] * float(np.exp(rng.normal(0.0, lr_sigma))), lr_clip[0], lr_clip[1]),\n",
    "        \"min_child_weight\": _clip(best[\"min_child_weight\"] * float(np.exp(rng.normal(0.0, mcw_sigma))), mcw_clip[0], mcw_clip[1]),\n",
    "        \"subsample\": _clip(best[\"subsample\"] + rng.normal(0.0, ss_sigma), ss_clip[0], ss_clip[1]),\n",
    "        \"colsample_bytree\": _clip(best[\"colsample_bytree\"] + rng.normal(0.0, cs_sigma), cs_clip[0], cs_clip[1]),\n",
    "        \"gamma\": _clip(best[\"gamma\"] + rng.normal(0.0, gm_sigma), gm_clip[0], gm_clip[1]),\n",
    "        \"reg_alpha\": _log_jitter(rng, best[\"reg_alpha\"], sigma=ra_sigma, lo_exp=ra_clip[0], hi_exp=ra_clip[1]),\n",
    "        \"reg_lambda\": _log_jitter(rng, best[\"reg_lambda\"], sigma=rl_sigma, lo_exp=rl_clip[0], hi_exp=rl_clip[1]),\n",
    "        \"max_delta_step\": _clip(best[\"max_delta_step\"] + rng.normal(0.0, mds_sigma), mds_clip[0], mds_clip[1]),\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_refine_low_lr(rng, best, refine_cfg, lowlr_cfg):\n",
    "    \"\"\"Refine with lower learning rate for stability.\"\"\"\n",
    "    lr_shift = lowlr_cfg[\"lr_shift\"]\n",
    "    lr_clip = lowlr_cfg[\"lr_clip\"]\n",
    "    lr_sigma = refine_cfg[\"lr_sigma\"]\n",
    "    low_lr = _clip(best[\"learning_rate\"] * float(np.exp(rng.normal(lr_shift, lr_sigma))), lr_clip[0], lr_clip[1])\n",
    "    params = sample_refine(rng, best, refine_cfg)\n",
    "    params[\"learning_rate\"] = low_lr\n",
    "    return params\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Single trial runner\n",
    "# -------------------------\n",
    "def run_trial(trial_id: int, stage: str, params: dict):\n",
    "    \"\"\"Run a single HPO trial.\"\"\"\n",
    "    model = xgb.XGBRegressor(**BASE_MODEL_CFG, **params)\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.fit(\n",
    "        X_train_sel, y_train,\n",
    "        sample_weight=w_train_arr,\n",
    "        eval_set=[(X_valid_es, y_valid_es)],\n",
    "        sample_weight_eval_set=[w_valid_es],\n",
    "        verbose=False\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    best_iter = getattr(model, \"best_iteration\", None)\n",
    "    best_score = getattr(model, \"best_score\", None)\n",
    "\n",
    "    pred_es = model.predict(X_valid_es)\n",
    "    es_wrmse = w_rmse(y_valid_es, pred_es, w_valid_es)\n",
    "\n",
    "    pred_sc = model.predict(X_valid_sc)\n",
    "    sc_wrmse = w_rmse(y_valid_sc, pred_sc, w_valid_sc)\n",
    "\n",
    "    row = {\n",
    "        \"stage\": stage,\n",
    "        \"trial\": int(trial_id),\n",
    "        \"valid_sc_wrmse\": sc_wrmse,\n",
    "        \"valid_sc_wmae\": w_mae(y_valid_sc, pred_sc, w_valid_sc),\n",
    "        \"valid_sc_diracc\": dir_acc(y_valid_sc, pred_sc),\n",
    "        \"best_iteration\": None if best_iter is None else int(best_iter),\n",
    "        \"best_score_rmse_eval_es\": None if best_score is None else float(best_score),\n",
    "        \"valid_es_wrmse_explicit\": float(es_wrmse),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        **params,\n",
    "    }\n",
    "    return model, row\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# HPO loop (Stage 1 + Stage 2 + Stage 2 LOW-LR)\n",
    "# -------------------------\n",
    "results = []\n",
    "best_model = None\n",
    "best_row = None\n",
    "best_valid = np.inf\n",
    "\n",
    "best_params_keys = [\n",
    "    \"max_depth\", \"learning_rate\", \"min_child_weight\", \"subsample\", \"colsample_bytree\",\n",
    "    \"gamma\", \"reg_alpha\", \"reg_lambda\", \"max_delta_step\"\n",
    "]\n",
    "\n",
    "\n",
    "def is_better(row, best_row, tie_tol):\n",
    "    \"\"\"Check if row is better than best_row (lower wRMSE, tie-break by higher iteration).\"\"\"\n",
    "    if best_row is None:\n",
    "        return True\n",
    "\n",
    "    a = float(row[\"valid_sc_wrmse\"])\n",
    "    b = float(best_row[\"valid_sc_wrmse\"])\n",
    "\n",
    "    if a < (b - tie_tol):\n",
    "        return True\n",
    "    if abs(a - b) <= tie_tol:\n",
    "        ai = -1 if row[\"best_iteration\"] is None else int(row[\"best_iteration\"])\n",
    "        bi = -1 if best_row[\"best_iteration\"] is None else int(best_row[\"best_iteration\"])\n",
    "        return ai > bi\n",
    "    return False\n",
    "\n",
    "\n",
    "print(f\"\\n[INFO] HPO start | VALID mode: {valid_mode}\")\n",
    "print(f\"[INFO] Stage1 trials={N_TRIALS_STAGE1} | Stage2 trials={N_TRIALS_STAGE2} | Stage2 LOW-LR={N_TRIALS_STAGE2_LOWLR}\")\n",
    "print(f\"[INFO] n_estimators={N_ESTIMATORS} | early_stop={EARLY_STOP}\")\n",
    "\n",
    "# Stage 1 (broad)\n",
    "for i in range(N_TRIALS_STAGE1):\n",
    "    params = sample_broad(rng, BROAD_CFG)\n",
    "    model_i, row_i = run_trial(trial_id=i, stage=\"STAGE1_BROAD\", params=params)\n",
    "    results.append(row_i)\n",
    "\n",
    "    if is_better(row_i, best_row, TIE_TOL):\n",
    "        best_model = model_i\n",
    "        best_row = row_i\n",
    "        best_valid = float(best_row[\"valid_sc_wrmse\"])\n",
    "\n",
    "    if (i + 1) % PRINT_EVERY_STAGE1 == 0 or i == 0:\n",
    "        print(f\"[INFO] S1 {i:04d} | sc_wrmse={row_i['valid_sc_wrmse']:.6f} | best={best_valid:.6f} | best_iter={row_i['best_iteration']}\")\n",
    "\n",
    "best_params_stage1 = {k: best_row[k] for k in best_params_keys}\n",
    "\n",
    "# Stage 2 (refine around best)\n",
    "for j in range(N_TRIALS_STAGE2):\n",
    "    params = sample_refine(rng, best_params_stage1, REFINE_CFG)\n",
    "    model_j, row_j = run_trial(trial_id=j, stage=\"STAGE2_REFINE\", params=params)\n",
    "    results.append(row_j)\n",
    "\n",
    "    if is_better(row_j, best_row, TIE_TOL):\n",
    "        best_model = model_j\n",
    "        best_row = row_j\n",
    "        best_valid = float(best_row[\"valid_sc_wrmse\"])\n",
    "\n",
    "    if (j + 1) % PRINT_EVERY_STAGE2 == 0 or j == 0:\n",
    "        print(f\"[INFO] S2 {j:04d} | sc_wrmse={row_j['valid_sc_wrmse']:.6f} | best={best_valid:.6f} | best_iter={row_j['best_iteration']}\")\n",
    "\n",
    "# Stage 2B (LOW-LR refine branch)\n",
    "for k in range(N_TRIALS_STAGE2_LOWLR):\n",
    "    params = sample_refine_low_lr(rng, best_params_stage1, REFINE_CFG, LOWLR_CFG)\n",
    "    model_k, row_k = run_trial(trial_id=k, stage=\"STAGE2_LOWLR\", params=params)\n",
    "    results.append(row_k)\n",
    "\n",
    "    if is_better(row_k, best_row, TIE_TOL):\n",
    "        best_model = model_k\n",
    "        best_row = row_k\n",
    "        best_valid = float(best_row[\"valid_sc_wrmse\"])\n",
    "\n",
    "    if (k + 1) % PRINT_EVERY_STAGE2 == 0 or k == 0:\n",
    "        print(f\"[INFO] S2L {k:04d} | sc_wrmse={row_k['valid_sc_wrmse']:.6f} | best={best_valid:.6f} | best_iter={row_k['best_iteration']}\")\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"valid_sc_wrmse\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[INFO] Top 10 trials by VALID_SCORE wRMSE:\")\n",
    "display(res_df.head(10))\n",
    "\n",
    "print(\"\\n[INFO] BEST summary:\")\n",
    "best_summary = {\n",
    "    \"valid_mode\": valid_mode,\n",
    "    \"n_features\": int(len(selected_features)),\n",
    "    \"n_trials_total\": int(len(res_df)),\n",
    "    \"best_valid_sc_wrmse\": float(best_row[\"valid_sc_wrmse\"]),\n",
    "    \"best_valid_sc_wmae\": float(best_row[\"valid_sc_wmae\"]),\n",
    "    \"best_valid_sc_diracc\": float(best_row[\"valid_sc_diracc\"]),\n",
    "    \"best_iteration\": best_row[\"best_iteration\"],\n",
    "}\n",
    "print(pd.Series(best_summary))\n",
    "\n",
    "best_params = {k: best_row[k] for k in best_params_keys}\n",
    "best_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n",
    "for k in best_params_keys:\n",
    "    if k != \"max_depth\":\n",
    "        best_params[k] = float(best_params[k])\n",
    "\n",
    "print(\"\\n[INFO] BEST params:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Save outputs (LOCAL + DRIVE)\n",
    "# -------------------------\n",
    "BEST_MODEL_PATH = MS_OUT_LOCAL / \"best_model_xgb_reg_t1.json\"\n",
    "BEST_PARAMS_TXT = MS_OUT_LOCAL / \"best_params_xgb_reg_t1.txt\"\n",
    "BEST_PARAMS_PKL = MS_OUT_LOCAL / \"best_params_xgb_reg_t1.pkl\"\n",
    "\n",
    "# Save model\n",
    "best_model.get_booster().save_model(str(BEST_MODEL_PATH))\n",
    "copy_file(BEST_MODEL_PATH, MS_OUT_DRIVE / BEST_MODEL_PATH.name)\n",
    "\n",
    "# Save params as text\n",
    "lines = [\n",
    "    f\"valid_mode={valid_mode}\",\n",
    "    f\"best_valid_sc_wrmse={best_row['valid_sc_wrmse']}\",\n",
    "    f\"best_valid_sc_wmae={best_row['valid_sc_wmae']}\",\n",
    "    f\"best_valid_sc_diracc={best_row['valid_sc_diracc']}\",\n",
    "    f\"best_iteration={best_row['best_iteration']}\",\n",
    "    f\"n_estimators={N_ESTIMATORS}\",\n",
    "    f\"early_stop={EARLY_STOP}\",\n",
    "    f\"trials_stage1={N_TRIALS_STAGE1}\",\n",
    "    f\"trials_stage2={N_TRIALS_STAGE2}\",\n",
    "    f\"trials_stage2_lowlr={N_TRIALS_STAGE2_LOWLR}\",\n",
    "    f\"random_seed={RANDOM_SEED}\",\n",
    "]\n",
    "for k in best_params_keys:\n",
    "    lines.append(f\"{k}={best_params[k]}\")\n",
    "\n",
    "BEST_PARAMS_TXT.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "copy_file(BEST_PARAMS_TXT, MS_OUT_DRIVE / BEST_PARAMS_TXT.name)\n",
    "\n",
    "# Save params as pickle\n",
    "save_pickle(best_params, BEST_PARAMS_PKL)\n",
    "copy_file(BEST_PARAMS_PKL, MS_OUT_DRIVE / BEST_PARAMS_PKL.name)\n",
    "\n",
    "# Save ALSO to persistent location (for future runs without HPO)\n",
    "save_pickle(best_params, DATA_DIRS_LOCAL[\"processed\"] / \"best_params_xgb_reg_t1.pkl\")\n",
    "copy_file(DATA_DIRS_LOCAL[\"processed\"] / \"best_params_xgb_reg_t1.pkl\", \n",
    "          DATA_DIRS_DRIVE[\"processed\"] / \"best_params_xgb_reg_t1.pkl\")\n",
    "print(\"  - best_params_xgb_reg_t1.pkl (persistent)\")\n",
    "\n",
    "print(\"\\n[OK] Saved HPO outputs:\")\n",
    "print(\"  -\", BEST_MODEL_PATH.name)\n",
    "print(\"  -\", BEST_PARAMS_TXT.name)\n",
    "print(\"  -\", BEST_PARAMS_PKL.name)\n",
    "\n",
    "print(\"[OK] BLOCK 24 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 7: XGBoost Final Model\n",
    "\n",
    "**Train and evaluate best XGBoost model**\n",
    "\n",
    "**Block:** 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a08b4",
   "metadata": {},
   "source": [
    "## BLOCK 25 \u2014 FINAL MODEL TRAIN + BASELINE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed32fcb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n# Config\nHPO_CFG = RUN_PARAMS[\"hpo\"]\n\n# Directories\n# Fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE\nRUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])  # runs/RUN_ID/processed (LOCAL)\nFALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]  # data/processed (LOCAL)\nRUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])  # runs/RUN_ID/processed (DRIVE)\nFALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]  # data/processed (DRIVE)\n\nRUN_MS_LOCAL = Path(LOCAL_PATHS[\"ms_dir\"])  # runs/RUN_ID/model_selection (LOCAL)\nRUN_MS_DRIVE = Path(DRIVE_PATHS[\"ms_dir\"])  # runs/RUN_ID/model_selection (DRIVE)\nMODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\nMODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n\nprint(\"[INFO] Final Model output dirs:\")\nprint(\"  - LOCAL:\", MODELS_OUT_LOCAL)\nprint(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n\n# -------------------------\n# 1. Load data from snapshot (data/processed/)\n# -------------------------\nprint(\"\\n[INFO] Loading data (RUN_ID -> data/processed fallback):\")\n\nX_train_sel = load_with_fallback(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\nX_valid_sel = load_with_fallback(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\nX_test_sel = load_with_fallback(\"X_test_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n\ny_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\ny_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\ny_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n\nw_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\nw_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\nw_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n\nbest_params = load_with_fallback(\"best_params_xgb_reg_t1.pkl\", RUN_MS_LOCAL, FALLBACK_PROC_LOCAL, RUN_MS_DRIVE, FALLBACK_PROC_DRIVE)\n\nprint(f\"[INFO] Loaded: X_train_sel={X_train_sel.shape} | X_valid_sel={X_valid_sel.shape} | X_test_sel={X_test_sel.shape}\")\n\ny_train = y_train_t1.astype(float).copy()\ny_valid = y_valid_t1.astype(float).copy()\ny_test = y_test_t1.astype(float).copy()\n\nw_train_arr = np.asarray(w_train, dtype=float)\nw_valid_arr = np.asarray(w_valid, dtype=float)\nw_test_arr = np.asarray(w_test, dtype=float)\n\n# -------------------------\n# Align VALID + TEST data with Neural Networks (skip first lookback-1 rows)\n# This ensures metrics are comparable across XGBoost and Neural models\n# -------------------------\nLOOKBACK = int(HPO_CFG[\"lookback\"])\nSKIP_ROWS = LOOKBACK - 1\n\n# Align VALID\nX_valid_sel = X_valid_sel.iloc[SKIP_ROWS:]\ny_valid = y_valid.iloc[SKIP_ROWS:]\nw_valid_arr = w_valid_arr[SKIP_ROWS:]\n\n# Align TEST\nX_test_sel = X_test_sel.iloc[SKIP_ROWS:]\ny_test = y_test.iloc[SKIP_ROWS:]\nw_test_arr = w_test_arr[SKIP_ROWS:]\n\nprint(f\"[INFO] Aligned VALID+TEST with lookback={LOOKBACK}: skipped first {SKIP_ROWS} rows\")\nprint(f\"[INFO] VALID shapes after alignment: X={X_valid_sel.shape}\")\nprint(f\"[INFO] TEST shapes after alignment: X={X_test_sel.shape}\")\n\n\n# -------------------------\n# 2. Split VALID into ES and SCORE (date-based)\n# -------------------------\nVALID_ES_START = HPO_CFG[\"valid_es_start\"]\nVALID_ES_END = HPO_CFG[\"valid_es_end\"]\nVALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\nVALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n\n\ndef split_valid_es_score(Xv, yv, wv):\n    \"\"\"Split validation into ES (early stopping) and SCORE (model selection) sets.\"\"\"\n    if not isinstance(Xv.index, pd.DatetimeIndex):\n        return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n\n    wv_s = pd.Series(wv, index=Xv.index)\n    \n    es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n    sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n    \n    m_es = (Xv.index >= es_start) & (Xv.index <= es_end)\n    m_sc = (Xv.index >= sc_start) & (Xv.index <= sc_end)\n    mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n\n    if m_es.sum() > 0 and m_sc.sum() > 0:\n        return (Xv.loc[m_es], yv.loc[m_es], wv_s.loc[m_es].to_numpy(float)), \\\n               (Xv.loc[m_sc], yv.loc[m_sc], wv_s.loc[m_sc].to_numpy(float)), \\\n               mode_str\n\n    return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n\n\n(X_es, y_es, w_es), (X_sc, y_sc, w_sc), valid_mode = split_valid_es_score(\n    X_valid_sel, y_valid, w_valid_arr\n)\n\nprint(f\"[INFO] VALID mode: {valid_mode}\")\nprint(f\"[INFO] VALID_ES: {X_es.shape} | VALID_SCORE: {X_sc.shape}\")\n\n\n# -------------------------\n# 3. Metrics functions\n# -------------------------\n# Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n\n\n# -------------------------\n# 4. BASELINES (Zero + Naive)\n# -------------------------\nprint(\"\\n[INFO] Computing BASELINES...\")\n\nbaseline_results = []\n\n# ----- BASELINE ZERO (predict 0) -----\nprint(\"  [1] BASELINE_ZERO (predict 0):\")\n\n# Zero baseline on VALID_SCORE\npred_zero_sc = np.zeros(len(y_sc), dtype=float)\nbaseline_results.append({\n    \"model\": \"BASELINE_ZERO\",\n    \"split\": \"VALID_SCORE\",\n    \"n\": int(len(y_sc)),\n    \"wRMSE\": w_rmse(y_sc, pred_zero_sc, w_sc),\n    \"wMAE\": w_mae(y_sc, pred_zero_sc, w_sc),\n    \"DirAcc\": dir_acc(y_sc, pred_zero_sc),\n})\n\n# Zero baseline on TEST\npred_zero_test = np.zeros(len(y_test), dtype=float)\nbaseline_results.append({\n    \"model\": \"BASELINE_ZERO\",\n    \"split\": \"TEST\",\n    \"n\": int(len(y_test)),\n    \"wRMSE\": w_rmse(y_test, pred_zero_test, w_test_arr),\n    \"wMAE\": w_mae(y_test, pred_zero_test, w_test_arr),\n    \"DirAcc\": dir_acc(y_test, pred_zero_test),\n})\n\nfor r in baseline_results[-2:]:\n    print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n\n# ----- BASELINE NAIVE (last-value forecast) -----\n# Predict: y[t] = y[t-1] (tomorrow's return = today's return)\nprint(\"  [2] BASELINE_NAIVE (last-value forecast):\")\n\n# For naive forecast, we need the previous day's actual value\n# y_sc and y_test are already aligned - we need to load full y to get lag\n\n# Try to load full y for naive baseline\ntry:\n    y_full = load_with_fallback(\"y_full.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    \n    # Get indices for valid_score and test\n    y_full_arr = np.asarray(y_full, dtype=float)\n    \n    # Naive: predict previous value\n    # For valid_score period, we need the value from day before\n    # Assuming y_sc starts where y_valid ends (after ES period)\n    # We'll use shifted y values\n    \n    # Simple approach: shift y by 1 within each split\n    pred_naive_sc = np.roll(y_sc, 1)\n    pred_naive_sc[0] = 0  # First value has no previous, use 0\n    \n    pred_naive_test = np.roll(y_test, 1)\n    pred_naive_test[0] = 0  # First value has no previous, use 0\n    \n    naive_available = True\nexcept:\n    # If y_full not available, use simple shift within splits\n    pred_naive_sc = np.roll(np.asarray(y_sc), 1)\n    pred_naive_sc[0] = 0\n    \n    pred_naive_test = np.roll(np.asarray(y_test), 1)\n    pred_naive_test[0] = 0\n    \n    naive_available = True\n\nif naive_available:\n    # Naive baseline on VALID_SCORE\n    baseline_results.append({\n        \"model\": \"BASELINE_NAIVE\",\n        \"split\": \"VALID_SCORE\",\n        \"n\": int(len(y_sc)),\n        \"wRMSE\": w_rmse(y_sc, pred_naive_sc, w_sc),\n        \"wMAE\": w_mae(y_sc, pred_naive_sc, w_sc),\n        \"DirAcc\": dir_acc(y_sc, pred_naive_sc),\n    })\n    \n    # Naive baseline on TEST\n    baseline_results.append({\n        \"model\": \"BASELINE_NAIVE\",\n        \"split\": \"TEST\",\n        \"n\": int(len(y_test)),\n        \"wRMSE\": w_rmse(y_test, pred_naive_test, w_test_arr),\n        \"wMAE\": w_mae(y_test, pred_naive_test, w_test_arr),\n        \"DirAcc\": dir_acc(y_test, pred_naive_test),\n    })\n    \n    for r in baseline_results[-2:]:\n        print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n\nprint(\"\\n[INFO] BASELINE Summary:\")\nfor r in baseline_results:\n    print(f\"  - {r['model']:15} | {r['split']:12} | wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n\n\n# -------------------------\n# 5. Train FINAL MODEL (early stop on VALID_ES)\n# -------------------------\nprint(\"\\n[INFO] Training FINAL MODEL...\")\n\nN_ESTIMATORS = int(HPO_CFG[\"n_estimators\"])\nEARLY_STOP = int(HPO_CFG[\"early_stopping_rounds\"])\nRANDOM_SEED = int(HPO_CFG[\"random_state\"])\n\n# Ensure correct types in best_params\nbest_params = dict(best_params)\nbest_params[\"max_depth\"] = int(best_params[\"max_depth\"])\nfor k in [\"learning_rate\", \"min_child_weight\", \"subsample\", \"colsample_bytree\",\n          \"gamma\", \"reg_alpha\", \"reg_lambda\", \"max_delta_step\"]:\n    if k in best_params:\n        best_params[k] = float(best_params[k])\n\n# XGBoost settings from config\nOBJECTIVE = HPO_CFG[\"objective\"]\nEVAL_METRIC = HPO_CFG[\"eval_metric\"]\nTREE_METHOD = HPO_CFG[\"tree_method\"]\n\nmodel = xgb.XGBRegressor(\n    n_estimators=N_ESTIMATORS,\n    objective=OBJECTIVE,\n    eval_metric=EVAL_METRIC,\n    tree_method=TREE_METHOD,\n    random_state=RANDOM_SEED,\n    n_jobs=-1,\n    verbosity=0,\n    early_stopping_rounds=EARLY_STOP,\n    **best_params\n)\n\nmodel.fit(\n    X_train_sel, y_train,\n    sample_weight=w_train_arr,\n    eval_set=[(X_es, y_es)],\n    sample_weight_eval_set=[w_es],\n    verbose=False\n)\n\nbest_iter = getattr(model, \"best_iteration\", None)\nbest_score = getattr(model, \"best_score\", None)\nprint(f\"[INFO] Training complete. best_iteration={best_iter} | best_score(ES)={best_score}\")\n\n\n# -------------------------\n# 6. Evaluate FINAL MODEL on VALID_SCORE + TEST\n# -------------------------\nprint(\"\\n[INFO] Evaluating FINAL MODEL...\")\n\nmodel_results = []\n\n# Model on VALID_SCORE\npred_model_sc = model.predict(X_sc)\nmodel_results.append({\n    \"model\": \"FINAL_XGB\",\n    \"split\": \"VALID_SCORE\",\n    \"n\": int(len(y_sc)),\n    \"wRMSE\": w_rmse(y_sc, pred_model_sc, w_sc),\n    \"wMAE\": w_mae(y_sc, pred_model_sc, w_sc),\n    \"DirAcc\": dir_acc(y_sc, pred_model_sc),\n})\n\n# Model on TEST\npred_model_test = model.predict(X_test_sel)\nmodel_results.append({\n    \"model\": \"FINAL_XGB\",\n    \"split\": \"TEST\",\n    \"n\": int(len(y_test)),\n    \"wRMSE\": w_rmse(y_test, pred_model_test, w_test_arr),\n    \"wMAE\": w_mae(y_test, pred_model_test, w_test_arr),\n    \"DirAcc\": dir_acc(y_test, pred_model_test),\n})\n\nprint(\"[INFO] FINAL MODEL results:\")\nfor r in model_results:\n    print(f\"  - {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n\n\n# -------------------------\n# 7. Comparison: BASELINE vs MODEL\n# -------------------------\nall_results = baseline_results + model_results\nmetrics_df = pd.DataFrame(all_results)\n\n# Add improvement column (compared to BASELINE_ZERO)\nbaseline_zero_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_ZERO\"}\nmetrics_df[\"wRMSE_vs_zero\"] = metrics_df.apply(\n    lambda row: baseline_zero_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n)\n\n# Add improvement vs BASELINE_NAIVE\nbaseline_naive_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_NAIVE\"}\nmetrics_df[\"wRMSE_vs_naive\"] = metrics_df.apply(\n    lambda row: baseline_naive_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n)\n\nprint(\"\\n[INFO] BASELINE vs FINAL MODEL comparison:\")\ndisplay(metrics_df)\n\n\n# -------------------------\n# 8. Build predictions DataFrames\n# -------------------------\npreds_valid_score_df = pd.DataFrame({\n    \"date\": X_sc.index,\n    \"actual\": y_sc.values,\n    \"baseline_zero\": pred_zero_sc,\n    \"baseline_naive\": pred_naive_sc,\n    \"predicted\": pred_model_sc,\n    \"sample_weight\": w_sc,\n}).reset_index(drop=True)\n\npreds_test_df = pd.DataFrame({\n    \"date\": X_test_sel.index,\n    \"actual\": y_test.values,\n    \"baseline_zero\": pred_zero_test,\n    \"baseline_naive\": pred_naive_test,\n    \"predicted\": pred_model_test,\n    \"sample_weight\": w_test_arr,\n}).reset_index(drop=True)\n\n\n# -------------------------\n# 9. Save artifacts (LOCAL + DRIVE)\n# -------------------------\n# Model\n# Save model as JSON\nmodel_path_local = MODELS_OUT_LOCAL / \"final_model_xgb.json\"\nmodel.get_booster().save_model(str(model_path_local))\ncopy_file(model_path_local, MODELS_OUT_DRIVE / model_path_local.name)\n\n# Also save as pickle for easier loading\nmodel_pkl_local = MODELS_OUT_LOCAL / \"final_model_xgb.pkl\"\nsave_pickle(model, model_pkl_local)\ncopy_file(model_pkl_local, MODELS_OUT_DRIVE / model_pkl_local.name)\n\n# Metrics\nmetrics_path_local = MODELS_OUT_LOCAL / \"final_metrics.csv\"\nmetrics_df.to_csv(metrics_path_local, index=False)\ncopy_file(metrics_path_local, MODELS_OUT_DRIVE / metrics_path_local.name)\n\n# Baseline results JSON (for CLI summary)\nOUTPUTS_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"outputs_dir\"]))\nOUTPUTS_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"outputs_dir\"]))\n\nfor baseline_row in baseline_results:\n    if baseline_row[\"split\"] == \"TEST\":\n        baseline_json = {\n            \"model\": baseline_row[\"model\"],\n            \"test_wrmse\": baseline_row[\"wRMSE\"],\n            \"test_wmae\": baseline_row.get(\"wMAE\"),\n            \"test_diracc\": baseline_row[\"DirAcc\"],\n        }\n        baseline_name = baseline_row[\"model\"].lower()\n        save_json(baseline_json, OUTPUTS_LOCAL / f\"{baseline_name}_results.json\")\n        save_json(baseline_json, OUTPUTS_DRIVE / f\"{baseline_name}_results.json\")\n\n# Predictions -> predictions/xgb/\nPRED_XGB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / \"xgb\")\nPRED_XGB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / \"xgb\")\n\npreds_valid_path_local = PRED_XGB_LOCAL / \"predictions_valid.csv\"\npreds_valid_score_df.to_csv(preds_valid_path_local, index=False)\ncopy_file(preds_valid_path_local, PRED_XGB_DRIVE / preds_valid_path_local.name)\n\npreds_test_path_local = PRED_XGB_LOCAL / \"predictions_test.csv\"\npreds_test_df.to_csv(preds_test_path_local, index=False)\ncopy_file(preds_test_path_local, PRED_XGB_DRIVE / preds_test_path_local.name)\n\nprint(\"\\n[OK] Saved FINAL MODEL artifacts:\")\nprint(\"  -\", model_path_local.name)\nprint(\"  -\", metrics_path_local.name)\nprint(\"  - predictions/xgb/\", preds_valid_path_local.name)\nprint(\"  - predictions/xgb/\", preds_test_path_local.name)\n\n\n\n# -------------------------\n# Tomorrow Prediction + Plot\n# -------------------------\nPLOT_CFG = RUN_PARAMS[\"plot\"]\nN_PLOT = int(PLOT_CFG[\"n_plot\"])\nFIGSIZE = tuple(PLOT_CFG[\"figsize\"])\nDPI = int(PLOT_CFG[\"dpi\"])\n\n# Predict tomorrow\nlast_date = X_test_sel.index[-1]\nX_last = X_test_sel.iloc[[-1]]\npred_tomorrow = float(model.predict(X_last)[0])\npred_tomorrow_date = last_date + pd.Timedelta(days=1)\n\n# Save tomorrow prediction\npred_tomorrow_df = pd.DataFrame([{\n    \"feature_set\": \"xgb_selected\",\n    \"last_data_date\": last_date,\n    \"predicted_for\": \"next_trading_day\",\n    \"pred_logret\": pred_tomorrow,\n    \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n}])\npred_tomorrow_df.to_csv(PRED_XGB_LOCAL / \"tomorrow.csv\", index=False)\ncopy_file(PRED_XGB_LOCAL / \"tomorrow.csv\", PRED_XGB_DRIVE / \"tomorrow.csv\")\n\n# Create backtest dataframe\nhist_df = pd.DataFrame({\n    \"date\": X_test_sel.index,\n    \"actual\": y_test.values,\n    \"y_pred\": pred_model_test,\n}).set_index(\"date\")\n\nhist_tail = hist_df.tail(N_PLOT).copy()\nhist_tail.to_csv(PRED_XGB_LOCAL / \"backtest.csv\")\ncopy_file(PRED_XGB_LOCAL / \"backtest.csv\", PRED_XGB_DRIVE / \"backtest.csv\")\n\n# Plot\nfig, ax = plt.subplots(figsize=FIGSIZE)\nax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\nax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted (y_pred)\")\nax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\nax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\nax.set_title(f\"XGBoost Predictions \u2014 xgb_selected \u2014 last {len(hist_tail)} days + tomorrow\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Log Return\")\nax.legend(loc=\"upper right\")\nplt.tight_layout()\nplt.savefig(PRED_XGB_LOCAL / \"plot.png\", dpi=DPI)\ncopy_file(PRED_XGB_LOCAL / \"plot.png\", PRED_XGB_DRIVE / \"plot.png\")\nplt.close(fig)\n\nprint(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\nprint(f\"[OK] Saved: predictions/xgb/ (tomorrow.csv, backtest.csv, plot.png)\")\n\n\n# -------------------------\n# SHAP Analysis (config-based)\n# -------------------------\nSHAP_CFG = RUN_PARAMS.get(\"shap\", {})\nSHAP_ENABLED = bool(SHAP_CFG.get(\"enabled\", True))\n\nif SHAP_ENABLED:\n    print(\"\\n[INFO] Computing SHAP values...\")\n    \n    try:\n        import shap\n        \n        # Config\n        SHAP_MAX_DISPLAY = int(SHAP_CFG.get(\"max_display\", 20))\n        SHAP_FIGSIZE = tuple(SHAP_CFG.get(\"figsize\", [10, 8]))\n        SHAP_BAR = bool(SHAP_CFG.get(\"plot_type_bar\", True))\n        SHAP_BEESWARM = bool(SHAP_CFG.get(\"plot_type_beeswarm\", True))\n        SHAP_SAVE_VALUES = bool(SHAP_CFG.get(\"save_values\", True))\n        \n        # Create explainer (TreeExplainer is fast for XGBoost)\n        explainer = shap.TreeExplainer(model)\n        \n        # Compute SHAP values on TEST set\n        shap_values = explainer.shap_values(X_test_sel)\n        \n        # Save SHAP values as DataFrame\n        if SHAP_SAVE_VALUES:\n            shap_df = pd.DataFrame(shap_values, columns=X_test_sel.columns, index=X_test_sel.index)\n            shap_df.to_csv(PRED_XGB_LOCAL / \"shap_values_test.csv\")\n            copy_file(PRED_XGB_LOCAL / \"shap_values_test.csv\", PRED_XGB_DRIVE / \"shap_values_test.csv\")\n        \n        # Feature importance (mean |SHAP|)\n        shap_importance = pd.DataFrame({\n            \"feature\": X_test_sel.columns,\n            \"mean_abs_shap\": np.abs(shap_values).mean(axis=0)\n        }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n        shap_importance.to_csv(PRED_XGB_LOCAL / \"shap_feature_importance.csv\", index=False)\n        copy_file(PRED_XGB_LOCAL / \"shap_feature_importance.csv\", PRED_XGB_DRIVE / \"shap_feature_importance.csv\")\n        \n        print(f\"[INFO] Top 10 features by SHAP importance:\")\n        print(shap_importance.head(10).to_string(index=False))\n        \n        # SHAP Summary Plot (bar)\n        if SHAP_BAR:\n            fig_bar, ax_bar = plt.subplots(figsize=SHAP_FIGSIZE)\n            shap.summary_plot(shap_values, X_test_sel, plot_type=\"bar\", show=False, max_display=SHAP_MAX_DISPLAY)\n            plt.tight_layout()\n            plt.savefig(PRED_XGB_LOCAL / \"shap_summary_bar.png\", dpi=DPI, bbox_inches=\"tight\")\n            copy_file(PRED_XGB_LOCAL / \"shap_summary_bar.png\", PRED_XGB_DRIVE / \"shap_summary_bar.png\")\n            plt.close()\n        \n        # SHAP Summary Plot (beeswarm)\n        if SHAP_BEESWARM:\n            fig_bee, ax_bee = plt.subplots(figsize=SHAP_FIGSIZE)\n            shap.summary_plot(shap_values, X_test_sel, show=False, max_display=SHAP_MAX_DISPLAY)\n            plt.tight_layout()\n            plt.savefig(PRED_XGB_LOCAL / \"shap_summary_beeswarm.png\", dpi=DPI, bbox_inches=\"tight\")\n            copy_file(PRED_XGB_LOCAL / \"shap_summary_beeswarm.png\", PRED_XGB_DRIVE / \"shap_summary_beeswarm.png\")\n            plt.close()\n        \n        print(\"[OK] Saved SHAP analysis:\")\n        if SHAP_SAVE_VALUES:\n            print(\"  - shap_values_test.csv\")\n        print(\"  - shap_feature_importance.csv\")\n        if SHAP_BAR:\n            print(\"  - shap_summary_bar.png\")\n        if SHAP_BEESWARM:\n            print(\"  - shap_summary_beeswarm.png\")\n        \n    except ImportError:\n        print(\"[WARN] shap not installed. Run: pip install shap\")\n    except Exception as e:\n        print(f\"[WARN] SHAP analysis failed: {e}\")\nelse:\n    print(\"[INFO] SHAP analysis disabled in config.\")\n\nprint(\"[OK] BLOCK 25 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 7B: LightGBM Model\n",
    "\n",
    "**Train and evaluate LightGBM model (similar to XGBoost)**\n",
    "\n",
    "**Blocks:** 25B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 25B \u2014 LIGHTGBM FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n# LIGHTGBM FINAL MODEL (identical structure to XGBoost)\n# ============================================================\ntry:\n    import lightgbm as lgb\n    LGB_AVAILABLE = True\n    print(\"[OK] LightGBM available\")\nexcept ImportError:\n    LGB_AVAILABLE = False\n    print(\"[SKIP] LightGBM not installed\")\n\nif LGB_AVAILABLE:\n    # Config (use lgb_hpo if exists, fallback to hpo)\n    HPO_CFG = RUN_PARAMS.get(\"lgb_hpo\", RUN_PARAMS[\"hpo\"])\n    \n    # Directories\n    RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n    FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n    RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n    FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n    \n    RUN_MS_LOCAL = Path(LOCAL_PATHS[\"ms_dir\"])\n    RUN_MS_DRIVE = Path(DRIVE_PATHS[\"ms_dir\"])\n    MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n    MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n    \n    print(\"[INFO] LightGBM Final Model output dirs:\")\n    print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n    print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n    \n    # -------------------------\n    # 1. Load data from snapshot\n    # -------------------------\n    print(\"\\n[INFO] Loading data (RUN_ID -> data/processed fallback):\")\n    \n    X_train_sel = load_with_fallback(\"X_train_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n    X_valid_sel = load_with_fallback(\"X_valid_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n    X_test_sel = load_with_fallback(\"X_test_xgb_selected.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n    \n    y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    \n    w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n    \n    # Load best_params\n    try:\n        best_params = load_with_fallback(\"best_params_lgb_reg_t1.pkl\", RUN_MS_LOCAL, FALLBACK_PROC_LOCAL, RUN_MS_DRIVE, FALLBACK_PROC_DRIVE)\n        print(\"[INFO] Loaded best_params_lgb from file\")\n    except FileNotFoundError:\n        LGB_FS_CFG = RUN_PARAMS.get(\"lgb_fs\", RUN_PARAMS[\"xgb_fs\"])\n        best_params = {\n            \"max_depth\": int(LGB_FS_CFG.get(\"max_depth\", 3)),\n            \"num_leaves\": int(LGB_FS_CFG.get(\"num_leaves\", 31)),\n            \"learning_rate\": float(LGB_FS_CFG.get(\"learning_rate\", 0.05)),\n            \"min_child_samples\": int(LGB_FS_CFG.get(\"min_child_samples\", 20)),\n            \"subsample\": float(LGB_FS_CFG.get(\"subsample\", 0.7)),\n            \"colsample_bytree\": float(LGB_FS_CFG.get(\"colsample_bytree\", 0.7)),\n            \"reg_alpha\": float(LGB_FS_CFG.get(\"reg_alpha\", 1e-4)),\n            \"reg_lambda\": float(LGB_FS_CFG.get(\"reg_lambda\", 5.0)),\n        }\n        print(\"[INFO] Using default LightGBM params from config\")\n    \n    print(f\"[INFO] Loaded: X_train_sel={X_train_sel.shape} | X_valid_sel={X_valid_sel.shape} | X_test_sel={X_test_sel.shape}\")\n    \n    y_train = y_train_t1.astype(float).copy()\n    y_valid = y_valid_t1.astype(float).copy()\n    y_test = y_test_t1.astype(float).copy()\n    \n    w_train_arr = np.asarray(w_train, dtype=float)\n    w_valid_arr = np.asarray(w_valid, dtype=float)\n    w_test_arr = np.asarray(w_test, dtype=float)\n    \n    # Align VALID + TEST with Neural Networks\n    LOOKBACK = int(HPO_CFG[\"lookback\"])\n    SKIP_ROWS = LOOKBACK - 1\n    \n    X_valid_sel = X_valid_sel.iloc[SKIP_ROWS:]\n    y_valid = y_valid.iloc[SKIP_ROWS:]\n    w_valid_arr = w_valid_arr[SKIP_ROWS:]\n    \n    X_test_sel = X_test_sel.iloc[SKIP_ROWS:]\n    y_test = y_test.iloc[SKIP_ROWS:]\n    w_test_arr = w_test_arr[SKIP_ROWS:]\n    \n    print(f\"[INFO] Aligned VALID+TEST with lookback={LOOKBACK}: skipped first {SKIP_ROWS} rows\")\n    print(f\"[INFO] VALID shapes after alignment: X={X_valid_sel.shape}\")\n    print(f\"[INFO] TEST shapes after alignment: X={X_test_sel.shape}\")\n    \n    # -------------------------\n    # 2. Split VALID into ES and SCORE\n    # -------------------------\n    VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n    VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n    VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n    VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n    \n    def split_valid_es_score_lgb(Xv, yv, wv):\n        if not isinstance(Xv.index, pd.DatetimeIndex):\n            return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n        wv_s = pd.Series(wv, index=Xv.index)\n        \n        es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n        sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n        \n        m_es = (Xv.index >= es_start) & (Xv.index <= es_end)\n        m_sc = (Xv.index >= sc_start) & (Xv.index <= sc_end)\n        mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n        if m_es.sum() > 0 and m_sc.sum() > 0:\n            return (Xv.loc[m_es], yv.loc[m_es], wv_s.loc[m_es].to_numpy(float)), \\\n                   (Xv.loc[m_sc], yv.loc[m_sc], wv_s.loc[m_sc].to_numpy(float)), \\\n                   mode_str\n        return (Xv, yv, wv), (Xv, yv, wv), \"FULL_VALID\"\n    \n    (X_es, y_es, w_es), (X_sc, y_sc, w_sc), valid_mode = split_valid_es_score_lgb(\n        X_valid_sel, y_valid, w_valid_arr\n    )\n    \n    print(f\"[INFO] VALID mode: {valid_mode}\")\n    print(f\"[INFO] VALID_ES: {X_es.shape} | VALID_SCORE: {X_sc.shape}\")\n    \n    # -------------------------\n    # 3. BASELINES\n    # -------------------------\n    print(\"\\n[INFO] Computing BASELINES...\")\n    \n    baseline_results = []\n    \n    print(\"  [1] BASELINE_ZERO (predict 0):\")\n    pred_zero_sc = np.zeros(len(y_sc), dtype=float)\n    baseline_results.append({\n        \"model\": \"BASELINE_ZERO\", \"split\": \"VALID_SCORE\", \"n\": int(len(y_sc)),\n        \"wRMSE\": w_rmse(y_sc, pred_zero_sc, w_sc),\n        \"wMAE\": w_mae(y_sc, pred_zero_sc, w_sc),\n        \"DirAcc\": dir_acc(y_sc, pred_zero_sc),\n    })\n    \n    pred_zero_test = np.zeros(len(y_test), dtype=float)\n    baseline_results.append({\n        \"model\": \"BASELINE_ZERO\", \"split\": \"TEST\", \"n\": int(len(y_test)),\n        \"wRMSE\": w_rmse(y_test, pred_zero_test, w_test_arr),\n        \"wMAE\": w_mae(y_test, pred_zero_test, w_test_arr),\n        \"DirAcc\": dir_acc(y_test, pred_zero_test),\n    })\n    \n    for r in baseline_results[-2:]:\n        print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n    \n    print(\"  [2] BASELINE_NAIVE (last-value forecast):\")\n    pred_naive_sc = np.roll(np.asarray(y_sc), 1)\n    pred_naive_sc[0] = 0\n    \n    pred_naive_test = np.roll(np.asarray(y_test), 1)\n    pred_naive_test[0] = 0\n    \n    baseline_results.append({\n        \"model\": \"BASELINE_NAIVE\", \"split\": \"VALID_SCORE\", \"n\": int(len(y_sc)),\n        \"wRMSE\": w_rmse(y_sc, pred_naive_sc, w_sc),\n        \"wMAE\": w_mae(y_sc, pred_naive_sc, w_sc),\n        \"DirAcc\": dir_acc(y_sc, pred_naive_sc),\n    })\n    \n    baseline_results.append({\n        \"model\": \"BASELINE_NAIVE\", \"split\": \"TEST\", \"n\": int(len(y_test)),\n        \"wRMSE\": w_rmse(y_test, pred_naive_test, w_test_arr),\n        \"wMAE\": w_mae(y_test, pred_naive_test, w_test_arr),\n        \"DirAcc\": dir_acc(y_test, pred_naive_test),\n    })\n    \n    for r in baseline_results[-2:]:\n        print(f\"      {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n    \n    print(\"\\n[INFO] BASELINE Summary:\")\n    for r in baseline_results:\n        print(f\"  - {r['model']:15} | {r['split']:12} | wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n    \n    # -------------------------\n    # 4. Train FINAL MODEL\n    # -------------------------\n    print(\"\\n[INFO] Training FINAL LightGBM MODEL...\")\n    \n    N_ESTIMATORS = int(HPO_CFG[\"n_estimators\"])\n    EARLY_STOP = int(HPO_CFG[\"early_stopping_rounds\"])\n    RANDOM_SEED = int(HPO_CFG[\"random_state\"])\n    \n    best_params = dict(best_params)\n    if \"max_depth\" in best_params:\n        best_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n    if \"num_leaves\" in best_params:\n        best_params[\"num_leaves\"] = int(best_params[\"num_leaves\"])\n    if \"min_child_samples\" in best_params:\n        best_params[\"min_child_samples\"] = int(best_params[\"min_child_samples\"])\n    for k in [\"learning_rate\", \"subsample\", \"colsample_bytree\", \"reg_alpha\", \"reg_lambda\"]:\n        if k in best_params:\n            best_params[k] = float(best_params[k])\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=N_ESTIMATORS,\n        objective=\"regression\",\n        metric=\"rmse\",\n        random_state=RANDOM_SEED,\n        n_jobs=-1,\n        verbose=-1,\n        **best_params\n    )\n    \n    model.fit(\n        X_train_sel, y_train,\n        sample_weight=w_train_arr,\n        eval_set=[(X_es, y_es)],\n        eval_sample_weight=[w_es],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=EARLY_STOP, verbose=False),\n            lgb.log_evaluation(period=0)\n        ]\n    )\n    \n    best_iter = getattr(model, \"best_iteration_\", None)\n    print(f\"[INFO] Training complete. best_iteration={best_iter}\")\n    \n    # -------------------------\n    # 5. Evaluate FINAL MODEL\n    # -------------------------\n    print(\"\\n[INFO] Evaluating FINAL MODEL...\")\n    \n    model_results = []\n    \n    pred_model_sc = model.predict(X_sc)\n    model_results.append({\n        \"model\": \"FINAL_LGB\", \"split\": \"VALID_SCORE\", \"n\": int(len(y_sc)),\n        \"wRMSE\": w_rmse(y_sc, pred_model_sc, w_sc),\n        \"wMAE\": w_mae(y_sc, pred_model_sc, w_sc),\n        \"DirAcc\": dir_acc(y_sc, pred_model_sc),\n    })\n    \n    pred_model_test = model.predict(X_test_sel)\n    model_results.append({\n        \"model\": \"FINAL_LGB\", \"split\": \"TEST\", \"n\": int(len(y_test)),\n        \"wRMSE\": w_rmse(y_test, pred_model_test, w_test_arr),\n        \"wMAE\": w_mae(y_test, pred_model_test, w_test_arr),\n        \"DirAcc\": dir_acc(y_test, pred_model_test),\n    })\n    \n    print(\"[INFO] FINAL MODEL results:\")\n    for r in model_results:\n        print(f\"  - {r['split']}: wRMSE={r['wRMSE']:.6f} | DirAcc={r['DirAcc']:.4f}\")\n    \n    # -------------------------\n    # 6. Comparison\n    # -------------------------\n    all_results = baseline_results + model_results\n    metrics_df = pd.DataFrame(all_results)\n    \n    baseline_zero_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_ZERO\"}\n    metrics_df[\"wRMSE_vs_zero\"] = metrics_df.apply(\n        lambda row: baseline_zero_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n    )\n    \n    baseline_naive_wrmse = {r[\"split\"]: r[\"wRMSE\"] for r in baseline_results if r[\"model\"] == \"BASELINE_NAIVE\"}\n    metrics_df[\"wRMSE_vs_naive\"] = metrics_df.apply(\n        lambda row: baseline_naive_wrmse.get(row[\"split\"], 0) - row[\"wRMSE\"], axis=1\n    )\n    \n    print(\"\\n[INFO] BASELINE vs FINAL MODEL comparison:\")\n    display(metrics_df)\n    \n    # -------------------------\n    # 7. Build predictions DataFrames\n    # -------------------------\n    preds_valid_score_df = pd.DataFrame({\n        \"date\": X_sc.index,\n        \"actual\": y_sc.values,\n        \"baseline_zero\": pred_zero_sc,\n        \"baseline_naive\": pred_naive_sc,\n        \"predicted\": pred_model_sc,\n        \"sample_weight\": w_sc,\n    }).reset_index(drop=True)\n    \n    preds_test_df = pd.DataFrame({\n        \"date\": X_test_sel.index,\n        \"actual\": y_test.values,\n        \"baseline_zero\": pred_zero_test,\n        \"baseline_naive\": pred_naive_test,\n        \"predicted\": pred_model_test,\n        \"sample_weight\": w_test_arr,\n    }).reset_index(drop=True)\n    \n    # -------------------------\n    # 8. Save artifacts\n    # -------------------------\n    model_json_local = MODELS_OUT_LOCAL / \"final_model_lgb.json\"\n    model_dict = model.booster_.dump_model()\n    save_json(model_dict, model_json_local)\n    copy_file(model_json_local, MODELS_OUT_DRIVE / model_json_local.name)\n    \n    model_pkl_local = MODELS_OUT_LOCAL / \"final_model_lgb.pkl\"\n    save_pickle(model, model_pkl_local)\n    copy_file(model_pkl_local, MODELS_OUT_DRIVE / model_pkl_local.name)\n    \n    metrics_path_local = MODELS_OUT_LOCAL / \"final_metrics_lgb.csv\"\n    metrics_df.to_csv(metrics_path_local, index=False)\n    copy_file(metrics_path_local, MODELS_OUT_DRIVE / metrics_path_local.name)\n    \n    # Predictions -> predictions/lgb/\n    PRED_LGB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / \"lgb\")\n    PRED_LGB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / \"lgb\")\n    \n    preds_valid_path_local = PRED_LGB_LOCAL / \"predictions_valid.csv\"\n    preds_valid_score_df.to_csv(preds_valid_path_local, index=False)\n    copy_file(preds_valid_path_local, PRED_LGB_DRIVE / preds_valid_path_local.name)\n    \n    preds_test_path_local = PRED_LGB_LOCAL / \"predictions_test.csv\"\n    preds_test_df.to_csv(preds_test_path_local, index=False)\n    copy_file(preds_test_path_local, PRED_LGB_DRIVE / preds_test_path_local.name)\n    \n    print(\"\\n[OK] Saved FINAL LightGBM MODEL artifacts:\")\n    print(\"  -\", model_json_local.name)\n    print(\"  -\", model_pkl_local.name)\n    print(\"  -\", metrics_path_local.name)\n    print(\"  - predictions/lgb/\", preds_valid_path_local.name)\n    print(\"  - predictions/lgb/\", preds_test_path_local.name)\n    \n    # -------------------------\n    # Tomorrow Prediction + Plot\n    # -------------------------\n    PLOT_CFG = RUN_PARAMS[\"plot\"]\n    N_PLOT = int(PLOT_CFG[\"n_plot\"])\n    FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n    DPI = int(PLOT_CFG[\"dpi\"])\n    \n    last_date = X_test_sel.index[-1]\n    X_last = X_test_sel.iloc[[-1]]\n    pred_tomorrow = float(model.predict(X_last)[0])\n    \n    pred_tomorrow_df = pd.DataFrame([{\n        \"feature_set\": \"xgb_selected\",\n        \"last_data_date\": last_date,\n        \"predicted_for\": \"next_trading_day\",\n        \"pred_logret\": pred_tomorrow,\n        \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n    }])\n    pred_tomorrow_df.to_csv(PRED_LGB_LOCAL / \"tomorrow.csv\", index=False)\n    copy_file(PRED_LGB_LOCAL / \"tomorrow.csv\", PRED_LGB_DRIVE / \"tomorrow.csv\")\n    \n    hist_df = pd.DataFrame({\n        \"date\": X_test_sel.index,\n        \"actual\": y_test.values,\n        \"y_pred\": pred_model_test,\n    }).set_index(\"date\")\n    \n    hist_tail = hist_df.tail(N_PLOT).copy()\n    hist_tail.to_csv(PRED_LGB_LOCAL / \"backtest.csv\")\n    copy_file(PRED_LGB_LOCAL / \"backtest.csv\", PRED_LGB_DRIVE / \"backtest.csv\")\n    \n    fig, ax = plt.subplots(figsize=FIGSIZE)\n    ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n    ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted\")\n    pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n    ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n    ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n    ax.set_title(f\"LightGBM Predictions \u2014 last {len(hist_tail)} days + tomorrow\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Log Return\")\n    ax.legend(loc=\"upper right\")\n    plt.tight_layout()\n    plt.savefig(PRED_LGB_LOCAL / \"plot.png\", dpi=DPI)\n    copy_file(PRED_LGB_LOCAL / \"plot.png\", PRED_LGB_DRIVE / \"plot.png\")\n    plt.close(fig)\n    \n    print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n    print(f\"[OK] Saved: predictions/lgb/ (tomorrow.csv, backtest.csv, plot.png)\")\n    \n    # Store for later comparison\n    LGB_METRICS = metrics_df\n    \n    print(\"[OK] BLOCK 25B complete.\")\nelse:\n    LGB_METRICS = None\n    print(\"[SKIP] LightGBM section skipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 8: LSTM & GRU Neural Networks\n",
    "\n",
    "**Train and predict with LSTM and GRU**\n",
    "\n",
    "**Blocks:** 26-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a68cb",
   "metadata": {},
   "source": [
    "## BLOCK 26 \u2014 NEURAL NETWORK TRAINING (LSTM + GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not TF_AVAILABLE:\n",
    "    print(\"[SKIP] BLOCK 26 \u2014 TensorFlow not available\")\n",
    "else:\n",
    "    # Config\n",
    "    MODEL_TYPES = [\"lstm\", \"gru\"]\n",
    "    \n",
    "    # Directories\n",
    "    # Fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE\n",
    "    RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
    "    FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
    "    RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
    "    FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
    "    MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n",
    "    MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n",
    "    \n",
    "    print(\"[INFO] Neural Network output dirs:\")\n",
    "    print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n",
    "    print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Load shared data (y, weights)\n",
    "    # -------------------------\n",
    "    y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    \n",
    "    w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    \n",
    "    y_train = y_train_t1.astype(float).to_numpy()\n",
    "    y_valid = y_valid_t1.astype(float).to_numpy()\n",
    "    y_test = y_test_t1.astype(float).to_numpy()\n",
    "    \n",
    "    w_train_np = np.asarray(w_train, dtype=float)\n",
    "    w_valid_np = np.asarray(w_valid, dtype=float)\n",
    "    w_test_np = np.asarray(w_test, dtype=float)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Metrics\n",
    "    # -------------------------\n",
    "    # Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
    "    \n",
    "    # -------------------------\n",
    "    # VALID split function (date-based)\n",
    "    # -------------------------\n",
    "    HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
    "    VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
    "    VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
    "    VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
    "    VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
    "    \n",
    "    def split_valid_es_score_nn(Xv_df, yv, wv):\n",
    "        \"\"\"Split validation into ES and SCORE sets.\"\"\"\n",
    "        if not isinstance(Xv_df.index, pd.DatetimeIndex):\n",
    "            return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
    "        \n",
    "        yv_s = pd.Series(yv, index=Xv_df.index)\n",
    "        wv_s = pd.Series(wv, index=Xv_df.index)\n",
    "        \n",
    "        es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n",
    "        sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n",
    "        m_es = (Xv_df.index >= es_start) & (Xv_df.index <= es_end)\n",
    "        m_sc = (Xv_df.index >= sc_start) & (Xv_df.index <= sc_end)\n",
    "        mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n",
    "        \n",
    "        X_es, X_sc = Xv_df.loc[m_es], Xv_df.loc[m_sc]\n",
    "        y_es, y_sc = yv_s.loc[m_es].to_numpy(float), yv_s.loc[m_sc].to_numpy(float)\n",
    "        w_es, w_sc = wv_s.loc[m_es].to_numpy(float), wv_s.loc[m_sc].to_numpy(float)\n",
    "        \n",
    "        if len(X_es) > 0 and len(X_sc) > 0:\n",
    "            return (X_es, y_es, w_es), (X_sc, y_sc, w_sc), mode_str\n",
    "        return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
    "    \n",
    "    # -------------------------\n",
    "    # Sequence creation function\n",
    "    # -------------------------\n",
    "    def make_sequences_eod_nn(X_2d, y_1d, w_1d, idx, lookback, stride=1):\n",
    "        \"\"\"Create sequences: use X up to day t (inclusive) -> predict y[t].\"\"\"\n",
    "        X_2d, y_1d, w_1d = np.asarray(X_2d), np.asarray(y_1d), np.asarray(w_1d)\n",
    "        N, F = X_2d.shape\n",
    "        if N < lookback:\n",
    "            raise ValueError(f\"[ERROR] Not enough rows N={N} for lookback={lookback}.\")\n",
    "        X_seq, y_seq, w_seq, idx_seq = [], [], [], []\n",
    "        for t in range(lookback - 1, N, stride):\n",
    "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
    "            y_seq.append(y_1d[t])\n",
    "            w_seq.append(w_1d[t])\n",
    "            idx_seq.append(idx[t])\n",
    "        return (np.asarray(X_seq, dtype=np.float32),\n",
    "                np.asarray(y_seq, dtype=np.float32),\n",
    "                np.asarray(w_seq, dtype=np.float32),\n",
    "                pd.DatetimeIndex(idx_seq))\n",
    "    \n",
    "    # -------------------------\n",
    "    # Loop over model types (LSTM, GRU)\n",
    "    # -------------------------\n",
    "    for model_type in MODEL_TYPES:\n",
    "        NN_CFG = RUN_PARAMS[model_type]\n",
    "        LOOKBACK = int(NN_CFG[\"lookback\"])\n",
    "        STRIDE = int(NN_CFG[\"stride\"])\n",
    "        UNITS_1 = int(NN_CFG[\"units_1\"])\n",
    "        UNITS_2 = int(NN_CFG[\"units_2\"])\n",
    "        DENSE_UNITS = int(NN_CFG[\"dense_units\"])\n",
    "        DROPOUT = float(NN_CFG[\"dropout\"])\n",
    "        LR = float(NN_CFG[\"learning_rate\"])\n",
    "        CLIPNORM = float(NN_CFG[\"clipnorm\"])\n",
    "        DENSE_ACT = NN_CFG[\"dense_activation\"]\n",
    "        OUTPUT_ACT = NN_CFG[\"output_activation\"]\n",
    "        EPOCHS = int(NN_CFG[\"epochs\"])\n",
    "        BATCH_SIZE = int(NN_CFG[\"batch_size\"])\n",
    "        PATIENCE = int(NN_CFG[\"patience\"])\n",
    "        RANDOM_SEED = int(NN_CFG[\"random_state\"])\n",
    "        FEATURE_SETS = NN_CFG[\"feature_sets\"]\n",
    "        \n",
    "        all_nn_results = []\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# TRAINING {model_type.upper()} MODELS\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        for feature_set in FEATURE_SETS:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"[INFO] Training {model_type.upper()} with feature set: {feature_set}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Load feature-specific X matrices\n",
    "            X_train_nn = load_with_fallback(f\"X_train_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
    "            X_valid_nn = load_with_fallback(f\"X_valid_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
    "            X_test_nn = load_with_fallback(f\"X_test_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
    "            \n",
    "            n_features = X_train_nn.shape[1]\n",
    "            print(f\"[INFO] Shapes: TRAIN={X_train_nn.shape} | VALID={X_valid_nn.shape} | TEST={X_test_nn.shape}\")\n",
    "            \n",
    "            # Split VALID -> ES + SCORE\n",
    "            (X_valid_es_df, y_valid_es, w_valid_es), (X_valid_sc_df, y_valid_sc, w_valid_sc), valid_mode = split_valid_es_score_nn(\n",
    "                X_valid_nn, y_valid, w_valid_np\n",
    "            )\n",
    "            print(f\"[INFO] VALID mode: {valid_mode}\")\n",
    "            \n",
    "            # Scaling (fit on TRAIN only)\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_nn.values)\n",
    "            X_valid_es_scaled = scaler.transform(X_valid_es_df.values)\n",
    "            X_valid_sc_scaled = scaler.transform(X_valid_sc_df.values)\n",
    "            X_test_scaled = scaler.transform(X_test_nn.values)\n",
    "            \n",
    "            # Create sequences\n",
    "            Xtr_seq, ytr_seq, wtr_seq, idx_tr = make_sequences_eod_nn(X_train_scaled, y_train, w_train_np, X_train_nn.index, LOOKBACK, STRIDE)\n",
    "            Xes_seq, yes_seq, wes_seq, idx_es = make_sequences_eod_nn(X_valid_es_scaled, y_valid_es, w_valid_es, X_valid_es_df.index, LOOKBACK, STRIDE)\n",
    "            Xsc_seq, ysc_seq, wsc_seq, idx_sc = make_sequences_eod_nn(X_valid_sc_scaled, y_valid_sc, w_valid_sc, X_valid_sc_df.index, LOOKBACK, STRIDE)\n",
    "            Xte_seq, yte_seq, wte_seq, idx_te = make_sequences_eod_nn(X_test_scaled, y_test, w_test_np, X_test_nn.index, LOOKBACK, STRIDE)\n",
    "            \n",
    "            print(f\"[INFO] Sequence shapes: TRAIN={Xtr_seq.shape} | VALID_ES={Xes_seq.shape} | VALID_SCORE={Xsc_seq.shape} | TEST={Xte_seq.shape}\")\n",
    "            \n",
    "            # Build model\n",
    "            tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
    "            \n",
    "            inp = keras.Input(shape=(LOOKBACK, n_features))\n",
    "            if model_type == \"lstm\":\n",
    "                x = layers.LSTM(UNITS_1, return_sequences=True, dropout=DROPOUT)(inp)\n",
    "            else:\n",
    "                x = layers.GRU(UNITS_1, return_sequences=True, dropout=DROPOUT)(inp)\n",
    "            x = layers.LayerNormalization()(x)\n",
    "            if model_type == \"lstm\":\n",
    "                x = layers.LSTM(UNITS_2, return_sequences=False, dropout=DROPOUT)(x)\n",
    "            else:\n",
    "                x = layers.GRU(UNITS_2, return_sequences=False, dropout=DROPOUT)(x)\n",
    "            x = layers.Dense(DENSE_UNITS, activation=DENSE_ACT)(x)\n",
    "            x = layers.Dropout(DROPOUT)(x)\n",
    "            out = layers.Dense(1, activation=OUTPUT_ACT)(x)\n",
    "            \n",
    "            model = keras.Model(inp, out)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=LR, clipnorm=CLIPNORM), loss=NN_CFG[\"loss\"])\n",
    "            print(f\"[INFO] Model built: {model.count_params()} parameters\")\n",
    "            \n",
    "            # Custom callback for SCORE set tracking\n",
    "            class ScoreSetCallback(keras.callbacks.Callback):\n",
    "                def __init__(self, X_score, y_score, w_score):\n",
    "                    super().__init__()\n",
    "                    self.Xs, self.ys, self.ws = X_score, y_score, w_score\n",
    "                    self.best = np.inf\n",
    "                    self.best_weights = None\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    pred = self.model.predict(self.Xs, verbose=0).reshape(-1)\n",
    "                    score = w_rmse(self.ys, pred, self.ws)\n",
    "                    if score < self.best:\n",
    "                        self.best = score\n",
    "                        self.best_weights = self.model.get_weights()\n",
    "            \n",
    "            score_cb = ScoreSetCallback(Xsc_seq, ysc_seq, wsc_seq)\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
    "                score_cb,\n",
    "            ]\n",
    "            \n",
    "            # Train\n",
    "            print(f\"[INFO] Training {model_type.upper()} (epochs={EPOCHS}, batch_size={BATCH_SIZE}, patience={PATIENCE})...\")\n",
    "            history = model.fit(\n",
    "                Xtr_seq, ytr_seq,\n",
    "                sample_weight=wtr_seq,\n",
    "                validation_data=(Xes_seq, yes_seq, wes_seq),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=0,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            \n",
    "            # Restore best weights by SCORE\n",
    "            if score_cb.best_weights is not None:\n",
    "                model.set_weights(score_cb.best_weights)\n",
    "                print(f\"[INFO] Restored best weights by VALID_SCORE wRMSE = {score_cb.best:.6f}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            pred_sc = model.predict(Xsc_seq, verbose=0).reshape(-1)\n",
    "            pred_te = model.predict(Xte_seq, verbose=0).reshape(-1)\n",
    "            baseline_sc, baseline_te = np.zeros_like(ysc_seq), np.zeros_like(yte_seq)\n",
    "            \n",
    "            # Metrics\n",
    "            results = {\n",
    "                \"model_type\": model_type,\n",
    "                \"feature_set\": feature_set,\n",
    "                \"n_features\": n_features,\n",
    "                \"valid_mode\": valid_mode,\n",
    "                \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "                # Config params for comparison\n",
    "                f\"{model_type}_units_1\": UNITS_1,\n",
    "                f\"{model_type}_units_2\": UNITS_2,\n",
    "                \"dropout\": DROPOUT,\n",
    "                \"learning_rate\": LR,\n",
    "                \"lookback\": LOOKBACK,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"baseline_valid_wrmse\": w_rmse(ysc_seq, baseline_sc, wsc_seq),\n",
    "                \"baseline_valid_diracc\": dir_acc(ysc_seq, baseline_sc),\n",
    "                \"baseline_test_wrmse\": w_rmse(yte_seq, baseline_te, wte_seq),\n",
    "                \"baseline_test_diracc\": dir_acc(yte_seq, baseline_te),\n",
    "                \"model_valid_wrmse\": w_rmse(ysc_seq, pred_sc, wsc_seq),\n",
    "                \"model_valid_wmae\": w_mae(ysc_seq, pred_sc, wsc_seq),\n",
    "                \"model_valid_diracc\": dir_acc(ysc_seq, pred_sc),\n",
    "                \"model_test_wrmse\": w_rmse(yte_seq, pred_te, wte_seq),\n",
    "                \"model_test_wmae\": w_mae(yte_seq, pred_te, wte_seq),\n",
    "                \"model_test_diracc\": dir_acc(yte_seq, pred_te),\n",
    "            }\n",
    "            results[\"valid_wrmse_improvement\"] = results[\"baseline_valid_wrmse\"] - results[\"model_valid_wrmse\"]\n",
    "            results[\"test_wrmse_improvement\"] = results[\"baseline_test_wrmse\"] - results[\"model_test_wrmse\"]\n",
    "            all_nn_results.append(results)\n",
    "            \n",
    "            print(f\"\\n[RESULT] {model_type.upper()} | {feature_set} | n_features={n_features}\")\n",
    "            print(f\"  BASELINE VALID_SCORE: wRMSE={results['baseline_valid_wrmse']:.6f} | DirAcc={results['baseline_valid_diracc']:.4f}\")\n",
    "            print(f\"  MODEL    VALID_SCORE: wRMSE={results['model_valid_wrmse']:.6f} | DirAcc={results['model_valid_diracc']:.4f}\")\n",
    "            print(f\"  MODEL    TEST:        wRMSE={results['model_test_wrmse']:.6f} | DirAcc={results['model_test_diracc']:.4f}\")\n",
    "            print(f\"  Improvement (VALID):  {results['valid_wrmse_improvement']:.6f}\")\n",
    "            \n",
    "            # Save model + scaler + config\n",
    "            model_path = MODELS_OUT_LOCAL / f\"{model_type}_{feature_set}.keras\"\n",
    "            scaler_path = MODELS_OUT_LOCAL / f\"{model_type}_{feature_set}_scaler.pkl\"\n",
    "            config_path = MODELS_OUT_LOCAL / f\"{model_type}_{feature_set}_config.json\"\n",
    "            model.save(model_path)\n",
    "            save_pickle(scaler, scaler_path)\n",
    "            save_json(NN_CFG, config_path)\n",
    "            copy_file(model_path, MODELS_OUT_DRIVE / model_path.name)\n",
    "            copy_file(scaler_path, MODELS_OUT_DRIVE / scaler_path.name)\n",
    "            copy_file(config_path, MODELS_OUT_DRIVE / config_path.name)\n",
    "            \n",
    "            # Predictions -> predictions/{model_type}_{feature_set}/\n",
    "            PRED_NN_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
    "            PRED_NN_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
    "            \n",
    "            preds_valid_df = pd.DataFrame({\n",
    "                \"date\": idx_sc, \"actual\": ysc_seq, \"baseline_zero\": baseline_sc,\n",
    "                \"predicted\": pred_sc, \"sample_weight\": wsc_seq,\n",
    "            }).reset_index(drop=True)\n",
    "            preds_test_df = pd.DataFrame({\n",
    "                \"date\": idx_te, \"actual\": yte_seq, \"baseline_zero\": baseline_te,\n",
    "                \"predicted\": pred_te, \"sample_weight\": wte_seq,\n",
    "            }).reset_index(drop=True)\n",
    "            \n",
    "            preds_valid_df.to_csv(PRED_NN_LOCAL / \"predictions_valid.csv\", index=False)\n",
    "            preds_test_df.to_csv(PRED_NN_LOCAL / \"predictions_test.csv\", index=False)\n",
    "            copy_file(PRED_NN_LOCAL / \"predictions_valid.csv\", PRED_NN_DRIVE / \"predictions_valid.csv\")\n",
    "            copy_file(PRED_NN_LOCAL / \"predictions_test.csv\", PRED_NN_DRIVE / \"predictions_test.csv\")\n",
    "            \n",
    "            print(f\"[OK] Saved: {model_path.name}, {scaler_path.name}\")\n",
    "            print(f\"[OK] Predictions: predictions/{model_type}_{feature_set}/\")\n",
    "        \n",
    "        # Summary for this model type\n",
    "        nn_results_df = pd.DataFrame(all_nn_results)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[INFO] {model_type.upper()} TRAINING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(nn_results_df[[\"feature_set\", \"n_features\", \"model_valid_wrmse\", \"model_test_wrmse\", \"valid_wrmse_improvement\"]])\n",
    "        \n",
    "        summary_path = MODELS_OUT_LOCAL / f\"{model_type}_summary.csv\"\n",
    "        nn_results_df.to_csv(summary_path, index=False)\n",
    "        copy_file(summary_path, MODELS_OUT_DRIVE / summary_path.name)\n",
    "        print(f\"[OK] Saved {model_type.upper()} summary: {summary_path.name}\")\n",
    "    \n",
    "    print(\"[OK] BLOCK 26 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef512111",
   "metadata": {},
   "source": [
    "## BLOCK 27 \u2014 NEURAL NETWORK PREDICT TOMORROW + BACKTEST PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not TF_AVAILABLE:\n",
    "    print(\"[SKIP] BLOCK 27 \u2014 TensorFlow not available\")\n",
    "else:\n",
    "    MODEL_TYPES = [\"lstm\", \"gru\"]\n",
    "    PLOT_CFG = RUN_PARAMS[\"plot\"]\n",
    "    N_PLOT = int(PLOT_CFG[\"n_plot\"])\n",
    "    FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n",
    "    DPI = int(PLOT_CFG[\"dpi\"])\n",
    "    \n",
    "    # Directories\n",
    "    PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
    "    MODELS_DIR_LOCAL = Path(LOCAL_PATHS[\"models_dir\"])\n",
    "    PRED_OUT_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\")\n",
    "    PRED_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\")\n",
    "    \n",
    "    print(\"[INFO] Neural Network Predictions output dirs:\")\n",
    "    print(\"  - LOCAL:\", PRED_OUT_LOCAL)\n",
    "    print(\"  - DRIVE:\", PRED_OUT_DRIVE)\n",
    "    \n",
    "    # Sequence creation\n",
    "    def make_sequences_pred_nn(X_2d, y_1d, idx, lookback):\n",
    "        X_2d, y_1d = np.asarray(X_2d), np.asarray(y_1d)\n",
    "        N, F = X_2d.shape\n",
    "        X_seq, y_seq, idx_seq = [], [], []\n",
    "        for t in range(lookback - 1, N):\n",
    "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
    "            y_seq.append(y_1d[t])\n",
    "            idx_seq.append(idx[t])\n",
    "        return (np.asarray(X_seq, dtype=np.float32),\n",
    "                np.asarray(y_seq, dtype=np.float32),\n",
    "                pd.DatetimeIndex(idx_seq))\n",
    "    \n",
    "    # Load shared data\n",
    "    y_test = load_with_fallback(\"y_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
    "    w_test = load_with_fallback(\"weights_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
    "    y_test_arr = y_test.astype(float).to_numpy()\n",
    "    w_test_arr = np.asarray(w_test, dtype=float)\n",
    "    \n",
    "    # Loop over model types\n",
    "    for model_type in MODEL_TYPES:\n",
    "        NN_CFG = RUN_PARAMS[model_type]\n",
    "        LOOKBACK = int(NN_CFG[\"lookback\"])\n",
    "        FEATURE_SETS = NN_CFG[\"feature_sets\"]\n",
    "        \n",
    "        all_pred_results = []\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# PREDICTING WITH {model_type.upper()} MODELS\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        for feature_set in FEATURE_SETS:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"[INFO] Predicting with {model_type.upper()}: {feature_set}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Load model + scaler\n",
    "            model_path = MODELS_DIR_LOCAL / f\"{model_type}_{feature_set}.keras\"\n",
    "            scaler_path = MODELS_DIR_LOCAL / f\"{model_type}_{feature_set}_scaler.pkl\"\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"[WARN] Model not found: {model_path}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            model = keras.models.load_model(model_path)\n",
    "            scaler = load_pickle(scaler_path)\n",
    "            \n",
    "            # Load X_test\n",
    "            X_test = load_with_fallback(f\"X_test_{feature_set}.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR, use_pandas=True)\n",
    "            n_features = X_test.shape[1]\n",
    "            print(f\"[INFO] Loaded: model={model_path.name} | X_test={X_test.shape}\")\n",
    "            \n",
    "            # Scale + Sequences\n",
    "            X_test_scaled = scaler.transform(X_test.values)\n",
    "            X_seq, y_seq, idx_seq = make_sequences_pred_nn(X_test_scaled, y_test_arr, X_test.index, LOOKBACK)\n",
    "            print(f\"[INFO] Sequences: {X_seq.shape}\")\n",
    "            \n",
    "            # Predict\n",
    "            pred_seq = model.predict(X_seq, verbose=0).reshape(-1)\n",
    "            hist_df = pd.DataFrame({\"date\": idx_seq, \"actual\": y_seq, \"y_pred\": pred_seq}).set_index(\"date\")\n",
    "            \n",
    "            # Tomorrow prediction\n",
    "            last_date = X_test.index[-1]\n",
    "            X_last_window = X_test_scaled[-LOOKBACK:, :].reshape(1, LOOKBACK, -1).astype(np.float32)\n",
    "            pred_tomorrow = float(model.predict(X_last_window, verbose=0).reshape(-1)[0])\n",
    "            pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n",
    "            \n",
    "            # Metrics\n",
    "            w_seq = w_test_arr[LOOKBACK - 1:]\n",
    "            w_norm = w_seq / (w_seq.sum() + EPS)\n",
    "            test_wrmse = float(np.sqrt(np.sum(w_norm * (y_seq - pred_seq) ** 2)))\n",
    "            test_dir_acc = float(np.mean((y_seq > 0) == (pred_seq > 0)))\n",
    "            \n",
    "            print(f\"[INFO] TEST wRMSE={test_wrmse:.6f} | DirAcc={test_dir_acc:.4f}\")\n",
    "            print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n",
    "            \n",
    "            result = {\n",
    "                \"model_type\": model_type,\n",
    "                \"feature_set\": feature_set,\n",
    "                \"n_features\": n_features,\n",
    "                \"test_wrmse\": test_wrmse,\n",
    "                \"test_dir_acc\": test_dir_acc,\n",
    "                \"last_data_date\": str(last_date.date()),\n",
    "                \"pred_tomorrow_logret\": pred_tomorrow,\n",
    "                \"pred_tomorrow_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
    "            }\n",
    "            all_pred_results.append(result)\n",
    "            \n",
    "            # Save -> predictions/{model_type}_{feature_set}/\n",
    "            PRED_NN_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
    "            PRED_NN_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{model_type}_{feature_set}\")\n",
    "            \n",
    "            pred_tomorrow_df = pd.DataFrame([{\n",
    "                \"feature_set\": feature_set, \"last_data_date\": last_date,\n",
    "                \"predicted_for\": \"next_trading_day\",\n",
    "                \"pred_logret\": pred_tomorrow,\n",
    "                \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
    "            }])\n",
    "            pred_tomorrow_df.to_csv(PRED_NN_LOCAL / \"tomorrow.csv\", index=False)\n",
    "            copy_file(PRED_NN_LOCAL / \"tomorrow.csv\", PRED_NN_DRIVE / \"tomorrow.csv\")\n",
    "            \n",
    "            hist_tail = hist_df.tail(N_PLOT).copy()\n",
    "            hist_tail.to_csv(PRED_NN_LOCAL / \"backtest.csv\")\n",
    "            copy_file(PRED_NN_LOCAL / \"backtest.csv\", PRED_NN_DRIVE / \"backtest.csv\")\n",
    "            \n",
    "            # Plot\n",
    "            fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "            ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n",
    "            ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted (y_pred)\")\n",
    "            ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n",
    "            ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
    "            ax.set_title(f\"{model_type.upper()} Predictions \u2014 {feature_set} \u2014 last {len(hist_tail)} days + tomorrow\")\n",
    "            ax.set_xlabel(\"Date\")\n",
    "            ax.set_ylabel(\"Log Return\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PRED_NN_LOCAL / \"plot.png\", dpi=DPI)\n",
    "            copy_file(PRED_NN_LOCAL / \"plot.png\", PRED_NN_DRIVE / \"plot.png\")\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"[OK] Saved: predictions/{model_type}_{feature_set}/ (tomorrow.csv, backtest.csv, plot.png)\")\n",
    "        \n",
    "        # Summary for this model type\n",
    "        pred_summary_df = pd.DataFrame(all_pred_results)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[INFO] {model_type.upper()} PREDICTION SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(pred_summary_df)\n",
    "        \n",
    "        summary_path = PRED_OUT_LOCAL / f\"{model_type}_predictions_summary.csv\"\n",
    "        pred_summary_df.to_csv(summary_path, index=False)\n",
    "        copy_file(summary_path, PRED_OUT_DRIVE / summary_path.name)\n",
    "        print(f\"[OK] Saved summary: {summary_path.name}\")\n",
    "    \n",
    "    print(\"[OK] BLOCK 27 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 9: Hybrid Neural Networks\n",
    "\n",
    "**Sequential and Parallel hybrid architectures**\n",
    "\n",
    "**Blocks:** 28-29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bccee",
   "metadata": {},
   "source": [
    "## BLOCK 28 \u2014 HYBRID NEURAL NETWORK TRAINING (Sequential + Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not TF_AVAILABLE:\n",
    "    print(\"[SKIP] BLOCK 28 \u2014 TensorFlow not available\")\n",
    "else:\n",
    "    # Config\n",
    "    HYBRID_TYPES = [\"hybrid_seq\", \"hybrid_par\"]\n",
    "    \n",
    "    # Directories\n",
    "    # Fallback: RUN_ID_LOCAL -> data/processed_LOCAL -> RUN_ID_DRIVE -> data/processed_DRIVE\n",
    "    RUN_PROC_LOCAL = Path(LOCAL_PATHS[\"proc_dir\"])\n",
    "    FALLBACK_PROC_LOCAL = DATA_DIRS_LOCAL[\"processed\"]\n",
    "    RUN_PROC_DRIVE = Path(DRIVE_PATHS[\"proc_dir\"])\n",
    "    FALLBACK_PROC_DRIVE = DATA_DIRS_DRIVE[\"processed\"]\n",
    "    MODELS_OUT_LOCAL = ensure_dir(Path(MODELS_DIR))\n",
    "    MODELS_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"models_dir\"]))\n",
    "    \n",
    "    print(\"[INFO] Hybrid Neural Network output dirs:\")\n",
    "    print(\"  - LOCAL:\", MODELS_OUT_LOCAL)\n",
    "    print(\"  - DRIVE:\", MODELS_OUT_DRIVE)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Load shared data (y, weights)\n",
    "    # -------------------------\n",
    "    y_train_t1 = load_with_fallback(\"y_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    y_valid_t1 = load_with_fallback(\"y_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    y_test_t1 = load_with_fallback(\"y_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    \n",
    "    w_train = load_with_fallback(\"weights_train.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    w_valid = load_with_fallback(\"weights_valid.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    w_test = load_with_fallback(\"weights_test.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE)\n",
    "    \n",
    "    y_train = y_train_t1.astype(float).to_numpy()\n",
    "    y_valid = y_valid_t1.astype(float).to_numpy()\n",
    "    y_test = y_test_t1.astype(float).to_numpy()\n",
    "    \n",
    "    w_train_np = np.asarray(w_train, dtype=float)\n",
    "    w_valid_np = np.asarray(w_valid, dtype=float)\n",
    "    w_test_np = np.asarray(w_test, dtype=float)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Metrics\n",
    "    # -------------------------\n",
    "    # Metric functions (w_rmse, w_mae, dir_acc) defined in Cell 5\n",
    "    \n",
    "    # -------------------------\n",
    "    # VALID split function (date-based)\n",
    "    # -------------------------\n",
    "    HPO_CFG = RUN_PARAMS[\"hpo\"]\n",
    "    VALID_ES_START = HPO_CFG[\"valid_es_start\"]\n",
    "    VALID_ES_END = HPO_CFG[\"valid_es_end\"]\n",
    "    VALID_SCORE_START = HPO_CFG[\"valid_score_start\"]\n",
    "    VALID_SCORE_END = HPO_CFG[\"valid_score_end\"]\n",
    "    \n",
    "    def split_valid_es_score_hybrid(Xv_df, yv, wv):\n",
    "        if not isinstance(Xv_df.index, pd.DatetimeIndex):\n",
    "            return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
    "        \n",
    "        yv_s = pd.Series(yv, index=Xv_df.index)\n",
    "        wv_s = pd.Series(wv, index=Xv_df.index)\n",
    "        \n",
    "        es_start, es_end = pd.Timestamp(VALID_ES_START), pd.Timestamp(VALID_ES_END)\n",
    "        sc_start, sc_end = pd.Timestamp(VALID_SCORE_START), pd.Timestamp(VALID_SCORE_END)\n",
    "        m_es = (Xv_df.index >= es_start) & (Xv_df.index <= es_end)\n",
    "        m_sc = (Xv_df.index >= sc_start) & (Xv_df.index <= sc_end)\n",
    "        mode_str = f\"VALID_ES={VALID_ES_START}:{VALID_ES_END} / VALID_SCORE={VALID_SCORE_START}:{VALID_SCORE_END}\"\n",
    "        \n",
    "        X_es, X_sc = Xv_df.loc[m_es], Xv_df.loc[m_sc]\n",
    "        y_es, y_sc = yv_s.loc[m_es].to_numpy(float), yv_s.loc[m_sc].to_numpy(float)\n",
    "        w_es, w_sc = wv_s.loc[m_es].to_numpy(float), wv_s.loc[m_sc].to_numpy(float)\n",
    "        \n",
    "        if len(X_es) > 0 and len(X_sc) > 0:\n",
    "            return (X_es, y_es, w_es), (X_sc, y_sc, w_sc), mode_str\n",
    "        return (Xv_df, yv, wv), (Xv_df, yv, wv), \"FULL_VALID\"\n",
    "    \n",
    "    # -------------------------\n",
    "    # Sequence creation function\n",
    "    # -------------------------\n",
    "    def make_sequences_eod_hybrid(X_2d, y_1d, w_1d, idx, lookback, stride=1):\n",
    "        X_2d, y_1d, w_1d = np.asarray(X_2d), np.asarray(y_1d), np.asarray(w_1d)\n",
    "        N, F = X_2d.shape\n",
    "        if N < lookback:\n",
    "            raise ValueError(f\"[ERROR] Not enough rows N={N} for lookback={lookback}.\")\n",
    "        X_seq, y_seq, w_seq, idx_seq = [], [], [], []\n",
    "        for t in range(lookback - 1, N, stride):\n",
    "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
    "            y_seq.append(y_1d[t])\n",
    "            w_seq.append(w_1d[t])\n",
    "            idx_seq.append(idx[t])\n",
    "        return (np.asarray(X_seq, dtype=np.float32),\n",
    "                np.asarray(y_seq, dtype=np.float32),\n",
    "                np.asarray(w_seq, dtype=np.float32),\n",
    "                pd.DatetimeIndex(idx_seq))\n",
    "    \n",
    "    # -------------------------\n",
    "    # Loop over hybrid types (Sequential, Parallel)\n",
    "    # -------------------------\n",
    "    for hybrid_type in HYBRID_TYPES:\n",
    "        HYB_CFG = RUN_PARAMS[hybrid_type]\n",
    "        LOOKBACK = int(HYB_CFG[\"lookback\"])\n",
    "        STRIDE = int(HYB_CFG[\"stride\"])\n",
    "        LSTM_UNITS = int(HYB_CFG[\"lstm_units\"])\n",
    "        GRU_UNITS = int(HYB_CFG[\"gru_units\"])\n",
    "        DROPOUT = float(HYB_CFG[\"dropout\"])\n",
    "        LR = float(HYB_CFG[\"learning_rate\"])\n",
    "        EPOCHS = int(HYB_CFG[\"epochs\"])\n",
    "        BATCH_SIZE = int(HYB_CFG[\"batch_size\"])\n",
    "        PATIENCE = int(HYB_CFG[\"patience\"])\n",
    "        RANDOM_SEED = int(HYB_CFG[\"random_state\"])\n",
    "        FEATURE_SETS = HYB_CFG[\"feature_sets\"]\n",
    "        DENSE_UNITS = int(HYB_CFG[\"dense_units\"])\n",
    "        CLIPNORM = float(HYB_CFG[\"clipnorm\"])\n",
    "        LOSS = HYB_CFG[\"loss\"]\n",
    "        DENSE_ACT = HYB_CFG[\"dense_activation\"]\n",
    "        OUTPUT_ACT = HYB_CFG[\"output_activation\"]\n",
    "        \n",
    "        all_hybrid_results = []\n",
    "        \n",
    "        arch_name = \"Sequential (LSTM\u2192GRU)\" if hybrid_type == \"hybrid_seq\" else \"Parallel (LSTM\u2225GRU)\"\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# TRAINING {hybrid_type.upper()} \u2014 {arch_name}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        for feature_set in FEATURE_SETS:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"[INFO] Training {hybrid_type.upper()} with feature set: {feature_set}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Load feature-specific X matrices\n",
    "            X_train_nn = load_with_fallback(f\"X_train_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
    "            X_valid_nn = load_with_fallback(f\"X_valid_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
    "            X_test_nn = load_with_fallback(f\"X_test_{feature_set}.pkl\", RUN_PROC_LOCAL, FALLBACK_PROC_LOCAL, RUN_PROC_DRIVE, FALLBACK_PROC_DRIVE, use_pandas=True)\n",
    "            \n",
    "            n_features = X_train_nn.shape[1]\n",
    "            print(f\"[INFO] Shapes: TRAIN={X_train_nn.shape} | VALID={X_valid_nn.shape} | TEST={X_test_nn.shape}\")\n",
    "            \n",
    "            # Split VALID -> ES + SCORE\n",
    "            (X_valid_es_df, y_valid_es, w_valid_es), (X_valid_sc_df, y_valid_sc, w_valid_sc), valid_mode = split_valid_es_score_hybrid(\n",
    "                X_valid_nn, y_valid, w_valid_np\n",
    "            )\n",
    "            print(f\"[INFO] VALID mode: {valid_mode}\")\n",
    "            \n",
    "            # Scaling (fit on TRAIN only)\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_nn.values)\n",
    "            X_valid_es_scaled = scaler.transform(X_valid_es_df.values)\n",
    "            X_valid_sc_scaled = scaler.transform(X_valid_sc_df.values)\n",
    "            X_test_scaled = scaler.transform(X_test_nn.values)\n",
    "            \n",
    "            # Create sequences\n",
    "            Xtr_seq, ytr_seq, wtr_seq, idx_tr = make_sequences_eod_hybrid(X_train_scaled, y_train, w_train_np, X_train_nn.index, LOOKBACK, STRIDE)\n",
    "            Xes_seq, yes_seq, wes_seq, idx_es = make_sequences_eod_hybrid(X_valid_es_scaled, y_valid_es, w_valid_es, X_valid_es_df.index, LOOKBACK, STRIDE)\n",
    "            Xsc_seq, ysc_seq, wsc_seq, idx_sc = make_sequences_eod_hybrid(X_valid_sc_scaled, y_valid_sc, w_valid_sc, X_valid_sc_df.index, LOOKBACK, STRIDE)\n",
    "            Xte_seq, yte_seq, wte_seq, idx_te = make_sequences_eod_hybrid(X_test_scaled, y_test, w_test_np, X_test_nn.index, LOOKBACK, STRIDE)\n",
    "            \n",
    "            print(f\"[INFO] Sequence shapes: TRAIN={Xtr_seq.shape} | VALID_ES={Xes_seq.shape} | VALID_SCORE={Xsc_seq.shape} | TEST={Xte_seq.shape}\")\n",
    "            \n",
    "            # Build model\n",
    "            tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
    "            \n",
    "            inp = keras.Input(shape=(LOOKBACK, n_features))\n",
    "            \n",
    "            if hybrid_type == \"hybrid_seq\":\n",
    "                # Sequential: Input \u2192 LSTM \u2192 LayerNorm \u2192 GRU \u2192 Dense \u2192 Output\n",
    "                x = layers.LSTM(LSTM_UNITS, return_sequences=True, dropout=DROPOUT)(inp)\n",
    "                x = layers.LayerNormalization()(x)\n",
    "                x = layers.GRU(GRU_UNITS, return_sequences=False, dropout=DROPOUT)(x)\n",
    "            else:\n",
    "                # Parallel: Input \u2192 [LSTM, GRU] \u2192 Concat \u2192 Dense \u2192 Output\n",
    "                lstm_out = layers.LSTM(LSTM_UNITS, return_sequences=False, dropout=DROPOUT)(inp)\n",
    "                gru_out = layers.GRU(GRU_UNITS, return_sequences=False, dropout=DROPOUT)(inp)\n",
    "                x = layers.Concatenate()([lstm_out, gru_out])\n",
    "            \n",
    "            x = layers.Dense(DENSE_UNITS, activation=DENSE_ACT)(x)\n",
    "            x = layers.Dropout(DROPOUT)(x)\n",
    "            out = layers.Dense(1, activation=OUTPUT_ACT)(x)\n",
    "            \n",
    "            model = keras.Model(inp, out)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=LR, clipnorm=CLIPNORM), loss=LOSS)\n",
    "            print(f\"[INFO] Model built: {model.count_params()} parameters\")\n",
    "            \n",
    "            # Custom callback for SCORE set tracking\n",
    "            class ScoreSetCallback(keras.callbacks.Callback):\n",
    "                def __init__(self, X_score, y_score, w_score):\n",
    "                    super().__init__()\n",
    "                    self.Xs, self.ys, self.ws = X_score, y_score, w_score\n",
    "                    self.best = np.inf\n",
    "                    self.best_weights = None\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    pred = self.model.predict(self.Xs, verbose=0).reshape(-1)\n",
    "                    score = w_rmse(self.ys, pred, self.ws)\n",
    "                    if score < self.best:\n",
    "                        self.best = score\n",
    "                        self.best_weights = self.model.get_weights()\n",
    "            \n",
    "            score_cb = ScoreSetCallback(Xsc_seq, ysc_seq, wsc_seq)\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
    "                score_cb,\n",
    "            ]\n",
    "            \n",
    "            # Train\n",
    "            print(f\"[INFO] Training {hybrid_type.upper()} (epochs={EPOCHS}, batch_size={BATCH_SIZE}, patience={PATIENCE})...\")\n",
    "            history = model.fit(\n",
    "                Xtr_seq, ytr_seq,\n",
    "                sample_weight=wtr_seq,\n",
    "                validation_data=(Xes_seq, yes_seq, wes_seq),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=0,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            \n",
    "            # Restore best weights by SCORE\n",
    "            if score_cb.best_weights is not None:\n",
    "                model.set_weights(score_cb.best_weights)\n",
    "                print(f\"[INFO] Restored best weights by VALID_SCORE wRMSE = {score_cb.best:.6f}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            pred_sc = model.predict(Xsc_seq, verbose=0).reshape(-1)\n",
    "            pred_te = model.predict(Xte_seq, verbose=0).reshape(-1)\n",
    "            baseline_sc, baseline_te = np.zeros_like(ysc_seq), np.zeros_like(yte_seq)\n",
    "            \n",
    "            # Metrics\n",
    "            results = {\n",
    "                \"model_type\": hybrid_type,\n",
    "                \"feature_set\": feature_set,\n",
    "                \"n_features\": n_features,\n",
    "                \"valid_mode\": valid_mode,\n",
    "                \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "                # Config params for comparison\n",
    "                \"hybrid_lstm_units\": LSTM_UNITS,\n",
    "                \"hybrid_gru_units\": GRU_UNITS,\n",
    "                \"dropout\": DROPOUT,\n",
    "                \"learning_rate\": LR,\n",
    "                \"lookback\": LOOKBACK,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"baseline_valid_wrmse\": w_rmse(ysc_seq, baseline_sc, wsc_seq),\n",
    "                \"baseline_valid_diracc\": dir_acc(ysc_seq, baseline_sc),\n",
    "                \"baseline_test_wrmse\": w_rmse(yte_seq, baseline_te, wte_seq),\n",
    "                \"baseline_test_diracc\": dir_acc(yte_seq, baseline_te),\n",
    "                \"model_valid_wrmse\": w_rmse(ysc_seq, pred_sc, wsc_seq),\n",
    "                \"model_valid_wmae\": w_mae(ysc_seq, pred_sc, wsc_seq),\n",
    "                \"model_valid_diracc\": dir_acc(ysc_seq, pred_sc),\n",
    "                \"model_test_wrmse\": w_rmse(yte_seq, pred_te, wte_seq),\n",
    "                \"model_test_wmae\": w_mae(yte_seq, pred_te, wte_seq),\n",
    "                \"model_test_diracc\": dir_acc(yte_seq, pred_te),\n",
    "            }\n",
    "            results[\"valid_wrmse_improvement\"] = results[\"baseline_valid_wrmse\"] - results[\"model_valid_wrmse\"]\n",
    "            results[\"test_wrmse_improvement\"] = results[\"baseline_test_wrmse\"] - results[\"model_test_wrmse\"]\n",
    "            all_hybrid_results.append(results)\n",
    "            \n",
    "            print(f\"\\n[RESULT] {hybrid_type.upper()} | {feature_set} | n_features={n_features}\")\n",
    "            print(f\"  BASELINE VALID_SCORE: wRMSE={results['baseline_valid_wrmse']:.6f} | DirAcc={results['baseline_valid_diracc']:.4f}\")\n",
    "            print(f\"  MODEL    VALID_SCORE: wRMSE={results['model_valid_wrmse']:.6f} | DirAcc={results['model_valid_diracc']:.4f}\")\n",
    "            print(f\"  MODEL    TEST:        wRMSE={results['model_test_wrmse']:.6f} | DirAcc={results['model_test_diracc']:.4f}\")\n",
    "            print(f\"  Improvement (VALID):  {results['valid_wrmse_improvement']:.6f}\")\n",
    "            \n",
    "            # Save model + scaler + config\n",
    "            model_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_{feature_set}.keras\"\n",
    "            scaler_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_{feature_set}_scaler.pkl\"\n",
    "            config_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_{feature_set}_config.json\"\n",
    "            model.save(model_path)\n",
    "            save_pickle(scaler, scaler_path)\n",
    "            save_json(HYB_CFG, config_path)\n",
    "            copy_file(model_path, MODELS_OUT_DRIVE / model_path.name)\n",
    "            copy_file(scaler_path, MODELS_OUT_DRIVE / scaler_path.name)\n",
    "            copy_file(config_path, MODELS_OUT_DRIVE / config_path.name)\n",
    "            \n",
    "            # Predictions -> predictions/{hybrid_type}_{feature_set}/\n",
    "            PRED_HYB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
    "            PRED_HYB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
    "            \n",
    "            preds_valid_df = pd.DataFrame({\n",
    "                \"date\": idx_sc, \"actual\": ysc_seq, \"baseline_zero\": baseline_sc,\n",
    "                \"predicted\": pred_sc, \"sample_weight\": wsc_seq,\n",
    "            }).reset_index(drop=True)\n",
    "            preds_test_df = pd.DataFrame({\n",
    "                \"date\": idx_te, \"actual\": yte_seq, \"baseline_zero\": baseline_te,\n",
    "                \"predicted\": pred_te, \"sample_weight\": wte_seq,\n",
    "            }).reset_index(drop=True)\n",
    "            \n",
    "            preds_valid_df.to_csv(PRED_HYB_LOCAL / \"predictions_valid.csv\", index=False)\n",
    "            preds_test_df.to_csv(PRED_HYB_LOCAL / \"predictions_test.csv\", index=False)\n",
    "            copy_file(PRED_HYB_LOCAL / \"predictions_valid.csv\", PRED_HYB_DRIVE / \"predictions_valid.csv\")\n",
    "            copy_file(PRED_HYB_LOCAL / \"predictions_test.csv\", PRED_HYB_DRIVE / \"predictions_test.csv\")\n",
    "            \n",
    "            print(f\"[OK] Saved: {model_path.name}, {scaler_path.name}\")\n",
    "            print(f\"[OK] Predictions: predictions/{hybrid_type}_{feature_set}/\")\n",
    "        \n",
    "        # Summary for this hybrid type\n",
    "        hybrid_results_df = pd.DataFrame(all_hybrid_results)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[INFO] {hybrid_type.upper()} TRAINING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(hybrid_results_df[[\"feature_set\", \"n_features\", \"model_valid_wrmse\", \"model_test_wrmse\", \"valid_wrmse_improvement\"]])\n",
    "        \n",
    "        summary_path = MODELS_OUT_LOCAL / f\"{hybrid_type}_summary.csv\"\n",
    "        hybrid_results_df.to_csv(summary_path, index=False)\n",
    "        copy_file(summary_path, MODELS_OUT_DRIVE / summary_path.name)\n",
    "        print(f\"[OK] Saved {hybrid_type.upper()} summary: {summary_path.name}\")\n",
    "    \n",
    "    print(\"[OK] BLOCK 28 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9342fa",
   "metadata": {},
   "source": [
    "## BLOCK 29 \u2014 HYBRID NEURAL NETWORK PREDICT TOMORROW + BACKTEST PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae412c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not TF_AVAILABLE:\n",
    "    print(\"[SKIP] BLOCK 29 \u2014 TensorFlow not available\")\n",
    "else:\n",
    "    HYBRID_TYPES = [\"hybrid_seq\", \"hybrid_par\"]\n",
    "    PLOT_CFG = RUN_PARAMS[\"plot\"]\n",
    "    N_PLOT = int(PLOT_CFG[\"n_plot\"])\n",
    "    FIGSIZE = tuple(PLOT_CFG[\"figsize\"])\n",
    "    DPI = int(PLOT_CFG[\"dpi\"])\n",
    "    \n",
    "    # Directories\n",
    "    PROC_DATA_DIR = DATA_DIRS_LOCAL[\"processed\"]\n",
    "    MODELS_DIR_LOCAL = Path(LOCAL_PATHS[\"models_dir\"])\n",
    "    PRED_OUT_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\")\n",
    "    PRED_OUT_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\")\n",
    "    \n",
    "    print(\"[INFO] Hybrid Predictions output dirs:\")\n",
    "    print(\"  - LOCAL:\", PRED_OUT_LOCAL)\n",
    "    print(\"  - DRIVE:\", PRED_OUT_DRIVE)\n",
    "    \n",
    "    # Sequence creation\n",
    "    def make_sequences_pred_hybrid(X_2d, y_1d, idx, lookback):\n",
    "        X_2d, y_1d = np.asarray(X_2d), np.asarray(y_1d)\n",
    "        N, F = X_2d.shape\n",
    "        X_seq, y_seq, idx_seq = [], [], []\n",
    "        for t in range(lookback - 1, N):\n",
    "            X_seq.append(X_2d[t - lookback + 1:t + 1, :])\n",
    "            y_seq.append(y_1d[t])\n",
    "            idx_seq.append(idx[t])\n",
    "        return (np.asarray(X_seq, dtype=np.float32),\n",
    "                np.asarray(y_seq, dtype=np.float32),\n",
    "                pd.DatetimeIndex(idx_seq))\n",
    "    \n",
    "    # Load shared data\n",
    "    y_test = load_with_fallback(\"y_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
    "    w_test = load_with_fallback(\"weights_test.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR)\n",
    "    y_test_arr = y_test.astype(float).to_numpy()\n",
    "    w_test_arr = np.asarray(w_test, dtype=float)\n",
    "    \n",
    "    # Loop over hybrid types\n",
    "    for hybrid_type in HYBRID_TYPES:\n",
    "        HYB_CFG = RUN_PARAMS[hybrid_type]\n",
    "        LOOKBACK = int(HYB_CFG[\"lookback\"])\n",
    "        FEATURE_SETS = HYB_CFG[\"feature_sets\"]\n",
    "        \n",
    "        all_pred_results = []\n",
    "        \n",
    "        arch_name = \"Sequential (LSTM\u2192GRU)\" if hybrid_type == \"hybrid_seq\" else \"Parallel (LSTM\u2225GRU)\"\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# PREDICTING WITH {hybrid_type.upper()} \u2014 {arch_name}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        for feature_set in FEATURE_SETS:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"[INFO] Predicting with {hybrid_type.upper()}: {feature_set}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Load model + scaler\n",
    "            model_path = MODELS_DIR_LOCAL / f\"{hybrid_type}_{feature_set}.keras\"\n",
    "            scaler_path = MODELS_DIR_LOCAL / f\"{hybrid_type}_{feature_set}_scaler.pkl\"\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"[WARN] Model not found: {model_path}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            model = keras.models.load_model(model_path)\n",
    "            scaler = load_pickle(scaler_path)\n",
    "            \n",
    "            # Load X_test\n",
    "            X_test = load_with_fallback(f\"X_test_{feature_set}.pkl\", PROC_DATA_DIR, DRIVE_PROC_DATA_DIR, use_pandas=True)\n",
    "            n_features = X_test.shape[1]\n",
    "            print(f\"[INFO] Loaded: model={model_path.name} | X_test={X_test.shape}\")\n",
    "            \n",
    "            # Scale + Sequences\n",
    "            X_test_scaled = scaler.transform(X_test.values)\n",
    "            X_seq, y_seq, idx_seq = make_sequences_pred_hybrid(X_test_scaled, y_test_arr, X_test.index, LOOKBACK)\n",
    "            print(f\"[INFO] Sequences: {X_seq.shape}\")\n",
    "            \n",
    "            # Predict\n",
    "            pred_seq = model.predict(X_seq, verbose=0).reshape(-1)\n",
    "            hist_df = pd.DataFrame({\"date\": idx_seq, \"actual\": y_seq, \"y_pred\": pred_seq}).set_index(\"date\")\n",
    "            \n",
    "            # Tomorrow prediction\n",
    "            last_date = X_test.index[-1]\n",
    "            X_last_window = X_test_scaled[-LOOKBACK:, :].reshape(1, LOOKBACK, -1).astype(np.float32)\n",
    "            pred_tomorrow = float(model.predict(X_last_window, verbose=0).reshape(-1)[0])\n",
    "            pred_tomorrow_date = last_date + pd.Timedelta(days=1)\n",
    "            \n",
    "            # Metrics\n",
    "            w_seq = w_test_arr[LOOKBACK - 1:]\n",
    "            w_norm = w_seq / (w_seq.sum() + EPS)\n",
    "            test_wrmse = float(np.sqrt(np.sum(w_norm * (y_seq - pred_seq) ** 2)))\n",
    "            test_dir_acc = float(np.mean((y_seq > 0) == (pred_seq > 0)))\n",
    "            \n",
    "            print(f\"[INFO] TEST wRMSE={test_wrmse:.6f} | DirAcc={test_dir_acc:.4f}\")\n",
    "            print(f\"[INFO] Tomorrow prediction: {pred_tomorrow:.6f} ({np.expm1(pred_tomorrow)*100:.4f}%)\")\n",
    "            \n",
    "            result = {\n",
    "                \"model_type\": hybrid_type,\n",
    "                \"feature_set\": feature_set,\n",
    "                \"n_features\": n_features,\n",
    "                \"test_wrmse\": test_wrmse,\n",
    "                \"test_dir_acc\": test_dir_acc,\n",
    "                \"last_data_date\": str(last_date.date()),\n",
    "                \"pred_tomorrow_logret\": pred_tomorrow,\n",
    "                \"pred_tomorrow_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
    "            }\n",
    "            all_pred_results.append(result)\n",
    "            \n",
    "            # Save -> predictions/{hybrid_type}_{feature_set}/\n",
    "            PRED_HYB_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
    "            PRED_HYB_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"run_dir\"]) / \"predictions\" / f\"{hybrid_type}_{feature_set}\")\n",
    "            \n",
    "            pred_tomorrow_df = pd.DataFrame([{\n",
    "                \"feature_set\": feature_set, \"last_data_date\": last_date,\n",
    "                \"predicted_for\": \"next_trading_day\",\n",
    "                \"pred_logret\": pred_tomorrow,\n",
    "                \"pred_return_pct\": float(np.expm1(pred_tomorrow) * 100),\n",
    "            }])\n",
    "            pred_tomorrow_df.to_csv(PRED_HYB_LOCAL / \"tomorrow.csv\", index=False)\n",
    "            copy_file(PRED_HYB_LOCAL / \"tomorrow.csv\", PRED_HYB_DRIVE / \"tomorrow.csv\")\n",
    "            \n",
    "            hist_tail = hist_df.tail(N_PLOT).copy()\n",
    "            hist_tail.to_csv(PRED_HYB_LOCAL / \"backtest.csv\")\n",
    "            copy_file(PRED_HYB_LOCAL / \"backtest.csv\", PRED_HYB_DRIVE / \"backtest.csv\")\n",
    "            \n",
    "            # Plot\n",
    "            fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "            ax.plot(hist_tail.index, hist_tail[\"actual\"].values, linewidth=1, label=\"Actual\")\n",
    "            ax.plot(hist_tail.index, hist_tail[\"y_pred\"].values, linewidth=1, label=\"Predicted (y_pred)\")\n",
    "            ax.scatter([pred_tomorrow_date], [pred_tomorrow], s=90, marker=\"X\", color=\"red\", label=f\"Tomorrow: {pred_tomorrow:.4f}\")\n",
    "            ax.axhline(0.0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
    "            ax.set_title(f\"{hybrid_type.upper()} Predictions \u2014 {feature_set} \u2014 last {len(hist_tail)} days + tomorrow\")\n",
    "            ax.set_xlabel(\"Date\")\n",
    "            ax.set_ylabel(\"Log Return\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PRED_HYB_LOCAL / \"plot.png\", dpi=DPI)\n",
    "            copy_file(PRED_HYB_LOCAL / \"plot.png\", PRED_HYB_DRIVE / \"plot.png\")\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"[OK] Saved: predictions/{hybrid_type}_{feature_set}/ (tomorrow.csv, backtest.csv, plot.png)\")\n",
    "        \n",
    "        # Summary for this hybrid type\n",
    "        pred_summary_df = pd.DataFrame(all_pred_results)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[INFO] {hybrid_type.upper()} PREDICTION SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(pred_summary_df)\n",
    "        \n",
    "        summary_path = PRED_OUT_LOCAL / f\"{hybrid_type}_predictions_summary.csv\"\n",
    "        pred_summary_df.to_csv(summary_path, index=False)\n",
    "        copy_file(summary_path, PRED_OUT_DRIVE / summary_path.name)\n",
    "        print(f\"[OK] Saved summary: {summary_path.name}\")\n",
    "    \n",
    "    print(\"[OK] BLOCK 29 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 9B: Ensemble Model\n",
    "\n",
    "**Combine predictions from all models**\n",
    "\n",
    "Methods:\n",
    "- Simple Average\n",
    "- Weighted Average (by inverse wRMSE)\n",
    "- Stacking (meta-model)\n",
    "- Rank Average\n",
    "\n",
    "**Prerequisites:** Run Sections 7, 7B, 8, 9 first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 30 \u2014 ENSEMBLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n# ============================================================\n# BLOCK 30 \u2014 ENSEMBLE MODEL\n# ============================================================\n# Supports: simple_average, weighted_average, rank_average, stacking\n# Weight methods: inverse_wrmse, inverse_wrmse_squared\n\n# Config\nENSEMBLE_CFG = RUN_PARAMS.get(\"ensemble\", {})\nENSEMBLE_METHOD = ENSEMBLE_CFG.get(\"method\", \"weighted_average\")\nENSEMBLE_MODELS = ENSEMBLE_CFG.get(\"models\", [\"xgb\", \"lgb\", \"lstm\", \"gru\", \"hybrid_seq\", \"hybrid_par\"])\nWEIGHT_METHOD = ENSEMBLE_CFG.get(\"weight_method\", \"inverse_wrmse\")\n\nprint(f\"[INFO] Ensemble method: {ENSEMBLE_METHOD}\")\nprint(f\"[INFO] Weight method: {WEIGHT_METHOD}\")\nprint(f\"[INFO] Models to combine: {ENSEMBLE_MODELS}\")\n\n# Directories\nMODELS_DIR_LOCAL = Path(LOCAL_PATHS[\"models_dir\"])\nMODELS_DIR_DRIVE = Path(DRIVE_PATHS[\"models_dir\"])\nOUTPUTS_LOCAL = ensure_dir(Path(LOCAL_PATHS[\"outputs_dir\"]))\nOUTPUTS_DRIVE = ensure_dir(Path(DRIVE_PATHS[\"outputs_dir\"]))\n\n# -------------------------\n# 1. Load predictions from all models\n# -------------------------\nprint(\"\\n[INFO] Loading predictions...\")\n\ndef load_predictions_ensemble(run_dir: Path, split: str, models: list) -> Tuple[pd.DataFrame, Optional[np.ndarray], Optional[np.ndarray]]:\n    \"\"\"Load predictions from available models.\n    \n    All predictions are stored in: run_dir/predictions/{model}/ or run_dir/predictions/{model}_{feature_set}/\n    \n    Returns:\n        Tuple of (predictions_df, actual_values, sample_weights)\n    \"\"\"\n    pred_base = run_dir / \"predictions\"\n    \n    # Model prediction paths (model_key -> list of possible paths)\n    model_paths = {\n        \"xgb\": [pred_base / \"xgb\" / f\"predictions_{split}.csv\"],\n        \"lgb\": [pred_base / \"lgb\" / f\"predictions_{split}.csv\"],\n        \"lstm\": [\n            pred_base / \"lstm_neural_40\" / f\"predictions_{split}.csv\",\n            pred_base / \"lstm_xgb_selected\" / f\"predictions_{split}.csv\",\n        ],\n        \"gru\": [\n            pred_base / \"gru_neural_40\" / f\"predictions_{split}.csv\",\n            pred_base / \"gru_xgb_selected\" / f\"predictions_{split}.csv\",\n        ],\n        \"hybrid_seq\": [\n            pred_base / \"hybrid_seq_neural_40\" / f\"predictions_{split}.csv\",\n            pred_base / \"hybrid_seq_xgb_selected\" / f\"predictions_{split}.csv\",\n        ],\n        \"hybrid_par\": [\n            pred_base / \"hybrid_par_neural_40\" / f\"predictions_{split}.csv\",\n            pred_base / \"hybrid_par_xgb_selected\" / f\"predictions_{split}.csv\",\n        ],\n    }\n    \n    predictions = {}\n    actual = None\n    weights = None\n    \n    for model_name in models:\n        if model_name not in model_paths:\n            continue\n        \n        # Find first existing file\n        file_path = None\n        for path in model_paths[model_name]:\n            if path.exists():\n                file_path = path\n                break\n        \n        if file_path is None:\n            print(f\"  [SKIP] {model_name}: file not found\")\n            continue\n        \n        df = pd.read_csv(file_path)\n        \n        # Handle different column names\n        pred_col = None\n        for col in [\"predicted\", \"prediction\", \"y_pred_model\", \"y_pred\"]:\n            if col in df.columns:\n                pred_col = col\n                break\n        \n        actual_col = None\n        for col in [\"actual\", \"y_true\"]:\n            if col in df.columns:\n                actual_col = col\n                break\n        \n        if pred_col:\n            predictions[model_name] = df[pred_col].values\n            if actual is None and actual_col:\n                actual = df[actual_col].values\n            # Load sample weights if available\n            if weights is None and \"sample_weight\" in df.columns:\n                weights = df[\"sample_weight\"].values\n            print(f\"  [OK] {model_name}: {len(predictions[model_name])} predictions\")\n    \n    if len(predictions) == 0:\n        return pd.DataFrame(), None, None\n    \n    # Align lengths\n    min_len = min(len(p) for p in predictions.values())\n    pred_df = pd.DataFrame({name: pred[:min_len] for name, pred in predictions.items()})\n    \n    return (pred_df, \n            actual[:min_len] if actual is not None else None,\n            weights[:min_len] if weights is not None else None)\n\n\n# Load from local first, then drive\nvalid_pred, valid_actual, valid_weights_loaded = None, None, None\ntest_pred, test_actual, test_weights_loaded = None, None, None\n\n# Get run directories (parent of models_dir)\nRUN_DIR_LOCAL = MODELS_DIR_LOCAL.parent\nRUN_DIR_DRIVE = MODELS_DIR_DRIVE.parent\n\nfor run_dir in [RUN_DIR_LOCAL, RUN_DIR_DRIVE]:\n    if (run_dir / \"predictions\").exists():\n        if valid_pred is None or len(valid_pred.columns) == 0:\n            valid_pred, valid_actual, valid_weights_loaded = load_predictions_ensemble(run_dir, \"valid\", ENSEMBLE_MODELS)\n        if test_pred is None or len(test_pred.columns) == 0:\n            test_pred, test_actual, test_weights_loaded = load_predictions_ensemble(run_dir, \"test\", ENSEMBLE_MODELS)\n\nif valid_pred is None or len(valid_pred.columns) == 0 or test_pred is None or len(test_pred.columns) == 0:\n    print(\"[ERROR] No predictions found! Run model training first.\")\n    ENSEMBLE_METRICS = None\nelse:\n    print(f\"\\n[INFO] Loaded models: {list(valid_pred.columns)}\")\n    print(f\"[INFO] Valid predictions: {len(valid_pred)} samples\")\n    print(f\"[INFO] Test predictions: {len(test_pred)} samples\")\n    \n    # -------------------------\n    # 2. Load metrics for weighting\n    # -------------------------\n    print(\"\\n[INFO] Loading model metrics...\")\n    \n    model_metrics = {}\n    \n    metric_files = {\n        \"xgb\": \"final_metrics.csv\",\n        \"lgb\": \"final_metrics_lgb.csv\",\n    }\n    \n    for model_name in valid_pred.columns:\n        if model_name in metric_files:\n            for models_dir in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n                file_path = models_dir / metric_files[model_name]\n                if file_path.exists():\n                    df = pd.read_csv(file_path)\n                    if \"test_wrmse\" in df.columns:\n                        model_metrics[model_name] = {\"test_wrmse\": float(df[\"test_wrmse\"].iloc[0])}\n                    elif \"wRMSE\" in df.columns:\n                        test_row = df[df.get(\"split\", \"\") == \"TEST\"]\n                        if len(test_row) > 0:\n                            model_metrics[model_name] = {\"test_wrmse\": float(test_row[\"wRMSE\"].iloc[0])}\n                    break\n        \n        # Check for summary files (neural networks)\n        for models_dir in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n            summary_path = models_dir / f\"{model_name}_summary.csv\"\n            if summary_path.exists() and model_name not in model_metrics:\n                df = pd.read_csv(summary_path)\n                if \"model_test_wrmse\" in df.columns:\n                    model_metrics[model_name] = {\"test_wrmse\": float(df[\"model_test_wrmse\"].iloc[0])}\n                break\n    \n    for name, metrics in model_metrics.items():\n        print(f\"  {name}: wRMSE = {metrics['test_wrmse']:.6f}\")\n    \n    # -------------------------\n    # 3. Compute ensemble weights\n    # -------------------------\n    print(f\"\\n[INFO] Computing ensemble weights ({WEIGHT_METHOD})...\")\n    \n    if ENSEMBLE_METHOD == \"simple_average\":\n        weights = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n        print(\"  Using equal weights (simple_average)\")\n        \n    elif ENSEMBLE_METHOD in [\"weighted_average\", \"stacking\"]:\n        if len(model_metrics) > 0:\n            if WEIGHT_METHOD == \"inverse_wrmse\":\n                raw_weights = {m: 1.0 / (model_metrics[m][\"test_wrmse\"] + EPS) \n                              for m in valid_pred.columns if m in model_metrics}\n            elif WEIGHT_METHOD == \"inverse_wrmse_squared\":\n                raw_weights = {m: 1.0 / ((model_metrics[m][\"test_wrmse\"] ** 2) + EPS) \n                              for m in valid_pred.columns if m in model_metrics}\n            else:\n                raise ValueError(f\"Unknown weight_method: {WEIGHT_METHOD}\")\n            \n            total = sum(raw_weights.values())\n            weights = {m: w / total for m, w in raw_weights.items()}\n            \n            # Add equal weight for models without metrics\n            for m in valid_pred.columns:\n                if m not in weights:\n                    weights[m] = 1.0 / len(valid_pred.columns)\n                    print(f\"  [WARN] No metrics for {m}, using equal weight\")\n        else:\n            weights = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n            print(\"  [WARN] No model metrics found, using equal weights\")\n            \n    elif ENSEMBLE_METHOD == \"rank_average\":\n        weights = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n        print(\"  Using equal weights (rank_average)\")\n    \n    else:\n        raise ValueError(f\"Unknown ensemble method: {ENSEMBLE_METHOD}\")\n    \n    print(f\"  Weights: {weights}\")\n    \n    # -------------------------\n    # 4. Generate ensemble predictions\n    # -------------------------\n    print(f\"\\n[INFO] Running ensemble ({ENSEMBLE_METHOD})...\")\n    \n    if ENSEMBLE_METHOD == \"simple_average\":\n        ensemble_valid = valid_pred.mean(axis=1).values\n        ensemble_test = test_pred.mean(axis=1).values\n        \n    elif ENSEMBLE_METHOD == \"weighted_average\":\n        ensemble_valid = np.zeros(len(valid_pred))\n        ensemble_test = np.zeros(len(test_pred))\n        for model, weight in weights.items():\n            if model in valid_pred.columns:\n                ensemble_valid += weight * valid_pred[model].values\n                ensemble_test += weight * test_pred[model].values\n                \n    elif ENSEMBLE_METHOD == \"rank_average\":\n        # Rank-based averaging\n        valid_ranks = valid_pred.rank(pct=True)\n        test_ranks = test_pred.rank(pct=True)\n        \n        avg_valid_rank = valid_ranks.mean(axis=1)\n        avg_test_rank = test_ranks.mean(axis=1)\n        \n        # Convert back to prediction scale using inverse normal CDF\n        from scipy import stats\n        mean_pred = valid_pred.mean(axis=1).mean()\n        std_pred = valid_pred.std(axis=1).mean()\n        \n        ensemble_valid = mean_pred + std_pred * stats.norm.ppf(avg_valid_rank.clip(0.001, 0.999))\n        ensemble_test = mean_pred + std_pred * stats.norm.ppf(avg_test_rank.clip(0.001, 0.999))\n        \n    elif ENSEMBLE_METHOD == \"stacking\":\n        from sklearn.linear_model import Ridge\n        \n        meta_params = ENSEMBLE_CFG.get(\"meta_params\", {\"alpha\": 1.0})\n        meta_model = Ridge(**meta_params)\n        meta_model.fit(valid_pred.values, valid_actual)\n        \n        ensemble_valid = meta_model.predict(valid_pred.values)\n        ensemble_test = meta_model.predict(test_pred.values)\n        \n        # Update weights with stacking coefficients\n        weights = dict(zip(valid_pred.columns, meta_model.coef_))\n        print(f\"  Stacking coefficients: {weights}\")\n    \n    # -------------------------\n    # 5. Compute metrics\n    # -------------------------\n    print(\"\\n[INFO] Computing ensemble metrics...\")\n    \n    # Load sample weights (fallback to loaded or uniform)\n    valid_weights = valid_weights_loaded\n    test_weights = test_weights_loaded\n    \n    if valid_weights is None:\n        try:\n            valid_weights = load_with_fallback(\"weights_valid.pkl\", \n                Path(LOCAL_PATHS[\"proc_dir\"]), DATA_DIRS_LOCAL[\"processed\"],\n                Path(DRIVE_PATHS[\"proc_dir\"]), DATA_DIRS_DRIVE[\"processed\"])[:len(valid_actual)]\n        except:\n            valid_weights = np.ones(len(valid_actual))\n            print(\"  [WARN] Using uniform weights for validation\")\n    \n    if test_weights is None:\n        try:\n            test_weights = load_with_fallback(\"weights_test.pkl\",\n                Path(LOCAL_PATHS[\"proc_dir\"]), DATA_DIRS_LOCAL[\"processed\"],\n                Path(DRIVE_PATHS[\"proc_dir\"]), DATA_DIRS_DRIVE[\"processed\"])[:len(test_actual)]\n        except:\n            test_weights = np.ones(len(test_actual))\n            print(\"  [WARN] Using uniform weights for test\")\n    \n    ensemble_metrics = {\n        \"model\": f\"Ensemble-{ENSEMBLE_METHOD}\",\n        \"method\": ENSEMBLE_METHOD,\n        \"weight_method\": WEIGHT_METHOD,\n        \"models\": list(valid_pred.columns),\n        \"weights\": weights,\n        \"valid_wrmse\": w_rmse(valid_actual, ensemble_valid, valid_weights),\n        \"valid_wmae\": w_mae(valid_actual, ensemble_valid, valid_weights),\n        \"valid_diracc\": dir_acc(valid_actual, ensemble_valid),\n        \"test_wrmse\": w_rmse(test_actual, ensemble_test, test_weights),\n        \"test_wmae\": w_mae(test_actual, ensemble_test, test_weights),\n        \"test_diracc\": dir_acc(test_actual, ensemble_test),\n    }\n    \n    print(f\"\\n[RESULTS] Ensemble ({ENSEMBLE_METHOD}):\")\n    print(f\"  Valid wRMSE: {ensemble_metrics['valid_wrmse']:.6f} | wMAE: {ensemble_metrics['valid_wmae']:.6f} | DirAcc: {ensemble_metrics['valid_diracc']:.4f}\")\n    print(f\"  Test  wRMSE: {ensemble_metrics['test_wrmse']:.6f} | wMAE: {ensemble_metrics['test_wmae']:.6f} | DirAcc: {ensemble_metrics['test_diracc']:.4f}\")\n    \n    # -------------------------\n    # 6. Compare with individual models\n    # -------------------------\n    print(\"\\n[COMPARE] Individual models vs Ensemble:\")\n    print(f\"  {'Model':<15} {'Test wRMSE':<12} {'Improvement':<12}\")\n    print(f\"  {'-'*39}\")\n    for model_name, metrics in model_metrics.items():\n        model_wrmse = metrics[\"test_wrmse\"]\n        improvement = (model_wrmse - ensemble_metrics[\"test_wrmse\"]) / model_wrmse * 100\n        print(f\"  {model_name:<15} {model_wrmse:<12.6f} {improvement:>+.2f}%\")\n    print(f\"  {'ENSEMBLE':<15} {ensemble_metrics['test_wrmse']:<12.6f} {'---':<12}\")\n    \n    # -------------------------\n    # 7. Save outputs (LOCAL + DRIVE)\n    # -------------------------\n    print(f\"\\n[INFO] Saving ensemble outputs...\")\n    print(f\"  LOCAL:  {OUTPUTS_LOCAL}\")\n    print(f\"  DRIVE:  {OUTPUTS_DRIVE}\")\n    \n    # 7.1 Predictions with sample weights - Valid\n    valid_df = pd.DataFrame({\n        \"actual\": valid_actual,\n        \"predicted\": ensemble_valid,\n        \"sample_weight\": valid_weights[:len(valid_actual)],\n    })\n    valid_df.to_csv(OUTPUTS_LOCAL / \"ensemble_predictions_valid.csv\", index=False)\n    copy_file(OUTPUTS_LOCAL / \"ensemble_predictions_valid.csv\", OUTPUTS_DRIVE / \"ensemble_predictions_valid.csv\")\n    \n    # 7.2 Predictions with sample weights - Test\n    test_df = pd.DataFrame({\n        \"actual\": test_actual,\n        \"predicted\": ensemble_test,\n        \"sample_weight\": test_weights[:len(test_actual)],\n    })\n    test_df.to_csv(OUTPUTS_LOCAL / \"ensemble_predictions_test.csv\", index=False)\n    copy_file(OUTPUTS_LOCAL / \"ensemble_predictions_test.csv\", OUTPUTS_DRIVE / \"ensemble_predictions_test.csv\")\n    \n    # 7.3 Metrics CSV\n    metrics_df = pd.DataFrame([{\n        \"model\": ensemble_metrics[\"model\"],\n        \"method\": ensemble_metrics[\"method\"],\n        \"weight_method\": ensemble_metrics[\"weight_method\"],\n        \"n_models\": len(ensemble_metrics[\"models\"]),\n        \"valid_wrmse\": ensemble_metrics[\"valid_wrmse\"],\n        \"valid_wmae\": ensemble_metrics[\"valid_wmae\"],\n        \"valid_diracc\": ensemble_metrics[\"valid_diracc\"],\n        \"test_wrmse\": ensemble_metrics[\"test_wrmse\"],\n        \"test_wmae\": ensemble_metrics[\"test_wmae\"],\n        \"test_diracc\": ensemble_metrics[\"test_diracc\"],\n    }])\n    metrics_df.to_csv(OUTPUTS_LOCAL / \"ensemble_metrics.csv\", index=False)\n    copy_file(OUTPUTS_LOCAL / \"ensemble_metrics.csv\", OUTPUTS_DRIVE / \"ensemble_metrics.csv\")\n    \n    # 7.4 Full results JSON\n    save_json(ensemble_metrics, OUTPUTS_LOCAL / \"ensemble_results.json\")\n    copy_file(OUTPUTS_LOCAL / \"ensemble_results.json\", OUTPUTS_DRIVE / \"ensemble_results.json\")\n    \n    # 7.5 Weights JSON\n    save_json(weights, OUTPUTS_LOCAL / \"ensemble_weights.json\")\n    copy_file(OUTPUTS_LOCAL / \"ensemble_weights.json\", OUTPUTS_DRIVE / \"ensemble_weights.json\")\n    \n    # 7.6 Save stacking meta-model if used\n    if ENSEMBLE_METHOD == \"stacking\":\n        save_pickle(meta_model, OUTPUTS_LOCAL / \"ensemble_meta_model.pkl\")\n        copy_file(OUTPUTS_LOCAL / \"ensemble_meta_model.pkl\", OUTPUTS_DRIVE / \"ensemble_meta_model.pkl\")\n        print(f\"  - ensemble_meta_model.pkl\")\n    \n    # 7.7 Tomorrow prediction (ensemble of individual model tomorrow predictions)\n    PRED_BASE = Path(LOCAL_PATHS[\"predictions_dir\"])\n    \n    tomorrow_paths = {\n        \"xgb\": PRED_BASE / \"xgb\" / \"tomorrow.csv\",\n        \"lgb\": PRED_BASE / \"lgb\" / \"tomorrow.csv\",\n        \"lstm\": PRED_BASE / \"lstm_xgb_selected\" / \"tomorrow.csv\",\n        \"gru\": PRED_BASE / \"gru_xgb_selected\" / \"tomorrow.csv\",\n        \"hybrid_seq\": PRED_BASE / \"hybrid_seq_xgb_selected\" / \"tomorrow.csv\",\n        \"hybrid_par\": PRED_BASE / \"hybrid_par_xgb_selected\" / \"tomorrow.csv\",\n    }\n    \n    tomorrow_preds = {}\n    for model_name, path in tomorrow_paths.items():\n        if path.exists():\n            df = pd.read_csv(path)\n            if \"pred_logret\" in df.columns and len(df) > 0:\n                tomorrow_preds[model_name] = float(df[\"pred_logret\"].iloc[0])\n    \n    if tomorrow_preds:\n        print(f\"\\n[INFO] Tomorrow predictions from {len(tomorrow_preds)} models\")\n        \n        # Calculate weighted average\n        total_weight = 0.0\n        weighted_sum = 0.0\n        \n        for model_name, pred in tomorrow_preds.items():\n            weight = weights.get(model_name, 0.0)\n            if weight > 0:\n                weighted_sum += weight * pred\n                total_weight += weight\n                print(f\"  {model_name}: {pred:.6f} (weight: {weight:.4f})\")\n        \n        if total_weight > 0:\n            ensemble_tomorrow = weighted_sum / total_weight\n            ensemble_tomorrow_pct = float(np.expm1(ensemble_tomorrow) * 100)\n            \n            print(f\"\\n[INFO] ENSEMBLE Tomorrow prediction: {ensemble_tomorrow:.6f} ({ensemble_tomorrow_pct:.4f}%)\")\n            \n            # Save tomorrow prediction\n            tomorrow_df = pd.DataFrame([{\n                \"method\": ENSEMBLE_METHOD,\n                \"n_models\": len(tomorrow_preds),\n                \"predicted_for\": \"next_trading_day\",\n                \"pred_logret\": ensemble_tomorrow,\n                \"pred_return_pct\": ensemble_tomorrow_pct,\n            }])\n            tomorrow_df.to_csv(OUTPUTS_LOCAL / \"tomorrow.csv\", index=False)\n            copy_file(OUTPUTS_LOCAL / \"tomorrow.csv\", OUTPUTS_DRIVE / \"tomorrow.csv\")\n            \n            # Update metrics with tomorrow prediction\n            ensemble_metrics[\"pred_tomorrow_logret\"] = ensemble_tomorrow\n            ensemble_metrics[\"pred_tomorrow_pct\"] = ensemble_tomorrow_pct\n            save_json(ensemble_metrics, OUTPUTS_LOCAL / \"ensemble_results.json\")\n            copy_file(OUTPUTS_LOCAL / \"ensemble_results.json\", OUTPUTS_DRIVE / \"ensemble_results.json\")\n            \n            print(f\"[OK] Saved: tomorrow.csv\")\n    \n    print(f\"\\n[OK] Saved ensemble outputs:\")\n    print(f\"  - ensemble_predictions_valid.csv\")\n    print(f\"  - ensemble_predictions_test.csv\")\n    print(f\"  - ensemble_metrics.csv\")\n    print(f\"  - ensemble_results.json\")\n    print(f\"  - ensemble_weights.json\")\n    \n    # Store for summary\n    ENSEMBLE_METRICS = ensemble_metrics\n\n\n# ============================================================\n# COMPARE ENSEMBLE METHODS (Optional)\n# ============================================================\ndef compare_ensemble_methods(\n    run_dir_local: Path,\n    run_dir_drive: Path,\n    models: list,\n    methods: list = None,\n    weight_methods: list = None,\n) -> pd.DataFrame:\n    \"\"\"Compare different ensemble methods.\n    \n    Args:\n        run_dir_local: Local run directory\n        run_dir_drive: Drive run directory\n        models: List of models to include\n        methods: List of ensemble methods to compare (default: all)\n        weight_methods: List of weight methods to compare (default: all)\n    \n    Returns:\n        DataFrame with comparison results\n    \"\"\"\n    if methods is None:\n        methods = [\"simple_average\", \"weighted_average\", \"rank_average\", \"stacking\"]\n    if weight_methods is None:\n        weight_methods = [\"inverse_wrmse\", \"inverse_wrmse_squared\"]\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ENSEMBLE METHOD COMPARISON\")\n    print(\"=\"*60)\n    \n    all_results = []\n    \n    for method in methods:\n        for wm in weight_methods:\n            # Skip weight_method for methods that don't use it\n            if method in [\"simple_average\", \"rank_average\"] and wm != \"inverse_wrmse\":\n                continue\n            \n            print(f\"\\n[INFO] Testing: {method} + {wm}\")\n            \n            try:\n                # Load predictions\n                valid_pred, valid_actual, valid_weights = None, None, None\n                test_pred, test_actual, test_weights = None, None, None\n                \n                for run_dir in [run_dir_local, run_dir_drive]:\n                    if (run_dir / \"predictions\").exists():\n                        if valid_pred is None or len(valid_pred.columns) == 0:\n                            valid_pred, valid_actual, valid_weights = load_predictions_ensemble(run_dir, \"valid\", models)\n                        if test_pred is None or len(test_pred.columns) == 0:\n                            test_pred, test_actual, test_weights = load_predictions_ensemble(run_dir, \"test\", models)\n                \n                if valid_pred is None or len(valid_pred.columns) == 0:\n                    print(\"  [SKIP] No predictions found\")\n                    continue\n                \n                # Load model metrics\n                model_metrics_cmp = {}\n                metric_files_cmp = {\"xgb\": \"final_metrics.csv\", \"lgb\": \"final_metrics_lgb.csv\"}\n                \n                for mn in valid_pred.columns:\n                    if mn in metric_files_cmp:\n                        for md in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n                            fp = md / metric_files_cmp[mn]\n                            if fp.exists():\n                                df = pd.read_csv(fp)\n                                if \"test_wrmse\" in df.columns:\n                                    model_metrics_cmp[mn] = {\"test_wrmse\": float(df[\"test_wrmse\"].iloc[0])}\n                                elif \"wRMSE\" in df.columns:\n                                    tr = df[df.get(\"split\", \"\") == \"TEST\"]\n                                    if len(tr) > 0:\n                                        model_metrics_cmp[mn] = {\"test_wrmse\": float(tr[\"wRMSE\"].iloc[0])}\n                                break\n                    \n                    for md in [MODELS_DIR_LOCAL, MODELS_DIR_DRIVE]:\n                        sp = md / f\"{mn}_summary.csv\"\n                        if sp.exists() and mn not in model_metrics_cmp:\n                            df = pd.read_csv(sp)\n                            if \"model_test_wrmse\" in df.columns:\n                                model_metrics_cmp[mn] = {\"test_wrmse\": float(df[\"model_test_wrmse\"].iloc[0])}\n                            break\n                \n                # Compute weights\n                if method == \"simple_average\" or method == \"rank_average\":\n                    w = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n                elif len(model_metrics_cmp) > 0:\n                    if wm == \"inverse_wrmse\":\n                        raw_w = {m: 1.0 / (model_metrics_cmp[m][\"test_wrmse\"] + EPS) \n                                for m in valid_pred.columns if m in model_metrics_cmp}\n                    else:  # inverse_wrmse_squared\n                        raw_w = {m: 1.0 / ((model_metrics_cmp[m][\"test_wrmse\"] ** 2) + EPS) \n                                for m in valid_pred.columns if m in model_metrics_cmp}\n                    total_w = sum(raw_w.values())\n                    w = {m: ww / total_w for m, ww in raw_w.items()}\n                    for m in valid_pred.columns:\n                        if m not in w:\n                            w[m] = 1.0 / len(valid_pred.columns)\n                else:\n                    w = {m: 1.0 / len(valid_pred.columns) for m in valid_pred.columns}\n                \n                # Generate predictions\n                if method == \"simple_average\":\n                    ens_valid = valid_pred.mean(axis=1).values\n                    ens_test = test_pred.mean(axis=1).values\n                elif method == \"weighted_average\":\n                    ens_valid = np.zeros(len(valid_pred))\n                    ens_test = np.zeros(len(test_pred))\n                    for m, ww in w.items():\n                        if m in valid_pred.columns:\n                            ens_valid += ww * valid_pred[m].values\n                            ens_test += ww * test_pred[m].values\n                elif method == \"rank_average\":\n                    from scipy import stats\n                    v_ranks = valid_pred.rank(pct=True).mean(axis=1)\n                    t_ranks = test_pred.rank(pct=True).mean(axis=1)\n                    mean_p = valid_pred.mean(axis=1).mean()\n                    std_p = valid_pred.std(axis=1).mean()\n                    ens_valid = mean_p + std_p * stats.norm.ppf(v_ranks.clip(0.001, 0.999))\n                    ens_test = mean_p + std_p * stats.norm.ppf(t_ranks.clip(0.001, 0.999))\n                elif method == \"stacking\":\n                    from sklearn.linear_model import Ridge\n                    meta = Ridge(alpha=1.0)\n                    meta.fit(valid_pred.values, valid_actual)\n                    ens_valid = meta.predict(valid_pred.values)\n                    ens_test = meta.predict(test_pred.values)\n                    w = dict(zip(valid_pred.columns, meta.coef_))\n                \n                # Load sample weights\n                sw_valid = valid_weights if valid_weights is not None else np.ones(len(valid_actual))\n                sw_test = test_weights if test_weights is not None else np.ones(len(test_actual))\n                \n                # Compute metrics\n                result = {\n                    \"method\": method,\n                    \"weight_method\": wm,\n                    \"n_models\": len(valid_pred.columns),\n                    \"valid_wrmse\": w_rmse(valid_actual, ens_valid, sw_valid),\n                    \"valid_diracc\": dir_acc(valid_actual, ens_valid),\n                    \"test_wrmse\": w_rmse(test_actual, ens_test, sw_test),\n                    \"test_diracc\": dir_acc(test_actual, ens_test),\n                }\n                all_results.append(result)\n                print(f\"  Test wRMSE: {result['test_wrmse']:.6f} | DirAcc: {result['test_diracc']:.4f}\")\n                \n            except Exception as e:\n                print(f\"  [ERROR] {e}\")\n    \n    if len(all_results) == 0:\n        print(\"\\n[WARN] No results to compare\")\n        return pd.DataFrame()\n    \n    # Create comparison DataFrame\n    cmp_df = pd.DataFrame(all_results).sort_values(\"test_wrmse\").reset_index(drop=True)\n    cmp_df[\"rank\"] = range(1, len(cmp_df) + 1)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPARISON RESULTS (sorted by Test wRMSE)\")\n    print(\"=\"*60)\n    print(cmp_df[[\"rank\", \"method\", \"weight_method\", \"test_wrmse\", \"test_diracc\"]].to_string(index=False))\n    \n    # Save comparison\n    cmp_df.to_csv(OUTPUTS_LOCAL / \"ensemble_comparison.csv\", index=False)\n    copy_file(OUTPUTS_LOCAL / \"ensemble_comparison.csv\", OUTPUTS_DRIVE / \"ensemble_comparison.csv\")\n    print(f\"\\n[OK] Saved ensemble_comparison.csv\")\n    \n    return cmp_df\n\n\n# Run comparison if enabled in config\nif ENSEMBLE_CFG.get(\"compare_methods\", False):\n    ENSEMBLE_COMPARISON = compare_ensemble_methods(\n        run_dir_local=RUN_DIR_LOCAL,\n        run_dir_drive=RUN_DIR_DRIVE,\n        models=ENSEMBLE_MODELS,\n    )\nelse:\n    ENSEMBLE_COMPARISON = None\n\nprint(\"\\n[OK] BLOCK 30 complete.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 31 \u2014 SUMMARY & RESULTS EXPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "\n# ============================================================\n# BLOCK 31 \u2014 SUMMARY & RESULTS EXPORT\n# ============================================================\n# Scans ALL runs and creates unified comparison tables\n# Outputs saved to: results_summary/ (project-level, not run-specific)\n\n# ============================================================\n# LOAD ALL MODEL RESULTS (from ALL RUN_IDs)\n# ============================================================\n\n# Scan all runs directories\nRUNS_DIR_DRIVE = Path(DRIVE_PROJECT_ROOT) / \"runs\"\nRUNS_DIR_LOCAL = Path(PROJECT_ROOT) / \"runs\"\n\nprint(f\"[INFO] Scanning all runs in: {RUNS_DIR_DRIVE}\")\n\nall_results = []\n\n# Get all RUN_ID folders\nrun_folders = []\nif RUNS_DIR_DRIVE.exists():\n    run_folders = sorted([d for d in RUNS_DIR_DRIVE.iterdir() if d.is_dir()], reverse=True)\n    print(f\"[INFO] Found {len(run_folders)} runs\")\n\nif len(run_folders) == 0:\n    print(\"[WARN] No runs found!\")\n\nfor run_folder in run_folders:\n    run_id = run_folder.name\n    models_dir = run_folder / \"models\"\n    ms_dir = run_folder / \"model_selection\"\n    config_dir = run_folder / \"config\"\n    \n    if not models_dir.exists():\n        continue\n    \n    print(f\"\\n[INFO] Loading from RUN_ID: {run_id}\")\n    \n    # --------------------------\n    # Load run config for period info\n    # --------------------------\n    run_config = {}\n    config_path = config_dir / \"run_params.json\"\n    if config_path.exists():\n        with open(config_path, \"r\") as f:\n            run_config = json.load(f)\n    \n    # Extract period info from config (date-based)\n    data_cfg = run_config.get(\"data\", {})\n    \n    train_start = data_cfg.get(\"limit_start_date\", \"N/A\")\n    train_end = data_cfg.get(\"train_end\", \"N/A\")\n    train_period = f\"{train_start[:10] if train_start != 'N/A' else 'N/A'} - {train_end}\"\n    \n    valid_start = data_cfg.get(\"valid_start\", \"N/A\")\n    valid_end = data_cfg.get(\"valid_end\", \"N/A\")\n    valid_period = f\"{valid_start} - {valid_end}\"\n    \n    test_start = data_cfg.get(\"test_start\", \"N/A\")\n    test_end = data_cfg.get(\"test_end\", \"latest\")\n    test_period = f\"{test_start} - {test_end if test_end else 'latest'}\"\n    \n    # Get actual data range from full_df.pkl\n    data_start = \"N/A\"\n    data_end = \"N/A\"\n    try:\n        # Search paths for full_df.pkl\n        full_df_paths = [\n            Path(DRIVE_PROJECT_ROOT) / \"data\" / \"interim\" / \"full_df.pkl\",\n            Path(PROJECT_ROOT) / \"data\" / \"interim\" / \"full_df.pkl\",\n        ]\n        \n        for full_df_path in full_df_paths:\n            if full_df_path.exists():\n                full_df = load_pickle(full_df_path)\n                if hasattr(full_df, 'index') and len(full_df.index) > 0:\n                    data_start = str(full_df.index.min().date())\n                    data_end = str(full_df.index.max().date())\n                break\n        \n        # Fallback to config if still N/A\n        if data_start == \"N/A\":\n            data_start = str(data_cfg.get(\"limit_start_date\", \"N/A\"))[:10]\n        \n        if data_end == \"N/A\":\n            test_end = data_cfg.get(\"test_end\")\n            if test_end:\n                data_end = test_end\n            else:\n                data_end = \"latest\"\n            \n    except Exception as e:\n        print(f\"    [WARN] Could not read data range: {e}\")\n    \n    # --------------------------\n    # 0. Baseline Results (from final_metrics.csv)\n    # --------------------------\n    xgb_metrics_path = models_dir / \"final_metrics.csv\"\n    \n    if xgb_metrics_path.exists():\n        metrics_df = pd.read_csv(xgb_metrics_path)\n        \n        # Look for baseline rows\n        for baseline_name in [\"BASELINE_ZERO\", \"BASELINE_NAIVE\"]:\n            baseline_rows = metrics_df[metrics_df[\"model\"] == baseline_name]\n            for _, row in baseline_rows.iterrows():\n                if row[\"split\"] == \"TEST\":\n                    all_results.append({\n                        \"run_id\": run_id,\n                        \"model\": baseline_name,\n                        \"feature_set\": \"baseline\",\n                        \"train_period\": train_period,\n                        \"valid_period\": valid_period,\n                        \"test_period\": test_period,\n                        \"data_start\": data_start,\n                        \"data_end\": data_end,\n                        \"test_wrmse\": float(row[\"wRMSE\"]),\n                        \"test_wmae\": float(row.get(\"wMAE\", 0)) if \"wMAE\" in row else None,\n                        \"test_diracc\": float(row[\"DirAcc\"]),\n                        \"params\": \"{}\",\n                    })\n                    print(f\"  \u2713 {baseline_name}: wRMSE={row['wRMSE']:.6f}\")\n    \n    # --------------------------\n    # 1. XGBoost Results\n    # --------------------------\n    \n    if xgb_metrics_path.exists():\n        xgb_metrics = pd.read_csv(xgb_metrics_path)\n        xgb_test = xgb_metrics[(xgb_metrics[\"model\"] == \"FINAL_XGB\") & (xgb_metrics[\"split\"] == \"TEST\")]\n        \n        if len(xgb_test) > 0:\n            xgb_test = xgb_test.iloc[0]\n            \n            # Load best params\n            xgb_params = {}\n            params_path = ms_dir / \"best_params_xgb_reg_t1.pkl\"\n            if params_path.exists():\n                xgb_params = load_pickle(params_path)\n            \n            all_results.append({\n                \"run_id\": run_id,\n                \"model\": \"XGBoost\",\n                \"feature_set\": \"xgb_selected\",\n                \"train_period\": train_period,\n                \"valid_period\": valid_period,\n                \"test_period\": test_period,\n                \"data_start\": data_start,\n                \"data_end\": data_end,\n                \"test_wrmse\": float(xgb_test[\"wRMSE\"]),\n                \"test_wmae\": float(xgb_test.get(\"wMAE\", 0)) if \"wMAE\" in xgb_test else None,\n                \"test_diracc\": float(xgb_test[\"DirAcc\"]),\n                \"params\": str(xgb_params),\n            })\n            print(f\"  \u2713 XGBoost: wRMSE={xgb_test['wRMSE']:.6f}\")\n    \n    \n    # --------------------------\n    # 1B. LightGBM Results\n    # --------------------------\n    lgb_metrics_path = models_dir / \"final_metrics_lgb.csv\"\n    lgb_json_path = run_folder / \"outputs\" / \"lgb_results.json\"\n    \n    if lgb_metrics_path.exists():\n        lgb_metrics = pd.read_csv(lgb_metrics_path)\n        lgb_test = lgb_metrics[(lgb_metrics[\"model\"] == \"FINAL_LGB\") & (lgb_metrics[\"split\"] == \"TEST\")]\n        if len(lgb_test) > 0:\n            lgb_row = lgb_test.iloc[0]\n            \n            # Load best params\n            lgb_params = {}\n            params_path = ms_dir / \"best_params_lgb_reg_t1.pkl\"\n            if params_path.exists():\n                lgb_params = load_pickle(params_path)\n            \n            all_results.append({\n                \"run_id\": run_id,\n                \"model\": \"LightGBM\",\n                \"feature_set\": \"xgb_selected\",\n                \"train_period\": train_period,\n                \"valid_period\": valid_period,\n                \"test_period\": test_period,\n                \"data_start\": data_start,\n                \"data_end\": data_end,\n                \"test_wrmse\": float(lgb_row[\"wRMSE\"]),\n                \"test_wmae\": float(lgb_row[\"wMAE\"]) if \"wMAE\" in lgb_row.index else None,\n                \"test_diracc\": float(lgb_row[\"DirAcc\"]),\n                \"params\": str(lgb_params),\n            })\n            print(f\"  \u2713 LightGBM: wRMSE={lgb_row['wRMSE']:.6f}\")\n    elif lgb_json_path.exists():\n        # Try loading from JSON\n        with open(lgb_json_path, \"r\") as f:\n            lgb_data = json.load(f)\n        \n        all_results.append({\n            \"run_id\": run_id,\n            \"model\": \"LightGBM\",\n            \"feature_set\": \"xgb_selected\",\n            \"train_period\": train_period,\n            \"valid_period\": valid_period,\n            \"test_period\": test_period,\n            \"data_start\": data_start,\n            \"data_end\": data_end,\n            \"test_wrmse\": float(lgb_data.get(\"test_wrmse\", 0)),\n            \"test_wmae\": float(lgb_data.get(\"test_wmae\", 0)) if \"test_wmae\" in lgb_data else None,\n            \"test_diracc\": float(lgb_data.get(\"test_diracc\", 0)),\n            \"params\": str({k: lgb_data.get(k) for k in [\"num_leaves\", \"max_depth\", \"learning_rate\", \"subsample\", \"colsample_bytree\"] if k in lgb_data}),\n        })\n        print(f\"  \u2713 LightGBM: wRMSE={lgb_data.get('test_wrmse', 0):.6f}\")\n\n# --------------------------\n    # 2. LSTM & GRU Results\n    # --------------------------\n    for model_type in [\"lstm\", \"gru\"]:\n        summary_path = models_dir / f\"{model_type}_summary.csv\"\n        \n        if summary_path.exists():\n            df = pd.read_csv(summary_path)\n            \n            for _, row in df.iterrows():\n                all_results.append({\n                    \"run_id\": run_id,\n                    \"model\": model_type.upper(),\n                    \"feature_set\": row[\"feature_set\"],\n                    \"train_period\": train_period,\n                    \"valid_period\": valid_period,\n                    \"test_period\": test_period,\n                    \"data_start\": data_start,\n                    \"data_end\": data_end,\n                    \"test_wrmse\": float(row[\"model_test_wrmse\"]),\n                    \"test_wmae\": float(row.get(\"model_test_wmae\", 0)) if \"model_test_wmae\" in row else None,\n                    \"test_diracc\": float(row[\"model_test_diracc\"]),\n                    \"params\": str(run_config.get(model_type, {})),\n                })\n            print(f\"  \u2713 {model_type.upper()}: {len(df)} configurations\")\n    \n    # --------------------------\n    # 3. Hybrid Results\n    # --------------------------\n    for hybrid_type in [\"hybrid_seq\", \"hybrid_par\"]:\n        summary_path = models_dir / f\"{hybrid_type}_summary.csv\"\n        \n        if summary_path.exists():\n            df = pd.read_csv(summary_path)\n            \n            for _, row in df.iterrows():\n                model_name = \"Hybrid-Seq\" if hybrid_type == \"hybrid_seq\" else \"Hybrid-Par\"\n                all_results.append({\n                    \"run_id\": run_id,\n                    \"model\": model_name,\n                    \"feature_set\": row[\"feature_set\"],\n                    \"train_period\": train_period,\n                    \"valid_period\": valid_period,\n                    \"test_period\": test_period,\n                    \"data_start\": data_start,\n                    \"data_end\": data_end,\n                    \"test_wrmse\": float(row[\"model_test_wrmse\"]),\n                    \"test_wmae\": float(row.get(\"model_test_wmae\", 0)) if \"model_test_wmae\" in row else None,\n                    \"test_diracc\": float(row[\"model_test_diracc\"]),\n                    \"params\": str(run_config.get(hybrid_type, {})),\n                })\n            print(f\"  \u2713 {hybrid_type}: {len(df)} configurations\")\n    \n    # --------------------------\n    # 4. Ensemble Results\n    # --------------------------\n    ensemble_path = run_folder / \"outputs\" / \"ensemble_results.json\"\n    \n    if ensemble_path.exists():\n        with open(ensemble_path, \"r\") as f:\n            ens_data = json.load(f)\n        \n        ens_method = ens_data.get(\"method\", \"weighted_average\")\n        all_results.append({\n            \"run_id\": run_id,\n            \"model\": f\"Ensemble-{ens_method}\",\n            \"feature_set\": \"all_models\",\n            \"train_period\": train_period,\n            \"valid_period\": valid_period,\n            \"test_period\": test_period,\n            \"data_start\": data_start,\n            \"data_end\": data_end,\n            \"test_wrmse\": float(ens_data.get(\"test_wrmse\", 0)),\n            \"test_wmae\": float(ens_data.get(\"test_wmae\", 0)) if \"test_wmae\" in ens_data else None,\n            \"test_diracc\": float(ens_data.get(\"test_diracc\", 0)),\n            \"params\": str(ens_data.get(\"weights\", {})),\n        })\n        print(f\"  \u2713 Ensemble ({ens_method}): wRMSE={ens_data.get('test_wrmse', 0):.6f}\")\n\nprint(f\"\\n[OK] Loaded {len(all_results)} total results from {len(run_folders)} runs\")\n\n# ============================================================\n# CREATE COMPARISON TABLES\n# ============================================================\n\nif len(all_results) == 0:\n    print(\"[ERROR] No results found! Run training sections first.\")\nelse:\n    # Create DataFrame\n    results_df = pd.DataFrame(all_results)\n    \n    # Filter out invalid results (wRMSE=0 is impossible except for perfect predictions)\n    # Keep BASELINE_ZERO which has legitimate 0 values for some metrics\n    invalid_mask = (\n        (results_df[\"test_wrmse\"] == 0) & \n        (results_df[\"test_diracc\"] == 0) &\n        (~results_df[\"model\"].str.contains(\"BASELINE\", case=False, na=False))\n    )\n    if invalid_mask.any():\n        n_invalid = invalid_mask.sum()\n        print(f\"[WARN] Filtering {n_invalid} invalid results (wRMSE=0 and DirAcc=0)\")\n        results_df = results_df[~invalid_mask]\n    \n    # Sort by test_wrmse (lower is better)\n    results_df = results_df.sort_values(\"test_wrmse\").reset_index(drop=True)\n    \n    # Add rank\n    results_df.insert(0, \"rank\", range(1, len(results_df) + 1))\n    \n    # Count unique runs\n    n_runs = results_df[\"run_id\"].nunique()\n    n_models = len(results_df)\n    \n    # =========================================\n    # REPORT 1: All Results from All RUN_IDs\n    # =========================================\n    print(\"\\n\" + \"=\"*100)\n    print(f\"REPORT 1: ALL RESULTS ({n_models} results from {n_runs} runs)\")\n    print(\"=\"*100)\n    \n    display_cols = [\"rank\", \"run_id\", \"model\", \"feature_set\", \n                    \"train_period\", \"valid_period\", \"test_period\", \"data_start\", \"data_end\",\n                    \"test_wrmse\", \"test_diracc\"]\n    display(results_df[display_cols])\n    \n    # =========================================\n    # REPORT 2: Best per Model Type (across ALL runs)\n    # =========================================\n    print(\"\\n\" + \"=\"*100)\n    print(\"REPORT 2: BEST PER MODEL TYPE (across all runs)\")\n    print(\"=\"*100)\n    \n    # Get best (lowest wRMSE) for each model type across ALL runs\n    best_per_model = results_df.loc[results_df.groupby(\"model\")[\"test_wrmse\"].idxmin()]\n    best_per_model = best_per_model.drop(columns=[\"rank\"], errors=\"ignore\")\n    best_per_model = best_per_model.sort_values(\"test_wrmse\").reset_index(drop=True)\n    best_per_model.insert(0, \"rank\", range(1, len(best_per_model) + 1))\n    \n    display_cols_best = [\"rank\", \"model\", \"feature_set\", \"run_id\",\n                         \"train_period\", \"valid_period\", \"test_period\", \"data_start\", \"data_end\",\n                         \"test_wrmse\", \"test_diracc\"]\n    display(best_per_model[display_cols_best])\n    \n    # =========================================\n    # Overall Best Model\n    # =========================================\n    best = results_df.iloc[0]\n    print(\"\\n\" + \"=\"*100)\n    print(\"OVERALL BEST MODEL (across all runs)\")\n    print(\"=\"*100)\n    print(f\"  Run ID:       {best['run_id']}\")\n    print(f\"  Model:        {best['model']}\")\n    print(f\"  Feature Set:  {best['feature_set']}\")\n    print(f\"  Train Period: {best['train_period']}\")\n    print(f\"  Valid Period: {best['valid_period']}\")\n    print(f\"  Test Period:  {best['test_period']}\")\n    print(f\"  Data Range:   {best['data_start']} to {best['data_end']}\")\n    print(f\"  Test wRMSE:   {best['test_wrmse']:.6f}\")\n    print(f\"  Test DirAcc:  {best['test_diracc']:.4f}\")\n    \n    # =========================================\n    # REPORT 3: Bootstrap Confidence Intervals\n    # =========================================\n    print(\"\\n\" + \"=\"*100)\n\n# ============================================================\n# SAVE RESULTS (LOCAL + DRIVE)\n# ============================================================\n\nfrom datetime import datetime\n\n# Directories - save to project level (not run-specific)\nRESULTS_LOCAL = ensure_dir(Path(PROJECT_ROOT) / \"results_summary\")\nRESULTS_DRIVE = ensure_dir(Path(DRIVE_PROJECT_ROOT) / \"results_summary\")\n\nprint(f\"[INFO] Results output dirs:\")\nprint(f\"  - LOCAL: {RESULTS_LOCAL}\")\nprint(f\"  - DRIVE: {RESULTS_DRIVE}\")\n\nif len(all_results) > 0:\n    n_runs = results_df[\"run_id\"].nunique()\n    \n    # =========================================\n    # Save Report 1: All Results (all runs)\n    # =========================================\n    all_results_cols = [\"rank\", \"run_id\", \"model\", \"feature_set\", \n                        \"train_period\", \"valid_period\", \"test_period\",\n                        \"data_start\", \"data_end\",\n                        \"test_wrmse\", \"test_wmae\", \"test_diracc\"]\n    all_results_df = results_df[all_results_cols].copy()\n    \n    all_results_df.to_csv(RESULTS_LOCAL / \"all_results.csv\", index=False)\n    all_results_df.to_csv(RESULTS_DRIVE / \"all_results.csv\", index=False)\n    print(\"[OK] Saved: all_results.csv\")\n    \n    # =========================================\n    # Save Report 2: Best per Model (all runs)\n    # =========================================\n    best_cols = [\"rank\", \"model\", \"feature_set\", \"run_id\",\n                 \"train_period\", \"valid_period\", \"test_period\",\n                 \"data_start\", \"data_end\",\n                 \"test_wrmse\", \"test_wmae\", \"test_diracc\"]\n    best_per_model_df = best_per_model[best_cols].copy()\n    \n    best_per_model_df.to_csv(RESULTS_LOCAL / \"best_per_model.csv\", index=False)\n    best_per_model_df.to_csv(RESULTS_DRIVE / \"best_per_model.csv\", index=False)\n    print(\"[OK] Saved: best_per_model.csv\")\n    \n    # =========================================\n    # Save Full Results with Params\n    # =========================================\n    results_df.to_csv(RESULTS_LOCAL / \"all_results_with_params.csv\", index=False)\n    results_df.to_csv(RESULTS_DRIVE / \"all_results_with_params.csv\", index=False)\n    print(\"[OK] Saved: all_results_with_params.csv\")\n    \n    # =========================================\n    # Generate RESULTS.md\n    # =========================================\n    best = results_df.iloc[0]\n    \n    md_lines = [\n        \"# Results Summary\",\n        \"\",\n        f\"**Last Updated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n        \"\",\n        f\"**Total Runs:** {n_runs} | **Total Configurations:** {len(results_df)}\",\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83c\udfc6 Best Results (Top 10)\",\n        \"\",\n        \"| # | Model | Feature Set | wRMSE | DirAcc | Run |\",\n        \"|---|-------|-------------|-------|--------|-----|\",\n    ]\n    \n    for _, row in results_df.head(10).iterrows():\n        wrmse = f\"{row['test_wrmse']:.6f}\" if pd.notna(row[\"test_wrmse\"]) else \"-\"\n        diracc = f\"{row['test_diracc']:.2%}\" if pd.notna(row[\"test_diracc\"]) else \"-\"\n        run_short = row['run_id'][-6:] if len(str(row['run_id'])) > 6 else row['run_id']\n        md_lines.append(f\"| {row['rank']} | {row['model']} | {row['feature_set']} | {wrmse} | {diracc} | {run_short} |\")\n    \n    md_lines.extend([\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83d\udcca Best per Model Type\",\n        \"\",\n        \"| # | Model | Feature Set | wRMSE | DirAcc |\",\n        \"|---|-------|-------------|-------|--------|\",\n    ])\n    \n    for _, row in best_per_model.iterrows():\n        wrmse = f\"{row['test_wrmse']:.6f}\" if pd.notna(row[\"test_wrmse\"]) else \"-\"\n        diracc = f\"{row['test_diracc']:.2%}\" if pd.notna(row[\"test_diracc\"]) else \"-\"\n        md_lines.append(f\"| {row['rank']} | {row['model']} | {row['feature_set']} | {wrmse} | {diracc} |\")\n    \n    md_lines.extend([\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83e\udd47 Overall Best\",\n        \"\",\n        \"| Metric | Value |\",\n        \"|--------|-------|\",\n        f\"| Model | **{best['model']}** |\",\n        f\"| Feature Set | {best['feature_set']} |\",\n        f\"| wRMSE | {best['test_wrmse']:.6f} |\",\n        f\"| DirAcc | {best['test_diracc']:.2%} |\",\n        f\"| Run ID | {best['run_id']} |\",\n        f\"| Data Range | {best['data_start']} \u2192 {best['data_end']} |\",\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83d\udcd6 Metrics\",\n        \"\",\n        \"| Metric | Description |\",\n        \"|--------|-------------|\",\n        \"| wRMSE | Weighted Root Mean Squared Error (\u2193 lower is better) |\",\n        \"| DirAcc | Directional Accuracy (\u2191 higher is better) |\",\n        \"\",\n        \"*Full details with period configurations available in CSV files.*\",\n    ])\n    \n    md_content = \"\\n\".join(md_lines)\n    \n    (RESULTS_LOCAL / \"RESULTS.md\").write_text(md_content)\n    (RESULTS_DRIVE / \"RESULTS.md\").write_text(md_content)\n    print(\"[OK] Saved: RESULTS.md\")\n    \n    # =========================================\n    # Save Bootstrap CI (if computed)\n    # =========================================\n    if 'bootstrap_df' in dir() and len(bootstrap_df) > 0:\n        bootstrap_df.to_csv(RESULTS_LOCAL / \"bootstrap_ci.csv\", index=False)\n        bootstrap_df.to_csv(RESULTS_DRIVE / \"bootstrap_ci.csv\", index=False)\n        print(\"[OK] Saved: bootstrap_ci.csv\")\n    \n    # =========================================\n    # TOMORROW PREDICTIONS SUMMARY\n    # =========================================\n    # Collect tomorrow.csv from all models across all runs\n    \n    print(\"\\n[INFO] Collecting tomorrow predictions...\")\n    \n    tomorrow_results = []\n    \n    # Use the most recent run for tomorrow predictions\n    if len(run_folders) > 0:\n        latest_run = run_folders[0]  # Already sorted desc\n        run_id = latest_run.name\n        pred_dir = latest_run / \"predictions\"\n        outputs_dir = latest_run / \"outputs\"\n        \n        # Model paths for tomorrow.csv\n        tomorrow_paths = {\n            (\"XGBoost\", \"xgb_selected\"): pred_dir / \"xgb\" / \"tomorrow.csv\",\n            (\"LightGBM\", \"xgb_selected\"): pred_dir / \"lgb\" / \"tomorrow.csv\",\n            (\"LSTM\", \"xgb_selected\"): pred_dir / \"lstm_xgb_selected\" / \"tomorrow.csv\",\n            (\"LSTM\", \"neural_40\"): pred_dir / \"lstm_neural_40\" / \"tomorrow.csv\",\n            (\"LSTM\", \"neural_80\"): pred_dir / \"lstm_neural_80\" / \"tomorrow.csv\",\n            (\"GRU\", \"xgb_selected\"): pred_dir / \"gru_xgb_selected\" / \"tomorrow.csv\",\n            (\"GRU\", \"neural_40\"): pred_dir / \"gru_neural_40\" / \"tomorrow.csv\",\n            (\"GRU\", \"neural_80\"): pred_dir / \"gru_neural_80\" / \"tomorrow.csv\",\n            (\"Hybrid-Seq\", \"xgb_selected\"): pred_dir / \"hybrid_seq_xgb_selected\" / \"tomorrow.csv\",\n            (\"Hybrid-Seq\", \"neural_40\"): pred_dir / \"hybrid_seq_neural_40\" / \"tomorrow.csv\",\n            (\"Hybrid-Seq\", \"neural_80\"): pred_dir / \"hybrid_seq_neural_80\" / \"tomorrow.csv\",\n            (\"Hybrid-Par\", \"xgb_selected\"): pred_dir / \"hybrid_par_xgb_selected\" / \"tomorrow.csv\",\n            (\"Hybrid-Par\", \"neural_40\"): pred_dir / \"hybrid_par_neural_40\" / \"tomorrow.csv\",\n            (\"Hybrid-Par\", \"neural_80\"): pred_dir / \"hybrid_par_neural_80\" / \"tomorrow.csv\",\n            (\"Ensemble\", \"all_models\"): outputs_dir / \"tomorrow.csv\",\n        }\n        \n        for (model, feature_set), path in tomorrow_paths.items():\n            if path.exists():\n                try:\n                    df = pd.read_csv(path)\n                    if len(df) > 0:\n                        row = df.iloc[0]\n                        tomorrow_results.append({\n                            \"run_id\": run_id,\n                            \"model\": model,\n                            \"feature_set\": row.get(\"feature_set\", feature_set),\n                            \"last_data_date\": row.get(\"last_data_date\", \"\"),\n                            \"pred_logret\": float(row.get(\"pred_logret\", 0)),\n                            \"pred_return_pct\": float(row.get(\"pred_return_pct\", row.get(\"pred_logret\", 0) * 100)),\n                        })\n                        print(f\"  \u2713 {model} ({feature_set}): {row.get('pred_return_pct', 0):.4f}%\")\n                except Exception as e:\n                    pass\n    \n    if tomorrow_results:\n        tomorrow_df = pd.DataFrame(tomorrow_results)\n        # Sort by predicted return (highest first for bullish, lowest first for bearish)\n        tomorrow_df = tomorrow_df.sort_values(\"pred_return_pct\", ascending=False).reset_index(drop=True)\n        tomorrow_df.insert(0, \"rank\", range(1, len(tomorrow_df) + 1))\n        \n        tomorrow_df.to_csv(RESULTS_LOCAL / \"tomorrow_summary.csv\", index=False)\n        tomorrow_df.to_csv(RESULTS_DRIVE / \"tomorrow_summary.csv\", index=False)\n        print(f\"[OK] Saved: tomorrow_summary.csv ({len(tomorrow_df)} predictions)\")\n        \n        # Show summary\n        print(\"\\n\" + \"=\"*70)\n        print(\"TOMORROW PREDICTIONS (next trading day)\")\n        print(\"=\"*70)\n        print(tomorrow_df[[\"rank\", \"model\", \"feature_set\", \"pred_return_pct\"]].to_string(index=False))\n    else:\n        print(\"[WARN] No tomorrow predictions found\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"FILES SAVED\")\n    print(\"=\"*70)\n    print(f\"\\nLOCAL: {RESULTS_LOCAL}/\")\n    print(f\"DRIVE: {RESULTS_DRIVE}/\")\n    print(\"\\nFiles:\")\n    print(\"  - all_results.csv          (Report 1: all runs with periods)\")\n    print(\"  - best_per_model.csv       (Report 2: best per model with periods)\")\n    print(\"  - all_results_with_params.csv\")\n    print(\"  - bootstrap_ci.csv         (Report 3: bootstrap confidence intervals)\")\n    print(\"  - tomorrow_summary.csv     (Report 4: tomorrow predictions)\")\n    print(\"  - RESULTS.md\")\n\n# ============================================================\n# DOWNLOAD FILES FOR GIT (Colab only)\n# ============================================================\n\nif IN_COLAB and len(all_results) > 0:\n    from google.colab import files\n    \n    print(\"[INFO] Downloading files for Git...\")\n    print(\"(Copy these to your repo: results/ folder)\\n\")\n    \n    # Download from DRIVE (project level - persistent location)\n    for f in RESULTS_DRIVE.glob(\"*\"):\n        if f.is_file() and f.suffix in [\".csv\", \".md\"]:\n            files.download(str(f))\n            print(f\"  Downloaded: {f.name}\")\n    \n    print(\"\\n[OK] Files downloaded!\")\n    print(\"\\nNext steps:\")\n    print(\"  1. Copy downloaded files to your repo: results/\")\n    print(\"  2. git add results/\")\n    print('  3. git commit -m \"Update results\"')\n    print(\"  4. git push\")\nelse:\n    if len(all_results) > 0:\n        print(f\"[INFO] Files saved to: {RESULTS_DRIVE}\")\n    else:\n        print(\"[INFO] No results to download\")\n\nprint(\"\\n[OK] BLOCK 31 complete.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 32 \u2014 BOOTSTRAP CONFIDENCE INTERVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BOOTSTRAP CONFIDENCE INTERVALS\n# ============================================================\n\n# Run bootstrap analysis ONLY on models in best_per_model\n# This provides statistical confidence for the metrics\n\n# --- Bootstrap helper functions (standalone) ---\n\ndef bootstrap_metric(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    weights = None,\n    metric_fn = None,\n    n_bootstrap: int = 1000,\n    confidence_level: float = 0.95,\n    random_state: int = 42\n):\n    \"\"\"Compute bootstrap confidence interval for a single metric.\"\"\"\n    y_true = _to_np(y_true)\n    y_pred = _to_np(y_pred)\n    n = len(y_true)\n    \n    if weights is None:\n        weights = np.ones(n)\n    weights = _to_np(weights)\n    \n    rng = np.random.RandomState(random_state)\n    \n    # Point estimate\n    point_estimate = metric_fn(y_true, y_pred, weights)\n    \n    # Bootstrap samples\n    bootstrap_values = []\n    for _ in range(n_bootstrap):\n        idx = rng.randint(0, n, size=n)\n        val = metric_fn(y_true[idx], y_pred[idx], weights[idx])\n        bootstrap_values.append(val)\n    \n    bootstrap_values = np.array(bootstrap_values)\n    \n    # Confidence interval (percentile method)\n    alpha = 1 - confidence_level\n    ci_lower = np.percentile(bootstrap_values, 100 * alpha / 2)\n    ci_upper = np.percentile(bootstrap_values, 100 * (1 - alpha / 2))\n    \n    return {\n        \"point_estimate\": float(point_estimate),\n        \"ci_lower\": float(ci_lower),\n        \"ci_upper\": float(ci_upper),\n        \"std\": float(np.std(bootstrap_values)),\n        \"n_bootstrap\": n_bootstrap,\n        \"confidence_level\": confidence_level,\n    }\n\n\ndef bootstrap_all_metrics(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    weights = None,\n    n_bootstrap: int = 1000,\n    confidence_level: float = 0.95,\n    random_state: int = 42\n):\n    \"\"\"Compute bootstrap confidence intervals for all standard metrics.\"\"\"\n    # Wrapper functions that accept weights\n    def wrmse_fn(yt, yp, w):\n        return w_rmse(yt, yp, w)\n    \n    def wmae_fn(yt, yp, w):\n        return w_mae(yt, yp, w)\n    \n    def diracc_fn(yt, yp, _w):\n        return dir_acc(yt, yp)  # DirAcc doesn't use weights\n    \n    metrics = {\n        \"wrmse\": wrmse_fn,\n        \"wmae\": wmae_fn,\n        \"diracc\": diracc_fn,\n    }\n    \n    results = {}\n    for name, fn in metrics.items():\n        results[name] = bootstrap_metric(\n            y_true, y_pred, weights, fn,\n            n_bootstrap=n_bootstrap,\n            confidence_level=confidence_level,\n            random_state=random_state\n        )\n    \n    return results\n\n\ndef format_ci(ci_result, decimals: int = 6) -> str:\n    \"\"\"Format confidence interval as string: point [lower, upper].\"\"\"\n    return (\n        f\"{ci_result['point_estimate']:.{decimals}f} \"\n        f\"[{ci_result['ci_lower']:.{decimals}f}, {ci_result['ci_upper']:.{decimals}f}]\"\n    )\n\n\n# --- Run Bootstrap Analysis ---\n\nprint(\"[INFO] Running Bootstrap Confidence Interval Analysis...\")\nprint(f\"[INFO] Models to analyze: {len(best_per_model)} (from best_per_model)\")\n\nN_BOOTSTRAP = 1000\nCONFIDENCE_LEVEL = 0.95\n\nbootstrap_results = []\n\n# Get the best run for each model from best_per_model\nfor _, row in best_per_model.iterrows():\n    model_name = row[\"model\"]\n    run_id = row[\"run_id\"]\n    feature_set = row.get(\"feature_set\", \"unknown\")\n    \n    print(f\"\\n[INFO] Analyzing: {model_name} (run: {run_id})\")\n    \n    # Find predictions file for this model/run\n    run_dir = RUNS_DIR_DRIVE / run_id\n    models_dir = run_dir / \"models\"\n    pred_dir = run_dir / \"predictions\"\n    \n    y_true = None\n    y_pred = None\n    weights = None\n    \n    # Try to load predictions based on model type\n    # All predictions are in predictions/{model}/ or predictions/{model}_{feature_set}/\n    try:\n        pred_path = None\n        \n        if model_name == \"XGBoost\":\n            pred_path = pred_dir / \"xgb\" / \"predictions_test.csv\"\n                \n        elif model_name == \"LightGBM\":\n            pred_path = pred_dir / \"lgb\" / \"predictions_test.csv\"\n                \n        elif model_name in [\"LSTM\", \"GRU\"]:\n            model_lower = model_name.lower()\n            # Try different feature_set paths\n            for fs in [\"neural_40\", \"xgb_selected\"]:\n                p = pred_dir / f\"{model_lower}_{fs}\" / \"predictions_test.csv\"\n                if p.exists():\n                    pred_path = p\n                    break\n                \n        elif model_name in [\"Hybrid-Seq\", \"Hybrid-Par\"]:\n            model_key = \"hybrid_seq\" if model_name == \"Hybrid-Seq\" else \"hybrid_par\"\n            for fs in [\"neural_40\", \"xgb_selected\"]:\n                p = pred_dir / f\"{model_key}_{fs}\" / \"predictions_test.csv\"\n                if p.exists():\n                    pred_path = p\n                    break\n                \n        elif \"Ensemble\" in model_name:\n            pred_path = run_dir / \"outputs\" / \"ensemble_predictions_test.csv\"\n                \n        elif model_name in [\"BASELINE_ZERO\", \"BASELINE_NAIVE\"]:\n            # Use XGBoost predictions for y_true\n            pred_path = pred_dir / \"xgb\" / \"predictions_test.csv\"\n        \n        # Load and parse predictions\n        if pred_path and pred_path.exists():\n            df = pd.read_csv(pred_path)\n            \n            # Get y_true (different column names)\n            if \"y_true\" in df.columns:\n                y_true = df[\"y_true\"].values\n            elif \"actual\" in df.columns:\n                y_true = df[\"actual\"].values\n            else:\n                raise ValueError(f\"No y_true/actual column in {pred_path}\")\n            \n            # Get predictions\n            if model_name == \"BASELINE_ZERO\":\n                y_pred = np.zeros(len(y_true))\n            elif model_name == \"BASELINE_NAIVE\":\n                y_pred = np.roll(y_true, 1)\n                y_pred[0] = 0\n            elif \"y_pred_model\" in df.columns:\n                y_pred = df[\"y_pred_model\"].values\n            elif \"predicted\" in df.columns:\n                y_pred = df[\"predicted\"].values\n            else:\n                raise ValueError(f\"No prediction column in {pred_path}\")\n            \n            # Get weights\n            if \"sample_weight\" in df.columns:\n                weights = df[\"sample_weight\"].values\n            else:\n                weights = np.ones(len(y_true))\n        \n        if y_true is not None and y_pred is not None:\n            # Run bootstrap\n            ci_results = bootstrap_all_metrics(\n                y_true, y_pred, weights,\n                n_bootstrap=N_BOOTSTRAP,\n                confidence_level=CONFIDENCE_LEVEL,\n                random_state=42\n            )\n            \n            bootstrap_results.append({\n                \"model\": model_name,\n                \"run_id\": run_id,\n                \"feature_set\": feature_set,\n                \"n_samples\": len(y_true),\n                \"n_bootstrap\": N_BOOTSTRAP,\n                \"confidence_level\": CONFIDENCE_LEVEL,\n                # wRMSE\n                \"wrmse\": ci_results[\"wrmse\"][\"point_estimate\"],\n                \"wrmse_ci_lower\": ci_results[\"wrmse\"][\"ci_lower\"],\n                \"wrmse_ci_upper\": ci_results[\"wrmse\"][\"ci_upper\"],\n                \"wrmse_std\": ci_results[\"wrmse\"][\"std\"],\n                # wMAE\n                \"wmae\": ci_results[\"wmae\"][\"point_estimate\"],\n                \"wmae_ci_lower\": ci_results[\"wmae\"][\"ci_lower\"],\n                \"wmae_ci_upper\": ci_results[\"wmae\"][\"ci_upper\"],\n                \"wmae_std\": ci_results[\"wmae\"][\"std\"],\n                # DirAcc\n                \"diracc\": ci_results[\"diracc\"][\"point_estimate\"],\n                \"diracc_ci_lower\": ci_results[\"diracc\"][\"ci_lower\"],\n                \"diracc_ci_upper\": ci_results[\"diracc\"][\"ci_upper\"],\n                \"diracc_std\": ci_results[\"diracc\"][\"std\"],\n            })\n            \n            print(f\"  wRMSE: {format_ci(ci_results['wrmse'])}\")\n            print(f\"  DirAcc: {format_ci(ci_results['diracc'], decimals=4)}\")\n        else:\n            print(f\"  [WARN] Could not load predictions for {model_name}\")\n            \n    except Exception as e:\n        print(f\"  [ERROR] {e}\")\n\n# Create DataFrame\nif len(bootstrap_results) > 0:\n    bootstrap_df = pd.DataFrame(bootstrap_results)\n    bootstrap_df = bootstrap_df.sort_values(\"wrmse\").reset_index(drop=True)\n    bootstrap_df.insert(0, \"rank\", range(1, len(bootstrap_df) + 1))\n    \n    print(\"\\n\" + \"=\"*100)\n    print(f\"BOOTSTRAP CONFIDENCE INTERVALS ({CONFIDENCE_LEVEL*100:.0f}% CI, n={N_BOOTSTRAP})\")\n    print(\"=\"*100)\n    \n    # Display summary\n    display_cols = [\"rank\", \"model\", \"wrmse\", \"wrmse_ci_lower\", \"wrmse_ci_upper\", \n                    \"diracc\", \"diracc_ci_lower\", \"diracc_ci_upper\"]\n    display(bootstrap_df[display_cols])\n    \n    # Save to results_summary\n    bootstrap_df.to_csv(RESULTS_LOCAL / \"bootstrap_ci.csv\", index=False)\n    bootstrap_df.to_csv(RESULTS_DRIVE / \"bootstrap_ci.csv\", index=False)\n    print(f\"\\n[OK] Saved: bootstrap_ci.csv\")\n    \n    # Also save as JSON for detailed results\n    save_json(bootstrap_results, RESULTS_LOCAL / \"bootstrap_ci.json\")\n    save_json(bootstrap_results, RESULTS_DRIVE / \"bootstrap_ci.json\")\n    print(\"[OK] Saved: bootstrap_ci.json\")\nelse:\n    print(\"[WARN] No bootstrap results computed\")\n"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}